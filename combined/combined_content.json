[
    {
        "document_name": "swarmauri/__init__.py",
        "content": "```swarmauri/__init__.py\n__version__ = \"0.4.1.dev100\"\n__long_desc__ = \"\"\"\n# Swarmauri SDK\n\nThe Swarmauri SDK offers a comprehensive suite of tools designed for building distributed, extensible systems using the Swarmauri framework. \n\n## Core \n- **Core Interfaces**: Define the fundamental communication and data-sharing protocols between components in a Swarmauri-based system.\n\n## Standard\n- **Base Classes**: Provide a foundation for constructing Swarmauri components, with standardized methods and properties.\n- **Mixins**: Reusable code fragments designed to be integrated into various classes, offering shared functionality across different components.\n- **Concrete Classes**: Ready-to-use, pre-implemented classes that fulfill standard system needs while adhering to Swarmauri principles. **These classes are the first in line for ongoing support and maintenance, ensuring they remain stable, performant, and up to date with future SDK developments.**\n\n## Community\n- **Third-Party Plug-in Integration**: Concrete classes designed to extend the framework\u00e2\u20ac\u2122s capabilities by utilizing third-party libraries and plugins.\n- **Open Source Contributions**: A collaborative space for developers to contribute new components, plug-ins, and features.\n\n## Experimental\n- **In-Development Components**: Early-stage features and components that push the boundaries of the Swarmauri framework, offering innovative solutions that are still in testing phases.\n\n# Features\n\n- **Polymorphism**: Allows for dynamic behavior switching between components, enabling flexible, context-aware system behavior.\n- **Discriminated Unions**: Provides a robust method for handling multiple possible object types in a type-safe manner.\n- **Serialization**: Efficiently encode and decode data for transmission across different environments and system components, with support for both standard and custom serialization formats.\n- **Intensional and Extensional Programming**: Leverages both rule-based (intensional) and set-based (extensional) approaches to building and manipulating complex data structures, offering developers a wide range of tools for system design.\n\n## Use Cases\n\n- **Modular Systems**: Develop scalable, pluggable systems that can evolve over time by adding or modifying components without disrupting the entire ecosystem.\n- **Distributed Architectures**: Build systems with distributed nodes that seamlessly communicate using the SDK\u00e2\u20ac\u2122s standardized interfaces.\n- **Third-Party Integrations**: Extend the system's capabilities by easily incorporating third-party tools, libraries, and services.\n- **Prototype and Experimentation**: Test cutting-edge ideas using the experimental components in the SDK, while retaining the reliability of core and standard features for production systems.\n\n# Future Development\n\nThe Swarmauri SDK is an evolving platform, and the community is encouraged to contribute to its growth. Upcoming releases will focus on enhancing the framework's modularity, providing more advanced serialization methods, and expanding the community-driven component library.\n\n\"\"\"\n\n```"
    },
    {
        "document_name": "swarmauri/community/__init__.py",
        "content": "```swarmauri/community/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/__init__.py",
        "content": "```swarmauri/community/tools/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/base/__init__.py",
        "content": "```swarmauri/community/tools/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/__init__.py",
        "content": "```swarmauri/community/tools/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/GmailSendTool.py",
        "content": "```swarmauri/community/tools/concrete/GmailSendTool.py\nimport base64\nimport json\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom googleapiclient import discovery\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass GmailSendTool(ToolBase):\n    SCOPES = ['https://www.googleapis.com/auth/gmail.send']\n\n    def __init__(self, credentials_path: str, sender_email: str):\n        \"\"\"\n        Initializes the GmailSendTool with a path to the credentials JSON file and the sender email.\n\n        Parameters:\n        credentials_path (str): The path to the Gmail service JSON file.\n        sender_email (str): The email address being used to send emails.\n        \"\"\"\n        \n        parameters = [\n            Parameter(\n                name=\"recipients\",\n                type=\"string\",\n                description=\"The email addresses of the recipients, separated by commas\",\n                required=True\n            ),\n            Parameter(\n                name=\"subject\",\n                type=\"string\",\n                description=\"The subject of the email\",\n                required=True\n            ),\n            Parameter(\n                name=\"htmlMsg\",\n                type=\"string\",\n                description=\"The HTML message to be sent as the email body\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"GmailSendTool\", \n                         description=\"Sends an email using the Gmail API.\",\n                         parameters=parameters)\n        self.credentials_path = credentials_path\n        self.sender_email = sender_email\n        \n\n    def authenticate(self):\n        \"\"\"\n        Authenticates the user and creates a Gmail API service for sending emails.\n        \"\"\"\n        credentials = service_account.Credentials.from_service_account_file(\n                self.credentials_path, scopes=self.SCOPES)\n        \n        delegated_credentials = credentials.with_subject(self.sender_email)\n        self.service = build('gmail', 'v1', credentials=delegated_credentials)\n\n    def create_message(self, to: str, subject: str, message_text: str):\n        \"\"\"\n        Create a MIMEText message for sending an email.\n\n        Parameters:\n        sender (str): The email address of the sender.\n        to (str): The email address of the recipient.\n        subject (str): The subject of the email.\n        message_text (str): The HTML body of the email.\n\n        Returns:\n        The created MIMEText message.\n        \"\"\"\n        message = MIMEMultipart('alternative')\n        message['from'] = self.sender_email\n        message['to'] = to\n        message['subject'] = subject\n        mime_text = MIMEText(message_text, 'html')\n        message.attach(mime_text)\n        raw_message = base64.urlsafe_b64encode(message.as_string().encode('utf-8'))\n        return {'raw': raw_message.decode('utf-8')}\n\n    def __call__(self, recipients, subject, htmlMsg):\n        \"\"\"\n        Sends an email to the specified recipients with the given subject and HTML message.\n        \n        Parameters:\n        sender (str): The email address of the sender.\n        recipients (str): The email address of the recipients, separated by commas.\n        subject (str): The subject of the email.\n        htmlMsg (str): The HTML content of the email body.\n\n        Returns:\n        The result of sending the email or an error message if the operation fails.\n        \"\"\"\n        self.authenticate()\n        try:\n            message = self.create_message(recipients, subject, htmlMsg)\n            sent_message = (self.service.users().messages().send(userId='me', body=message).execute())\n            return f\"Email sent successfully to {recipients}\"\n\n        except Exception as e:\n            return f\"An error occurred in sending the email: {e}\"\n        finally:\n            del self.service\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/ZapierHookTool.py",
        "content": "```swarmauri/community/tools/concrete/ZapierHookTool.py\nimport json\nimport requests\nfrom typing import Dict\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass ZapierHookTool(ToolBase):\n    def __init__(self, auth_token, zap_id):\n        parameters = [\n            Parameter(\n                name=\"payload\",\n                type=\"string\",\n                description=\"A Payload to send when triggering the Zapier webhook\",\n                required=True\n            )\n        ]\n        super().__init__(name=\"ZapierTool\", \n                         description=\"Tool to authenticate with Zapier and execute zaps.\", \n                        parameters=parameters)\n        self._auth_token = auth_token\n        self._zap_id = zap_id\n\n    def authenticate(self):\n        \"\"\"Set up the necessary headers for authentication.\"\"\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {self._auth_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def execute_zap(self, payload: str):\n        \"\"\"Execute a zap with given payload.\n\n        Args:\n            zap_id (str): The unique identifier for the Zap to trigger.\n            payload (dict): The data payload to send to the Zap.\n\n        Returns:\n            dict: The response from Zapier API.\n        \"\"\"\n        self.authenticate()\n        response = requests.post(f'https://hooks.zapier.com/hooks/catch/{self._zap_id}/',\n                                     headers=self.headers, json={\"data\":payload})\n        # Checking the HTTP response status for success or failure\n        if response.status_code == 200:\n            return json.dumps(response.json())\n        else:\n            response.raise_for_status()  # This will raise an error for non-200 responses\n\n    def __call__(self, payload: str):\n        \"\"\"Enable the tool to be called with zap_id and payload directly.\"\"\"\n        return self.execute_zap(payload)\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/DownloadPdfTool.py",
        "content": "```swarmauri/community/tools/concrete/DownloadPdfTool.py\nimport requests\nfrom typing import Dict\nfrom pathlib import Path\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass DownloadPDFTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"The URL of the PDF file to download\",\n                required=True\n            ),\n            Parameter(\n                name=\"destination\",\n                type=\"string\",\n                description=\"The path where the PDF file will be saved\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"DownloadPDFTool\",\n                         description=\"Downloads a PDF from a specified URL and saves it to a specified path.\",\n                         parameters=parameters)\n\n    def __call__(self, url: str, destination: str) -> Dict[str, str]:\n        \"\"\"\n        Download the PDF from the specified URL and saves it to the given destination path.\n\n        Parameters:\n        - url (str): The URL from where to download the PDF.\n        - destination (str): The local file path where the PDF should be saved.\n        \n        Returns:\n        - Dict[str, str]: A dictionary containing the result message and the destination path.\n        \"\"\"\n        try:\n            # Send a GET request to the specified URL\n            response = requests.get(url, stream=True)\n\n            # Raise an HTTPError if the status code is not 200 (OK)\n            response.raise_for_status()\n\n            # Ensure destination directory exists\n            Path(destination).parent.mkdir(parents=True, exist_ok=True)\n\n            # Open a file at the specified destination path and write the content of the response to it\n            with open(Path(destination), 'wb') as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n            \n            return {\n                \"message\": \"PDF downloaded successfully.\",\n                \"destination\": destination\n            }\n\n        except requests.exceptions.RequestException as e:\n            # Handle requests-related errors\n            return {\"error\": f\"Failed to download PDF: {e}\"}\n        except IOError as e:\n            # Handle file I/O errors\n            return {\"error\": f\"Failed to save PDF: {e}\"}\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/EntityRecognitionTool.py",
        "content": "```swarmauri/community/tools/concrete/EntityRecognitionTool.py\nimport json\nfrom transformers import pipeline, logging as hf_logging\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nhf_logging.set_verbosity_error()\n\nclass EntityRecognitionTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\"text\",\"string\",\"The text for entity recognition\",True)\n        ]\n        super().__init__(name=\"EntityRecognitionTool\", \n                         description=\"Extracts named entities from text\", \n                         parameters=parameters)\n        \n\n    def __call__(self, text: str) -> dict:\n        try:\n            self.nlp = pipeline(\"ner\")\n            entities = self.nlp(text)\n            organized_entities = {}\n            for entity in entities:\n                if entity['entity'] not in organized_entities:\n                    organized_entities[entity['entity']] = []\n                organized_entities[entity['entity']].append(entity['word'])\n            return json.dumps(organized_entities)\n        except Exception as e:\n            raise e\n        finally:\n            del self.nlp\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/GmailReadTool.py",
        "content": "```swarmauri/community/tools/concrete/GmailReadTool.py\nimport base64\nimport json\nfrom googleapiclient import discovery\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass GmailReadTool(ToolBase):\n    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n\n    def __init__(self, credentials_path: str, sender_email: str):\n        \"\"\"\n        Initializes the GmailReadTool with a path to the credentials JSON file.\n\n        Parameters:\n        credentials_path (str): The path to the Gmail service JSON file.\n        \"\"\"\n        \n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description='''The query to filter emails. For example, \"is:unread\" or \"from:example@gmail.com\" or \"from:sender@company.com\"''',\n                required=True\n            ),\n            Parameter(\n                name=\"max_results\",\n                type=\"integer\",\n                description='''The number of emails to return. Defaults to 10.'''\n            )\n        ]\n        \n        \n        super().__init__(name=\"GmailReadTool\", \n                         description=\"Read emails from a Gmail account.\", \n                         parameters = parameters)\n        self.credentials_path = credentials_path\n        self.sender_email = sender_email\n        \n\n    def authenticate(self):\n        \"\"\"\n        Authenticates the user and creates a Gmail API service.\n        \"\"\"\n        credentials = service_account.Credentials.from_service_account_file(\n                self.credentials_path, scopes=self.SCOPES)\n        \n        delegated_credentials = credentials.with_subject(self.sender_email)\n        self.service = discovery.build('gmail', 'v1', credentials=delegated_credentials)\n\n\n\n    def __call__(self, query='', max_results=10):\n        \"\"\"\n        Fetches emails from the authenticated Gmail account based on the given query.\n\n        Parameters:\n        query (str): The query to filter emails. For example, \"is:unread\".\n        max_results (int): The maximum number of email messages to fetch.\n\n        Returns:\n        list: A list of email messages.\n        \"\"\"\n        self.authenticate()\n        try:\n            # Call the Gmail API\n            \n            gmail_messages = self.service.users().messages()\n            results = gmail_messages.list(userId='me', q=query, maxResults=max_results).execute()\n            messages = results.get('messages', [])\n            message_data = \"\"\n            for message in messages:\n                \n                msg = gmail_messages.get(userId='me', id=message['threadId'], format=\"full\").execute()\n                headers = msg['payload']['headers']\n                \n                sender = next(header['value'] for header in headers if header['name'] == 'From')\n                subject = next(header['value'] for header in headers if header['name'] == 'Subject')\n                reply_to = next((header['value'] for header in headers if header['name'] == 'Reply-To'), subject)\n                date_time = next(header['value'] for header in headers if header['name'] == 'Date')\n                \n                #part = msg['payload']['parts'][0]\n                #data = part['body']['data']\n                #decoded_data = base64.urlsafe_b64decode(data.encode('ASCII'))\n\n                formatted_msg = f\"\\nsender:{sender} reply-to:{reply_to} subject: {subject} date_time:{date_time}\"\n                \n                message_data += formatted_msg\n                \n            \n            return message_data\n        \n        \n        \n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return []\n        \n        finally:\n            del self.service\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/PaCMAP.py",
        "content": "```swarmauri/community/tools/concrete/PaCMAP.py\nimport numpy as np\nimport pacmap  # Ensure pacmap is installed\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\nclass PaCMAPTool(ToolBase):\n    \"\"\"\n    A tool for applying the PaCMAP method for dimensionality reduction.\n    \"\"\"\n\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"X\",\n                type=\"object\",\n                description=\"X (np.ndarray): The high-dimensional data points to reduce.\",\n                required=True\n            ),\n            Parameter(\n                name=\"n_neighbors\",\n                type=\"integer\",\n                description=\"The size of local neighborhood (in terms of number of neighboring data points) used for manifold approximation.\",\n                required=False\n            ),\n            Parameter(\n                name=\"n_components\",\n                type=\"integer\",\n                description=\"The dimension of the space into which to embed the data.\",\n                required=True\n            ),\n            Parameter(\n                name=\"n_iterations\",\n                type=\"integer\",\n                description=\"The number of iterations used for optimization.\",\n                required=False\n            )\n        ]\n        \n        super().__init__(name=\"PaCMAPTool\", \n                         description=\"Applies PaCMAP for dimensionality reduction.\", \n                         parameters=parameters)\n\n    def __call__(self, **kwargs) -> np.ndarray:\n        \"\"\"\n        Applies the PaCMAP algorithm on the provided dataset.\n\n        Parameters:\n        - kwargs: Additional keyword arguments for the PaCMAP algorithm.\n\n        Returns:\n        - np.ndarray: The reduced dimension data points.\n        \"\"\"\n        # Set default values for any unspecified parameters\n        X = kwargs.get('X')\n        n_neighbors = kwargs.get('n_neighbors', 30)\n        n_components = kwargs.get('n_components', 2)\n        n_iterations = kwargs.get('n_iterations', 500)\n        \n        # Instantiate the PaCMAP instance with specified parameters\n        embedder = pacmap.PaCMAP(n_neighbors=n_neighbors, n_components=n_components, \n                                 n_iters=n_iterations, **kwargs)\n                                 \n        # Fit the model and transform the data\n        X_reduced = embedder.fit_transform(X)\n\n        return X_reduced\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/SentimentAnalysisTool.py",
        "content": "```swarmauri/community/tools/concrete/SentimentAnalysisTool.py\nfrom transformers import pipeline\nfrom transformers import logging as hf_logging\n\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nhf_logging.set_verbosity_error()\n\nclass SentimentAnalysisTool(ToolBase):\n    def __init__(self):\n        super().__init__(\"SentimentAnalysisTool\", \n                         \"Analyzes the sentiment of the given text.\", \n                         parameters=[\n                             Parameter(\"text\", \"string\", \"The text for sentiment analysis\", True)\n                         ])\n        \n\n    def __call__(self, text: str) -> str:\n        try:\n            self.analyzer = pipeline(\"sentiment-analysis\")\n            result = self.analyzer(text)\n            return result[0]['label']\n        except:\n            raise\n        finally:\n            del self.analyzer\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/WebScrapingTool.py",
        "content": "```swarmauri/community/tools/concrete/WebScrapingTool.py\nimport requests\nfrom bs4 import BeautifulSoup\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass WebScrapingTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"URL of the link, website, webpage, etc... to scrape\",\n                required=True\n            ),\n            Parameter(\n                name=\"selector\",\n                type=\"string\",\n                description=\"CSS selector to target specific elements\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"WebScrapingTool\", \n                         description=\"This is a web scraping tool that you can utilize to scrape links, websites, webpages, etc... This tool uses python's requests and BeautifulSoup libraries to parse a URL using a CSS to target specific elements.\", \n                         parameters=parameters)\n\n    def __call__(self, url: str, selector: str) -> str:\n        \"\"\"\n        Fetches content from the specified URL and extracts elements based on the provided CSS selector.\n        \n        Args:\n            url (str): The URL of the webpage to scrape.\n            selector (str): CSS selector to target specific elements in the webpage.\n\n        Returns:\n            str: Extracted text from the selector or an error message.\n        \"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raises HTTPError for bad requests (4xx or 5xx)\n\n            html_content = response.content\n            soup = BeautifulSoup(html_content, 'html.parser')\n\n            elements = soup.select(selector)\n            extracted_text = '\\n'.join([element.text for element in elements])\n            return extracted_text\n        except requests.RequestException as e:\n            return f\"Request Exception: {str(e)}\"\n        except Exception as e:\n            return f\"Unexpected error: {str(e)}\"\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/__init__.py",
        "content": "```swarmauri/community/retrievers/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/base/__init__.py",
        "content": "```swarmauri/community/retrievers/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/concrete/__init__.py",
        "content": "```swarmauri/community/retrievers/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/concrete/RedisDocumentRetriever.py",
        "content": "```swarmauri/community/retrievers/concrete/RedisDocumentRetriever.py\nfrom typing import List\nfrom redisearch import Client, Query\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.document_stores.concrete.ConcreteDocument import ConcreteDocument\nfrom ....standard.retrievers.base.DocumentRetrieverBase import DocumentRetrieverBase\n\nclass RedisDocumentRetriever(DocumentRetrieverBase):\n    \"\"\"\n    A document retriever that fetches documents from a Redis store.\n    \"\"\"\n    \n    def __init__(self, redis_idx_name, redis_host, redis_port):\n        \"\"\"\n        Initializes a new instance of RedisDocumentRetriever.\n\n        Args:\n            redis_client (Redis): An instance of the Redis client.\n        \"\"\"\n        self._redis_client = None\n        self._redis_idx_name = redis_idx_name\n        self._redis_host = redis_host\n        self._redis_port = redis_port\n\n    @property\n    def redis_client(self):\n        \"\"\"Lazily initialize and return the Redis client using a factory method.\"\"\"\n        if self._redis_client is None:\n            self._redis_client = Client(self.redis_idx_name, host=self.redis_host, port=self.redis_port)\n        return self._redis_client\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the most relevant documents based on the given query.\n        \n        Args:\n            query (str): The query string used for document retrieval.\n            top_k (int, optional): The number of top relevant documents to retrieve. Defaults to 5.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        query_result = self.redis_client.search(Query(query).paging(0, top_k))\n        \n        documents = [\n            ConcreteDocument(\n                doc_id=doc.id,\n                content=doc.text,  # Note: Adjust 'text' based on actual Redis document schema\n                metadata=doc.__dict__  # Including full document fields and values in metadata\n            )\n            for doc in query_result.docs\n        ]\n\n        return documents\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/__init__.py",
        "content": "```swarmauri/community/document_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/base/__init__.py",
        "content": "```swarmauri/community/document_stores/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/concrete/__init__.py",
        "content": "```swarmauri/community/document_stores/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/concrete/RedisDocumentStore.py",
        "content": "```swarmauri/community/document_stores/concrete/RedisDocumentStore.py\nfrom typing import List, Optional\nfrom ....standard.document_stores.base.DocumentStoreBase import DocumentStoreBase\nfrom ....core.documents.IDocument import IDocument\nimport redis\nimport json\nfrom redis.commands.search.field import TextField, NumericField, TagField\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\n\n\nclass RedisDocumentStore(DocumentStoreBase):\n    def __init__(self, host, password, port, db):\n        \"\"\"Store connection details without initializing the Redis client.\"\"\"\n        self._host = host\n        self._password = password\n        self._port = port\n        self._db = db\n        self._redis_client = None  # Delayed initialization\n\n    @property\n    def redis_client(self):\n        \"\"\"Lazily initialize and return the Redis client using a factory method.\"\"\"\n        if self._redis_client is None:\n            print('here')\n            self._redis_client = redis.Redis(host=self._host, \n                                             password=self._password, \n                                             port=self._port, \n                                             db=self._db)\n            print('there')\n        return self._redis_client\n\n    def add_document(self, document: IDocument) -> None:\n        \n        data = document.as_dict()\n        doc_id = data['id'] \n        del data['id']\n        self.redis_client.json().set(doc_id, '$', json.dumps(data))\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        with self.redis_client.pipeline() as pipe:\n            for document in documents:\n                pipe.set(document.doc_id, document)\n            pipe.execute()\n\n    def get_document(self, doc_id: str) -> Optional[IDocument]:\n        result = self.redis_client.json().get(doc_id)\n        if result:\n            return json.loads(result)\n        return None\n\n    def get_all_documents(self) -> List[IDocument]:\n        keys = self.redis_client.keys('*')\n        documents = []\n        for key in keys:\n            document_data = self.redis_client.get(key)\n            if document_data:\n                documents.append(json.loads(document_data))\n        return documents\n\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        self.add_document(updated_document)\n\n    def delete_document(self, doc_id: str) -> None:\n        self.redis_client.delete(doc_id)\n    \n    def __getstate__(self):\n        \"\"\"Return the object state for serialization, excluding the Redis client.\"\"\"\n        state = self.__dict__.copy()\n        state['_redis_client'] = None  # Exclude Redis client from serialization\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore the object state after serialization, reinitializing the Redis client.\"\"\"\n        self.__dict__.update(state)\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/AnnoyVectorStore.py",
        "content": "```swarmauri/community/vector_stores/AnnoyVectorStore.py\nimport os\nimport json\nimport pickle\nimport tempfile\nfrom typing import List, Union\nfrom annoy import AnnoyIndex\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.base.SaveLoadStoreBase import SaveLoadStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nfrom swarmauri.standard.vectorizers.concrete.Doc2VecVectorizer import Doc2VecVectorizer\n#from swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nclass AnnoyVectorStore(SaveLoadStoreBase, VectorDocumentStoreRetrieveBase):\n    \"\"\"\n    AnnoyVectorStore is a concrete implementation that integrates functionality\n    for saving, loading, storing, and retrieving vector documents, leveraging Annoy as the backend.\n    \"\"\"\n\n    def __init__(self, dimension: int, metric='euclidean', num_trees=10):\n        self.dimension = dimension\n        self.vectorizer = Doc2VecVectorizer()\n        self.metric = metric\n        self.num_trees = num_trees\n        self.index = AnnoyIndex(dimension, metric)\n        self.documents = []  # List of documents\n        self.id_to_index = {}  # Mapping from document ID to index in Annoy\n        SaveLoadStoreBase.__init__(self, self.vectorizer, [])\n\n    def get_state(self) -> dict:\n        \"\"\"\n        Retrieve the internal state of the vector store to be saved.\n        \n        Returns:\n            dict: The internal state of the vector store.\n        \"\"\"\n        return {\n            'documents': [doc.to_dict() for doc in self.documents],\n            'id_to_index': self.id_to_index\n        }\n\n    def set_state(self, state: dict) -> None:\n        \"\"\"\n        Set the internal state of the vector store when loading.\n        \n        Parameters:\n            state (dict): The state to set to the vector store.\n        \"\"\"\n        self.documents = [Document.from_dict(doc_dict) for doc_dict in state.get('documents', [])]\n        self.id_to_index = state['id_to_index']\n        for idx, document in enumerate(self.documents):\n            self.index.add_item(idx, document.content)\n        self.index.build(self.num_trees)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Add a single document to the document store.\n        \n        Parameters:\n            document (IDocument): The document to be added to the store.\n        \"\"\"\n        index = len(self.documents)\n        self.documents.append(document)\n        self.index.add_item(index, document.content)\n        self.id_to_index[document.id] = index\n        try:\n            self.index.build(self.num_trees)\n        except Exception as e:\n            self._rebuild_index()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n        \n        Parameters:\n            documents (List[IDocument]): A list of documents to be added to the store.\n        \"\"\"\n        start_idx = len(self.documents)\n        self.documents.extend(documents)\n        for i, doc in enumerate(documents):\n            idx = start_idx + i\n            self.index.add_item(idx, doc.content)\n            self.id_to_index[doc.id] = idx\n        try:\n            self.index.build(self.num_trees)\n        except Exception as e:\n            self._rebuild_index()\n\n    def get_document(self, id: str) -> Union[IDocument, None]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to retrieve.\n        \n        Returns:\n            Union[IDocument, None]: The requested document if found; otherwise, None.\n        \"\"\"\n        index = self.id_to_index.get(id)\n        if index is not None:\n            return self.documents[index]\n        return None\n\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n        \n        Returns:\n            List[IDocument]: A list of all documents in the store.\n        \"\"\"\n        return self.documents\n\n    def delete_document(self, id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to delete.\n        \"\"\"\n        if id in self.id_to_index:\n            index = self.id_to_index.pop(id)\n            self.documents.pop(index)\n            self._rebuild_index()\n\n    def update_document(self, id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Update a document in the document store.\n        \n        Parameters:\n            id (str): The unique identifier of the document to update.\n            updated_document (IDocument): The updated document instance.\n        \"\"\"\n        if id in self.id_to_index:\n            index = self.id_to_index[id]\n            self.documents[index] = updated_document\n            self._rebuild_index()\n\n    def clear_documents(self) -> None:\n        \"\"\"\n        Deletes all documents from the vector store\n        \"\"\"\n        self.documents = []\n        self.doc_id_to_index = {}\n        self.index = AnnoyIndex(self.dimension, self.metric)\n\n    def document_count(self) -> int:\n        \"\"\"\n        Returns the number of documents in the store.\n        \"\"\"\n        return len(self.documents)\n\n    def retrieve(self, query: List[float], top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        \n        Args:\n            query (List[float]): The content of the document for retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        indices = self.index.get_nns_by_vector(query, top_k, include_distances=False)\n        return [self.documents[idx] for idx in indices]\n\n    def save_store(self, directory_path: str) -> None:\n        \"\"\"\n        Saves the state of the vector store to the specified directory. This includes\n        both the vectorizer's model and the stored documents or vectors.\n\n        Parameters:\n            directory_path (str): The directory path where the store's state will be saved.\n        \"\"\"\n        state = self.get_state()\n        os.makedirs(directory_path, exist_ok=True)\n        state_file = os.path.join(directory_path, 'store_state.json')\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n\n        with open(state_file, 'w') as f:\n            json.dump(state, f, indent=4)\n        self.index.save(index_file)\n\n    def load_store(self, directory_path: str) -> None:\n        \"\"\"\n        Loads the state of the vector store from the specified directory. This includes\n        both the vectorizer's model and the stored documents or vectors.\n\n        Parameters:\n            directory_path (str): The directory path from where the store's state will be loaded.\n        \"\"\"\n        state_file = os.path.join(directory_path, 'store_state.json')\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n\n        with open(state_file, 'r') as f:\n            state = json.load(f)\n        self.set_state(state)\n        self.index.load(index_file)\n\n    def save_parts(self, directory_path: str, chunk_size: int = 10485760) -> None:\n        \"\"\"\n        Save the model in parts to handle large files by splitting them.\n        \"\"\"\n        state = self.get_state()\n        os.makedirs(directory_path, exist_ok=True)\n        temp_state_file = tempfile.NamedTemporaryFile(delete=False)\n\n        try:\n            pickle.dump(state, temp_state_file)\n            temp_state_file.close()\n\n            with open(temp_state_file.name, 'rb') as src:\n                part_num = 0\n                while True:\n                    chunk = src.read(chunk_size)\n                    if not chunk:\n                        break\n                    with open(os.path.join(directory_path, f'state_part_{part_num}.pkl'), 'wb') as dest:\n                        dest.write(chunk)\n                    part_num += 1\n        finally:\n            os.remove(temp_state_file.name)\n\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n        self.index.save(index_file)\n\n        with open(index_file, 'rb') as src:\n            part_num = 0\n            while True:\n                chunk = src.read(chunk_size)\n                if not chunk:\n                    break\n                with open(os.path.join(directory_path, f'index_part_{part_num}.ann'), 'wb') as dest:\n                    dest.write(chunk)\n                part_num += 1\n\n    def load_parts(self, directory_path: str, state_file_pattern: str, index_file_pattern: str) -> None:\n        \"\"\"\n        Load and combine model parts from a directory.\n        \"\"\"\n        temp_state_file = tempfile.NamedTemporaryFile(delete=False)\n        try:\n            with open(temp_state_file.name, 'ab') as dest:\n                part_num = 0\n                while True:\n                    part_file_path = os.path.join(directory_path, state_file_pattern.format(part_num))\n                    if not os.path.isfile(part_file_path):\n                        break\n                    with open(part_file_path, 'rb') as src:\n                        chunk = src.read()\n                        dest.write(chunk)\n                    part_num += 1\n\n            with open(temp_state_file.name, 'rb') as src:\n                state = pickle.load(src)\n            self.set_state(state)\n        finally:\n            os.remove(temp_state_file.name)\n\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n        self.index.load(index_file)\n\n    def _rebuild_index(self):\n        \"\"\"\n        Rebuild the Annoy index from the current documents.\n        \"\"\"\n        self.index = AnnoyIndex(self.dimension, self.metric)\n        for idx, document in enumerate(self.documents):\n            self.index.add_item(idx, document.content)\n        self.index.build(self.num_trees)\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/ChromadbVectorStore.py",
        "content": "```swarmauri/community/vector_stores/ChromadbVectorStore.py\nimport os\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.vector_stores.base.SaveLoadStoreBase import SaveLoadStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nfrom swarmauri.standard.vectorizers.concrete.Doc2VecVectorizer import Doc2VecVectorizer\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.documents.concrete.Document import Document\nimport chromadb\n\nclass ChromaDBVectorStore(VectorDocumentStoreRetrieveBase, SaveLoadStoreBase):\n    def __init__(self, db_name):\n        self.vectorizer = Doc2VecVectorizer()\n        self.metric = CosineDistance()\n        self.db_name = db_name\n        self.client = chromadb.Client()\n        self.collection = self.client.get_or_create_collection(name=db_name)\n        SaveLoadStoreBase.__init__(self, self.vectorizer, [])\n\n    def add_document(self, document: IDocument) -> None:\n        try:\n            embedding = self.vectorizer.infer_vector(document.content).data\n            self.collection.add(ids=[document.id],\n                    documents=[document.content], \n                    embeddings=[embedding], \n                    metadatas=[document.metadata] )\n        except:\n            texts = [document.content]\n            self.vectorizer.fit_transform(texts)\n            embedding = self.vectorizer.infer_vector(document.content).data\n            self.collection.add(ids=[document.id],\n                                documents=[document.content], \n                                embeddings=[embedding], \n                                metadatas=[document.metadata] )\n            \n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        ids = [doc.id for doc in documents]\n        texts = [doc.content for doc in documents]\n        embeddings = [self.vectorizer.infer_vector(doc.content).data for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        \n        self.collection.add(ids=ids,\n                            documents=texts, \n                            embeddings=embeddings, \n                            metadatas=metadatas)\n\n    def get_document(self, doc_id: str) -> Union[IDocument, None]:\n        try:\n            results = self.collection.get(ids=[doc_id])\n            document = Document(id=results['ids'][0],\n                             content=results['documents'][0],\n                                 metadata=results['metadatas'][0])\n        except Exception as e:\n            print(str(e))\n            document = None\n        return document if document else []\n\n    def get_all_documents(self) -> List[IDocument]:\n        try:\n            results = self.collection.get()\n            print(results)\n            return [Document(id=results['ids'][idx],\n                                 content=results['documents'][idx],\n                                 metadata=results['metadatas'][idx])\n                    for idx, value in enumerate(results['ids'])]\n        except Exception as e:\n            print(str(e))\n            document = None\n        return document if document else []\n            \n\n    def delete_document(self, doc_id: str) -> None:\n        self.collection.delete(ids=[doc_id])\n\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        self.delete_document(doc_id)\n        self.add_document(updated_document)\n\n    def clear_documents(self) -> None:\n        self.client.delete_collection(self.db_name)\n\n    def document_count(self) -> int:\n        try:\n            return len(self.get_all_documents())\n        except StopIteration:\n            return 0\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        embedding = self.vectorizer.infer_vector(query).data\n        results = self.collection.query(query_embeddings=embedding,\n                                        n_results=top_k)\n        print('retrieve reults', results)\n        print(results['ids'][0])\n        documents = []\n        for idx in range(len(results['ids'])):\n            documents.append(Document(id=results['ids'][idx],\n                             content=results['documents'][idx],\n                             metadata=results['metadatas'][idx]))\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/QdrantVectorStore.py",
        "content": "```swarmauri/community/vector_stores/QdrantVectorStore.py\nimport os\nimport json\nimport pickle\nimport tempfile\nfrom typing import List, Union, Literal\nfrom qdrant_client import QdrantClient, models\n\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.embeddings.concrete.Doc2VecEmbedding import Doc2VecEmbedding\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import VectorStoreRetrieveMixin\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import VectorStoreSaveLoadMixin    \n\n\nclass QdrantVectorStore(VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase):\n    \"\"\"\n    QdrantVectorStore is a concrete implementation that integrates functionality\n    for saving, loading, storing, and retrieving vector documents, leveraging Qdrant as the backend.\n    \"\"\"\n    type: Literal['QdrantVectorStore'] = 'QdrantVectorStore'\n\n    def __init__(self, url: str, api_key: str, collection_name: str, vector_size: int):\n        self.vectorizer = Doc2VecEmbedding(vector_size=vector_size)\n        self.metric = CosineDistance()\n        self.client = QdrantClient(url=url, api_key=api_key)\n        self.collection_name = collection_name\n        exists = self.client.collection_exists(collection_name)\n        \n        if not exists:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n            )   \n\n\n    def add_document(self, document: Document) -> None:\n        \"\"\"\n        Add a single document to the document store.\n        \n        Parameters:\n            document (Document): The document to be added to the store.\n        \"\"\"\n        try:\n            embedding = document.embedding or self.vectorizer.fit_transform(document.content).data \n            self.client.upsert(self.collection_name, points=[\n                models.PointStruct(\n                    id=document.id,\n                    vector=embedding,\n                    payload=document.metadata\n                )\n            ])\n            \n        except:\n            embedding = document.embedding or self.vectorizer.fit_transform(document.content).data \n            self.client.upsert(self.collection_name, points=[\n                models.PointStruct(\n                    id=document.id,\n                    vector=embedding,\n                    payload=document.metadata\n                )\n            ])\n            \n        \n\n    def add_documents(self, documents: List[Document]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n        \n        Parameters:\n            documents (List[Document]): A list of documents to be added to the store.\n        \"\"\"\n        self.vectorizer.fit_transform([doc.content for doc in documents])\n        points = [\n            models.PointStruct(\n                id=doc.id,\n                vector=doc.embedding or self.vectorizer.infer_vector(doc.content).data,\n                payload=doc.metadata\n            ) for doc in documents\n        ]\n        self.client.upsert(self.collection_name, points=points)\n\n    def get_document(self, id: str) -> Union[Document, None]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to retrieve.\n        \n        Returns:\n            Union[Document, None]: The requested document if found; otherwise, None.\n        \"\"\"\n        \n        raise NotImplementedError('Get document not implemented, use retrieve().')\n\n    def get_all_documents(self) -> List[Document]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n        \n        Returns:\n            List[Document]: A list of all documents in the store.\n        \"\"\"\n        raise NotImplementedError('Get all documents not implemented, use retrieve().')\n\n    def delete_document(self, id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to delete.\n        \"\"\"\n        self.client.delete(self.collection_name, points_selector=[id])\n\n    def update_document(self, id: str, updated_document: Document) -> None:\n        \"\"\"\n        Update a document in the document store.\n        \n        Parameters:\n            id (str): The unique identifier of the document to update.\n            updated_document (Document): The updated document instance.\n        \"\"\"\n        self.client.upsert(self.collection_name, points=[                           \n            models.PointStruct(\n                id=updated_document.id,\n                vector=updated_document.embedding,\n                payload=updated_document.metadata\n            )\n        ])\n\n    def clear_documents(self) -> None:\n        \"\"\"\n        Deletes all documents from the vector store\n        \"\"\"\n        self.documents = []\n        self.client.delete(self.collection_name, points_selector=models.FilterSelector())\n\n    def document_count(self) -> int:\n        \"\"\"\n        Returns the number of documents in the store.\n        \"\"\"\n        raise NotImplementedError('Get document not implemeneted, use retrieve().')\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        For the purpose of this example, this method performs a basic search.\n        \n        Args:\n            query (str): The query string used for document retrieval. \n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[Document]: A list of the top_k most relevant documents.\n        \"\"\"\n        # This should be modified to a query to the Qdrant service for relevance search\n        query_vector = self.vectorizer.infer_vector(query).data\n        documents = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_vector,\n            limit=top_k)\n        \n        matching_documents = [\n            doc.payload for doc in documents\n        ]\n        return matching_documents[:top_k]\n\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/__init__.py",
        "content": "```swarmauri/community/vector_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/__init__.py",
        "content": "```swarmauri/core/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/ComponentBase.py",
        "content": "```swarmauri/core/ComponentBase.py\nfrom typing import (\n    Optional, \n    List,\n    Literal, \n    TypeVar, \n    Type, \n    Union, \n    Annotated, \n    Generic, \n    ClassVar, \n    Set,\n    get_args)\n\nfrom uuid import uuid4\nfrom enum import Enum\nimport inspect\nimport hashlib\nfrom pydantic import BaseModel, Field, field_validator\nimport logging\nfrom swarmauri.core.typing import SubclassUnion\n\nclass ResourceTypes(Enum):\n    UNIVERSAL_BASE = 'ComponentBase'\n    AGENT = 'Agent'\n    AGENT_FACTORY = 'AgentFactory'\n    CHAIN = 'Chain'\n    CHAIN_METHOD = 'ChainMethod'\n    CHUNKER = 'Chunker'\n    CONVERSATION = 'Conversation'\n    DISTANCE = 'Distance'\n    DOCUMENT_STORE = 'DocumentStore'\n    DOCUMENT = 'Document'\n    EMBEDDING = 'Embedding'\n    EXCEPTION = 'Exception'\n    LLM = 'LLM'\n    MESSAGE = 'Message'\n    METRIC = 'Metric'\n    PARSER = 'Parser'\n    PROMPT = 'Prompt'\n    STATE = 'State'\n    CHAINSTEP = 'ChainStep'\n    SCHEMA_CONVERTER = 'SchemaConverter'\n    SWARM = 'Swarm'\n    TOOLKIT = 'Toolkit'\n    TOOL = 'Tool'\n    PARAMETER = 'Parameter'\n    TRACE = 'Trace'\n    UTIL = 'Util'\n    VECTOR_STORE = 'VectorStore'\n    VECTOR = 'Vector'\n\ndef generate_id() -> str:\n    return str(uuid4())\n\nclass ComponentBase(BaseModel):\n    name: Optional[str] = None\n    id: str = Field(default_factory=generate_id)\n    members: List[str] = Field(default_factory=list)\n    owner: Optional[str] = None\n    host: Optional[str] = None\n    resource: str = Field(default=\"ComponentBase\")\n    version: str = \"0.1.0\"\n    __swm_subclasses__: ClassVar[Set[Type['ComponentBase']]] = set()\n    type: Literal['ComponentBase'] = 'ComponentBase'\n    \n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        ComponentBase.__swm_register_subclass__(cls)\n    \n    # @classmethod\n    # def __swm__get_subclasses__(cls) -> set:\n    #     logging.debug('__swm__get_subclasses__ executed\\n')\n    #     def is_excluded_module(module_name: str) -> bool:\n    #         return (module_name == 'builtins' or \n    #                 module_name == 'types')\n\n    #     subclasses_dict = {cls.__name__: cls}\n    #     for subclass in cls.__subclasses__():\n    #         if not is_excluded_module(subclass.__module__):\n    #             subclasses_dict.update({_s.__name__: _s for _s in subclass.__swm__get_subclasses__() \n    #                 if not is_excluded_module(_s.__module__)})\n\n    #     return set(subclasses_dict.values())\n    \n    @classmethod\n    def __swm_register_subclass__(cls, subclass):\n        logging.debug('__swm_register_subclass__ executed\\n')\n        \n        if 'type' in subclass.__annotations__:\n            sub_type = subclass.__annotations__['type']\n            if sub_type not in [subclass.__annotations__['type'] for subclass in cls.__swm_subclasses__]:\n                cls.__swm_subclasses__.add(subclass)\n        else:\n            logging.warning(f'Subclass {subclass.__name__} does not have a type annotation')\n\n\n        # [subclass.__swm_reset_class__()  for subclass in cls.__swm_subclasses__ \n        #  if hasattr(subclass, '__swm_reset_class__')]\n    \n    \n    @classmethod\n    def __swm_reset_class__(cls):\n        logging.debug('__swm_reset_class__ executed\\n')\n        for each in cls.__fields__:\n            logging.debug(each, cls.__fields__[each].discriminator)\n            if (cls.__fields__[each].discriminator and each in cls.__annotations__\n               ):\n                if len(get_args(cls.__fields__[each].annotation)) > 0:\n                    for x in range(0, len(get_args(cls.__fields__[each].annotation))):\n                        if hasattr(get_args(cls.__fields__[each].annotation)[x], '__base__'):\n                            if (hasattr(get_args(cls.__fields__[each].annotation)[x].__base__, '__swm_subclasses__') and\n                            not get_args(cls.__fields__[each].annotation)[x].__base__.__name__ == 'ComponentBase'):\n\n                                baseclass = get_args(cls.__fields__[each].annotation)[x].__base__\n         \n                                sc = SubclassUnion[baseclass]\n                                \n                                cls.__annotations__[each] = sc\n                                cls.__fields__[each].annotation = sc\n\n        \n        # This is not necessary as the model_rebuild address forward_refs \n        # https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_post_init\n        # cls.update_forward_refs() \n        cls.model_rebuild(force=True)\n\n\n    @field_validator('type')\n    def set_type(cls, v, values):\n        if v == 'ComponentBase' and cls.__name__ != 'ComponentBase':\n            return cls.__name__\n        return v\n\n    def __swm_class_hash__(self):\n        sig_hash = hashlib.sha256()\n        for attr_name in dir(self):\n            attr_value = getattr(self, attr_name)\n            if callable(attr_value) and not attr_name.startswith(\"_\"):\n                sig = inspect.signature(attr_value)\n                sig_hash.update(str(sig).encode())\n        return sig_hash.hexdigest()\n\n    @classmethod\n    def swm_public_interfaces(cls):\n        methods = []\n        for attr_name in dir(cls):\n            attr_value = getattr(cls, attr_name)\n            if (callable(attr_value) and not attr_name.startswith(\"_\")) or isinstance(attr_value, property):\n                methods.append(attr_name)\n        return methods\n\n    @classmethod\n    def swm_ismethod_registered(cls, method_name: str):\n        return method_name in cls.public_interfaces()\n\n    @classmethod\n    def swm_method_signature(cls, input_signature):\n        for method_name in cls.public_interfaces():\n            method = getattr(cls, method_name)\n            if callable(method):\n                sig = str(inspect.signature(method))\n                if sig == input_signature:\n                    return True\n        return False\n\n    @property\n    def swm_path(self):\n        if self.host and self.owner:\n            return f\"{self.host}/{self.owner}/{self.resource}/{self.name}/{self.id}\"\n        if self.resource and self.name:\n            return f\"/{self.resource}/{self.name}/{self.id}\"\n        return f\"/{self.resource}/{self.id}\"\n\n    @property\n    def swm_isremote(self):\n        return bool(self.host)\n```"
    },
    {
        "document_name": "swarmauri/core/typing.py",
        "content": "```swarmauri/core/typing.py\nimport logging\nfrom pydantic import BaseModel, Field\nfrom typing import TypeVar, Generic, Union, Annotated, Type\n\n\nclass SubclassUnion:\n\n    @classmethod\n    def __class_getitem__(cls, baseclass):\n        subclasses = cls.__swm__get_subclasses__(baseclass)\n        return Union[tuple(subclasses)]\n\n    @classmethod\n    def __swm__get_subclasses__(cls, baseclass) -> set:\n        logging.debug('__swm__get_subclasses__ executed\\n')\n        def is_excluded_module(module_name: str) -> bool:\n            return (module_name == 'builtins' or \n                    module_name == 'types')\n\n        subclasses_dict = {baseclass.__name__: baseclass}\n        for subclass in baseclass.__subclasses__():\n            if not is_excluded_module(subclass.__module__):\n                subclasses_dict.update({_s.__name__: _s for _s in cls.__swm__get_subclasses__(subclass) \n                    if not is_excluded_module(_s.__module__)})\n\n        return set(subclasses_dict.values())\n```"
    },
    {
        "document_name": "swarmauri/core/llms/IFit.py",
        "content": "```swarmauri/core/llms/IFit.py\nfrom abc import ABC, abstractmethod\n\nclass IFit(ABC):\n    \"\"\"\n    Interface for training models.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X_train, y_train, epochs: int, batch_size: int) -> None:\n        \"\"\"\n        Train the model on the provided dataset.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/llms/IPredict.py",
        "content": "```swarmauri/core/llms/IPredict.py\nfrom abc import ABC, abstractmethod\n\nclass IPredict(ABC):\n    \"\"\"\n    Interface focusing on the basic properties and settings essential for defining models.\n    \"\"\"\n\n    @abstractmethod\n    def predict(self, *args, **kwargs) -> any:\n        \"\"\"\n        Generate predictions based on the input data provided to the model.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/llms/__init__.py",
        "content": "```swarmauri/core/llms/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/agent_apis/__init__.py",
        "content": "```swarmauri/core/agent_apis/__init__.py\nfrom .IAgentCommands import IAgentCommands\nfrom .IAgentRouterCRUD import IAgentRouterCRUD\n\n__all__ = ['IAgentCommands', 'IAgentRouterCRUD']\n```"
    },
    {
        "document_name": "swarmauri/core/agent_apis/IAgentRouterCRUD.py",
        "content": "```swarmauri/core/agent_apis/IAgentRouterCRUD.py\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Any, Dict\n\nclass IAgentRouterCRUD(ABC):\n    \"\"\"\n    Interface for managing API routes within a SwarmAgent.\n    \"\"\"\n    \n    @abstractmethod\n    def create_route(self, path: str, method: str, handler: Callable[[Any], Any]) -> None:\n        \"\"\"\n        Create a new route for the API.\n        \n        Parameters:\n        - path (str): The URL path for the route.\n        - method (str): The HTTP method (e.g., 'GET', 'POST').\n        - handler (Callable[[Any], Any]): The function that handles requests to this route.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def read_route(self, path: str, method: str) -> Dict:\n        \"\"\"\n        Retrieve information about a specific route.\n        \n        Parameters:\n        - path (str): The URL path for the route.\n        - method (str): The HTTP method.\n        \n        Returns:\n        - Dict: Information about the route, including path, method, and handler.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def update_route(self, path: str, method: str, new_handler: Callable[[Any], Any]) -> None:\n        \"\"\"\n        Update the handler function for an existing route.\n        \n        Parameters:\n        - path (str): The URL path for the route.\n        - method (str): The HTTP method.\n        - new_handler (Callable[[Any], Any]): The new function that handles requests to this route.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def delete_route(self, path: str, method: str) -> None:\n        \"\"\"\n        Delete a specific route from the API.\n        \n        Parameters:\n        - path (str): The URL path for the route.\n        - method (str): The HTTP method.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/agent_apis/IAgentCommands.py",
        "content": "```swarmauri/core/agent_apis/IAgentCommands.py\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Any, List\n\nclass IAgentCommands(ABC):\n    \"\"\"\n    Interface for the API object that enables a SwarmAgent to host various API routes.\n    \"\"\"\n\n\n    @abstractmethod\n    def invoke(self, request: Any) -> Any:\n        \"\"\"\n        Handles invocation requests synchronously.\n        \n        Parameters:\n            request (Any): The incoming request payload.\n\n        Returns:\n            Any: The response payload.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def ainvoke(self, request: Any) -> Any:\n        \"\"\"\n        Handles invocation requests asynchronously.\n        \n        Parameters:\n            request (Any): The incoming request payload.\n\n        Returns:\n            Any: The response payload.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def batch(self, requests: List[Any]) -> List[Any]:\n        \"\"\"\n        Handles batched invocation requests synchronously.\n        \n        Parameters:\n            requests (List[Any]): A list of incoming request payloads.\n\n        Returns:\n            List[Any]: A list of responses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def abatch(self, requests: List[Any]) -> List[Any]:\n        \"\"\"\n        Handles batched invocation requests asynchronously.\n\n        Parameters:\n            requests (List[Any]): A list of incoming request payloads.\n\n        Returns:\n            List[Any]: A list of responses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def stream(self, request: Any) -> Any:\n        \"\"\"\n        Handles streaming requests.\n\n        Parameters:\n            request (Any): The incoming request payload.\n\n        Returns:\n            Any: A streaming response.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_schema_config(self) -> dict:\n        \"\"\"\n        Retrieves the schema configuration for the API.\n\n        Returns:\n            dict: The schema configuration.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/conversations/__init__.py",
        "content": "```swarmauri/core/conversations/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/conversations/IMaxSize.py",
        "content": "```swarmauri/core/conversations/IMaxSize.py\nfrom abc import ABC, abstractmethod\n\nclass IMaxSize(ABC):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/conversations/ISystemContext.py",
        "content": "```swarmauri/core/conversations/ISystemContext.py\nfrom abc import ABC, abstractmethod\n\nclass ISystemContext(ABC):\n    pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/conversations/IConversation.py",
        "content": "```swarmauri/core/conversations/IConversation.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom swarmauri.core.messages.IMessage import IMessage\n\n\nclass IConversation(ABC):\n    \"\"\"\n    Interface for managing conversations, defining abstract methods for\n    adding messages, retrieving the latest message, getting all messages, and clearing history.\n    \"\"\"\n\n    @property\n    def history(self) -> List[IMessage]:\n        \"\"\"\n        Provides read-only access to the conversation history.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_messages(self, messages: List[IMessage]):\n        \"\"\"\n        Adds multiple messages to the conversation history.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_last(self) -> Optional[IMessage]:\n        \"\"\"\n        Retrieves the latest message from the conversation history.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def clear_history(self) -> None:\n        \"\"\"\n        Clears the conversation history.\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/documents/__init__.py",
        "content": "```swarmauri/core/documents/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/documents/IExperimentDocument.py",
        "content": "```swarmauri/core/documents/IExperimentDocument.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass IExperimentDocument(IDocument, ABC):\n    \"\"\"\n    Interface for an Experiment Document, extending the general IDocument interface\n    with additional properties and methods specific to experimental data.\n    \"\"\"\n    @property\n    @abstractmethod\n    def parameters(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the parameters used in the experiment.\n        \"\"\"\n        pass\n\n    @parameters.setter\n    @abstractmethod\n    def parameters(self, value: Dict[str, Any]) -> None:\n        \"\"\"\n        Set the parameters used in the experiment.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def results(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the results obtained from the experiment.\n        \"\"\"\n        pass\n\n    @results.setter\n    @abstractmethod\n    def results(self, value: Dict[str, Any]) -> None:\n        \"\"\"\n        Set the results obtained from the experiment.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def instruction(self) -> str:\n        \"\"\"\n        An instructional or descriptive text about what the experiment aims to achieve or how.\n        \"\"\"\n        pass\n\n    @instruction.setter\n    @abstractmethod\n    def instruction(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def feature_set(self) -> List[Any]:\n        \"\"\"\n        Description of the set of features or data used in the experiment.\n        \"\"\"\n        pass\n\n    @feature_set.setter\n    @abstractmethod\n    def feature_set(self, value: List[Any]) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def version(self) -> str:\n        \"\"\"\n        The version of the experiment, useful for tracking iterations and changes over time.\n        \"\"\"\n        pass\n\n    @version.setter\n    @abstractmethod\n    def version(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def artifacts(self) -> List[str]:\n        \"\"\"\n        A list of paths or identifiers for any artifacts generated by the experiment,\n        such as models, charts, or data dumps.\n        \"\"\"\n        pass\n\n    @artifacts.setter\n    @abstractmethod\n    def artifacts(self, value: List[str]) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def datetime_created(self) -> datetime:\n        \"\"\"\n        Timestamp marking when the experiment was initiated or created.\n        \"\"\"\n        pass\n\n    @datetime_created.setter\n    @abstractmethod\n    def datetime_created(self, value: datetime) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def datetime_completed(self) -> Optional[datetime]:\n        \"\"\"\n        Timestamp of when the experiment was completed. None if the experiment is still running.\n        \"\"\"\n        pass\n\n    @datetime_completed.setter\n    @abstractmethod\n    def datetime_completed(self, value: Optional[datetime]) -> None:\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/documents/IDocument.py",
        "content": "```swarmauri/core/documents/IDocument.py\nfrom abc import ABC\n\nclass IDocument(ABC):\n   pass\n```"
    },
    {
        "document_name": "swarmauri/core/messages/__init__.py",
        "content": "```swarmauri/core/messages/__init__.py\nfrom .IMessage import IMessage\n```"
    },
    {
        "document_name": "swarmauri/core/messages/IMessage.py",
        "content": "```swarmauri/core/messages/IMessage.py\nfrom abc import ABC, abstractmethod\n\nclass IMessage(ABC):\n    \"\"\"\n    An abstract interface representing a general message structure.\n\n    This interface defines the basic attributes that all\n    messages should have, including type, name, and content, \n    and requires subclasses to implement representation and formatting methods.\n    \"\"\"\n\n```"
    },
    {
        "document_name": "swarmauri/core/parsers/__init__.py",
        "content": "```swarmauri/core/parsers/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/parsers/IParser.py",
        "content": "```swarmauri/core/parsers/IParser.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union, Any\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass IParser(ABC):\n    \"\"\"\n    Abstract base class for parsers. It defines a public method to parse input data (str or Message) into documents,\n    and relies on subclasses to implement the specific parsing logic through protected and private methods.\n    \"\"\"\n\n    @abstractmethod\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Public method to parse input data (either a str or a Message) into a list of Document instances.\n        \n        This method leverages the abstract _parse_data method which must be\n        implemented by subclasses to define specific parsing logic.\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/prompts/__init__.py",
        "content": "```swarmauri/core/prompts/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/prompts/IPrompt.py",
        "content": "```swarmauri/core/prompts/IPrompt.py\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Any\n\nclass IPrompt(ABC):\n    \"\"\"\n    A base abstract class representing a prompt system.\n\n    Methods:\n        __call__: Abstract method that subclasses must implement to enable the instance to be called directly.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> str:\n        \"\"\"\n        Abstract method that subclasses must implement to define the behavior of the prompt when called.\n\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/prompts/IPromptMatrix.py",
        "content": "```swarmauri/core/prompts/IPromptMatrix.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple, Optional, Any\n\nclass IPromptMatrix(ABC):\n\n    @property\n    @abstractmethod\n    def shape(self) -> Tuple[int, int]:\n        \"\"\"Get the shape (number of agents, sequence length) of the prompt matrix.\"\"\"\n        pass\n\n    @abstractmethod\n    def add_prompt_sequence(self, sequence: List[Optional[str]]) -> None:\n        \"\"\"Add a new prompt sequence to the matrix.\"\"\"\n        pass\n\n    @abstractmethod\n    def remove_prompt_sequence(self, index: int) -> None:\n        \"\"\"Remove a prompt sequence from the matrix by index.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_prompt_sequence(self, index: int) -> List[Optional[str]]:\n        \"\"\"Get a prompt sequence from the matrix by index.\"\"\"\n        pass\n\n    @abstractmethod\n    def show(self) -> List[List[Optional[str]]]:\n        \"\"\"Show the entire prompt matrix.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/prompts/ITemplate.py",
        "content": "```swarmauri/core/prompts/ITemplate.py\nfrom typing import Dict, List, Any, Union\nfrom abc import ABC, abstractmethod\n\n\nclass ITemplate(ABC):\n    \"\"\"\n    Interface for template-based prompt generation within the SwarmAURI framework.\n    Defines standard operations and attributes for managing and utilizing templates.\n    \"\"\"\n    \n    @abstractmethod\n    def set_template(self, template: str) -> None:\n        \"\"\"\n        Sets or updates the current template string.\n\n        Args:\n            template (str): The new template string to be used for generating prompts.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def set_variables(self, \n                      variables: Union[List[Dict[str, Any]], Dict[str, Any]] = {}) -> None:\n        \"\"\"\n        Sets or updates the variables to be substituted into the template.\n\n        Args:\n            variables (List[Dict[str, str]]): A dictionary of variables where each key-value \n                                        pair corresponds to a placeholder name and its \n                                        replacement value in the template.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_prompt(self, **kwargs) -> str:\n        \"\"\"\n        Generates a prompt string based on the current template and provided keyword arguments.\n\n        Args:\n            **kwargs: Keyword arguments containing variables for template substitution. \n\n        Returns:\n            str: The generated prompt string with template variables replaced by their\n                 corresponding values provided in `kwargs`.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/agents/__init__.py",
        "content": "```swarmauri/core/agents/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgentParser.py",
        "content": "```swarmauri/core/agents/IAgentParser.py\nfrom abc import ABC, abstractmethod\nfrom swarmauri.core.parsers.IParser import IParser \n\nclass IAgentParser(ABC):\n    \n    @property\n    @abstractmethod\n    def parser(self) -> IParser:\n        pass\n\n    @parser.setter\n    @abstractmethod\n    def parser(self) -> IParser:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgent.py",
        "content": "```swarmauri/core/agents/IAgent.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Optional, Dict\n\nclass IAgent(ABC):\n\n    @abstractmethod\n    def exec(self, input_data: Optional[Any], llm_kwargs: Optional[Dict]) -> Any:\n        \"\"\"\n        Executive method that triggers the agent's action based on the input data.\n        \"\"\"\n        pass\n    \n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgentConversation.py",
        "content": "```swarmauri/core/agents/IAgentConversation.py\nfrom abc import ABC, abstractmethod\nfrom swarmauri.core.conversations.IConversation import IConversation\n\nclass IAgentConversation(ABC):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgentRetrieve.py",
        "content": "```swarmauri/core/agents/IAgentRetrieve.py\nfrom abc import ABC\n\nclass IAgentRetrieve(ABC):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgentSystemContext.py",
        "content": "```swarmauri/core/agents/IAgentSystemContext.py\nfrom abc import ABC, abstractmethod\n\nclass IAgentSystemContext(ABC):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgentToolkit.py",
        "content": "```swarmauri/core/agents/IAgentToolkit.py\nfrom abc import ABC\n\nclass IAgentToolkit(ABC):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/agents/IAgentVectorStore.py",
        "content": "```swarmauri/core/agents/IAgentVectorStore.py\nfrom abc import ABC\n\nclass IAgentVectorStore(ABC):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/__init__.py",
        "content": "```swarmauri/core/swarms/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/ISwarm.py",
        "content": "```swarmauri/core/swarms/ISwarm.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List, Dict\nfrom datetime import datetime\nfrom swarmauri.core.agents.IAgent import IAgent\nfrom swarmauri.core.chains.ICallableChain import ICallableChain\n\nclass ISwarm(ABC):\n    \"\"\"\n    Interface for a Swarm, representing a collective of agents capable of performing tasks, executing callable chains, and adaptable configurations.\n    \"\"\"\n\n    # Abstract properties and setters\n    @property\n    @abstractmethod\n    def id(self) -> str:\n        \"\"\"Unique identifier for the factory instance.\"\"\"\n        pass\n\n    @id.setter\n    @abstractmethod\n    def id(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n\n    @name.setter\n    @abstractmethod\n    def name(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def type(self) -> str:\n        pass\n\n    @type.setter\n    @abstractmethod\n    def type(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def date_created(self) -> datetime:\n        pass\n\n    @property\n    @abstractmethod\n    def last_modified(self) -> datetime:\n        pass\n\n    @last_modified.setter\n    @abstractmethod\n    def last_modified(self, value: datetime) -> None:\n        pass\n\n    def __hash__(self):\n        \"\"\"\n        The __hash__ method allows objects of this class to be used in sets and as dictionary keys.\n        __hash__ should return an integer and be defined based on immutable properties.\n        This is generally implemented directly in concrete classes rather than in the interface,\n        but it's declared here to indicate that implementing classes must provide it.\n        \"\"\"\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/ISwarmComponent.py",
        "content": "```swarmauri/core/swarms/ISwarmComponent.py\nfrom abc import ABC, abstractmethod\n\nclass ISwarmComponent(ABC):\n    \"\"\"\n    Interface for defining a general component within a swarm system.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, key: str, name: str):\n        \"\"\"\n        Initializes a swarm component with a unique key and name.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/ISwarmConfigurationExporter.py",
        "content": "```swarmauri/core/swarms/ISwarmConfigurationExporter.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict\nclass ISwarmConfigurationExporter(ABC):\n\n    @abstractmethod\n    def to_dict(self) -> Dict:\n        \"\"\"\n        Serializes the swarm configuration to a dictionary.\n\n        Returns:\n            Dict: The serialized configuration as a dictionary.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def to_json(self) -> str:\n        \"\"\"\n        Serializes the swarm configuration to a JSON string.\n\n        Returns:\n            str: The serialized configuration as a JSON string.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def to_pickle(self) -> bytes:\n        \"\"\"\n        Serializes the swarm configuration to a Pickle byte stream.\n\n        Returns:\n            bytes: The serialized configuration as a Pickle byte stream.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/ISwarmFactory.py",
        "content": "```swarmauri/core/swarms/ISwarmFactory.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, Dict, List, NamedTuple, Optional, Type, Union\nfrom swarmauri.core.swarms.ISwarm import ISwarm\nfrom swarmauri.core.chains.ICallableChain import ICallableChain \nfrom swarmauri.core.agents.IAgent import IAgent \n\nclass Step(NamedTuple):\n    description: str\n    callable: Callable  # Reference to the function to execute\n    args: Optional[List[Any]] = None\n    kwargs: Optional[Dict[str, Any]] = None\n\nclass CallableChainItem(NamedTuple):\n    key: str  # Unique identifier for the item within the chain\n    execution_context: Dict[str, Any]  # Execution context and metadata\n    steps: List[Step]\n\nclass AgentDefinition(NamedTuple):\n    type: str\n    configuration: Dict[str, Any]\n    capabilities: List[str]\n    dependencies: List[str]\n    execution_context: Dict[str, Any]\n\nclass FunctionParameter(NamedTuple):\n    name: str\n    type: Type\n    default: Optional[Any] = None\n    required: bool = True\n\nclass FunctionDefinition(NamedTuple):\n    identifier: str\n    parameters: List[FunctionParameter]\n    return_type: Type\n    execution_context: Dict[str, Any]\n    callable_source: Callable\n    \nclass ISwarmFactory(ABC):\n\n    @abstractmethod\n    def create_swarm(self, *args, **kwargs) -> ISwarm:\n        \"\"\"\n        Creates and returns a new swarm instance configured with the provided arguments.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_agent(self, agent_definition: AgentDefinition) -> IAgent:\n        \"\"\"\n        Creates a new agent based on the provided enhanced agent definition.\n        \n        Args:\n            agent_definition: An instance of AgentDefinition detailing the agent's setup.\n        \n        Returns:\n            An instance or identifier of the newly created agent.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def create_callable_chain(self, chain_definition: List[CallableChainItem]) -> ICallableChain:\n        \"\"\"\n        Creates a new callable chain based on the provided definition.\n\n        Args:\n            chain_definition: Details required to build the chain, such as sequence of functions and arguments.\n\n        Returns:\n            ICallableChain: The constructed callable chain instance.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def register_function(self, function_definition: FunctionDefinition) -> None:\n        \"\"\"\n        Registers a function within the factory ecosystem, making it available for callable chains and agents.\n\n        Args:\n            function_definition: An instance of FunctionDefinition detailing the function's specification.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def export_callable_chains(self, format_type: str = 'json') -> Union[dict, str, bytes]:\n        \"\"\"\n        Exports configurations of all callable chains in the specified format.\n        Supported formats: 'json', 'pickle'.\n\n        Args:\n            format_type (str): The format for exporting the configurations.\n\n        Returns:\n            Union[dict, str, bytes]: The callable chain configurations in the specified format.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_callable_chains(self, chains_data, format_type: str = 'json'):\n        \"\"\"\n        Loads callable chain configurations from given data.\n\n        Args:\n            chains_data (Union[dict, str, bytes]): Data containing callable chain configurations.\n            format_type (str): The format of the provided chains data.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def export_configuration(self, format_type: str = 'json') -> Union[dict, str, bytes]:\n        \"\"\"\n        Exports the swarm's and agents' configurations in the specified format.\n        Supported formats: 'json', 'pickle'. Default is 'json'.\n\n        Args:\n            format_type (str): The format for exporting the configurations.\n\n        Returns:\n            Union[dict, str, bytes]: The configurations in the specified format.\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/ISwarmAgentRegistration.py",
        "content": "```swarmauri/core/swarms/ISwarmAgentRegistration.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Optional\nfrom swarmauri.core.agents.IAgent import IAgent\n\nclass ISwarmAgentRegistration(ABC):\n    \"\"\"\n    Interface for registering agents with the swarm, designed to support CRUD operations on IAgent instances.\n    \"\"\"\n\n    @id.setter\n    @abstractmethod\n    def registry(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def registry(self) -> List[IAgent]:\n        pass\n\n    @abstractmethod\n    def register_agent(self, agent: IAgent) -> bool:\n        \"\"\"\n        Register a new agent with the swarm.\n\n        Parameters:\n            agent (IAgent): An instance of IAgent representing the agent to register.\n\n        Returns:\n            bool: True if the registration succeeded; False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_agent(self, agent_id: str, updated_agent: IAgent) -> bool:\n        \"\"\"\n        Update the details of an existing agent. This could include changing the agent's configuration,\n        task assignment, or any other mutable attribute.\n\n        Parameters:\n            agent_id (str): The unique identifier for the agent.\n            updated_agent (IAgent): An updated IAgent instance to replace the existing one.\n\n        Returns:\n            bool: True if the update was successful; False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_agent(self, agent_id: str) -> bool:\n        \"\"\"\n        Remove an agent from the swarm based on its unique identifier.\n\n        Parameters:\n            agent_id (str): The unique identifier for the agent to be removed.\n\n        Returns:\n            bool: True if the removal was successful; False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_agent(self, agent_id: str) -> Optional[IAgent]:\n        \"\"\"\n        Retrieve an agent's instance from its unique identifier.\n\n        Parameters:\n            agent_id (str): The unique identifier for the agent of interest.\n\n        Returns:\n            Optional[IAgent]: The IAgent instance if found; None otherwise.\n        \"\"\"\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/swarms/ISwarmChainCRUD.py",
        "content": "```swarmauri/core/swarms/ISwarmChainCRUD.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass ISwarmChainCRUD(ABC):\n    \"\"\"\n    Interface to provide CRUD operations for ICallableChain within swarms.\n    \"\"\"\n\n    @abstractmethod\n    def create_chain(self, chain_id: str, chain_definition: Dict[str, Any]) -> None:\n        \"\"\"\n        Creates a callable chain with the provided definition.\n\n        Parameters:\n        - chain_id (str): A unique identifier for the callable chain.\n        - chain_definition (Dict[str, Any]): The definition of the callable chain including steps and their configurations.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def read_chain(self, chain_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Retrieves the definition of a callable chain by its identifier.\n\n        Parameters:\n        - chain_id (str): The unique identifier of the callable chain to be retrieved.\n\n        Returns:\n        - Dict[str, Any]: The definition of the callable chain.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_chain(self, chain_id: str, new_definition: Dict[str, Any]) -> None:\n        \"\"\"\n        Updates an existing callable chain with a new definition.\n\n        Parameters:\n        - chain_id (str): The unique identifier of the callable chain to be updated.\n        - new_definition (Dict[str, Any]): The new definition of the callable chain including updated steps and configurations.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_chain(self, chain_id: str) -> None:\n        \"\"\"\n        Removes a callable chain from the swarm.\n\n        Parameters:\n        - chain_id (str): The unique identifier of the callable chain to be removed.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_chains(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Lists all callable chains currently managed by the swarm.\n\n        Returns:\n        - List[Dict[str, Any]]: A list of callable chain definitions.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/toolkits/__init__.py",
        "content": "```swarmauri/core/toolkits/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/toolkits/IToolkit.py",
        "content": "```swarmauri/core/toolkits/IToolkit.py\nfrom typing import Dict\nfrom abc import ABC, abstractmethod\nfrom swarmauri.core.tools.ITool import ITool\n\nclass IToolkit(ABC):\n    \"\"\"\n    A class representing a toolkit used by Swarm Agents.\n    Tools are maintained in a dictionary keyed by the tool's name.\n    \"\"\"\n\n    @abstractmethod\n    def add_tools(self, tools: Dict[str, ITool]):\n        \"\"\"\n        An abstract method that should be implemented by subclasses to add multiple tools to the toolkit.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_tool(self, tool: ITool):\n        \"\"\"\n        An abstract method that should be implemented by subclasses to add a single tool to the toolkit.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_tool(self, tool_name: str):\n        \"\"\"\n        An abstract method that should be implemented by subclasses to remove a tool from the toolkit by name.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_tool_by_name(self, tool_name: str) -> ITool:\n        \"\"\"\n        An abstract method that should be implemented by subclasses to retrieve a tool from the toolkit by name.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __len__(self) -> int:\n        \"\"\"\n        An abstract method that should be implemented by subclasses to return the number of tools in the toolkit.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/tools/__init__.py",
        "content": "```swarmauri/core/tools/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/tools/IParameter.py",
        "content": "```swarmauri/core/tools/IParameter.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union\n\nclass IParameter(ABC):\n    \"\"\"\n    An abstract class to represent a parameter for a tool.\n    \"\"\"\n\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/tools/ITool.py",
        "content": "```swarmauri/core/tools/ITool.py\nfrom abc import ABC, abstractmethod\n\nclass ITool(ABC):\n        \n    @abstractmethod\n    def call(self, *args, **kwargs):\n        pass\n    \n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/utils/__init__.py",
        "content": "```swarmauri/core/utils/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/utils/ITransactional.py",
        "content": "```swarmauri/core/utils/ITransactional.py\nfrom abc import ABC, abstractmethod\n\nclass ITransactional(ABC):\n\n    @abstractmethod\n    def begin_transaction(self):\n        \"\"\"\n        Initiates a transaction for a series of vector store operations.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def commit_transaction(self):\n        \"\"\"\n        Commits the current transaction, making all operations within the transaction permanent.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def abort_transaction(self):\n        \"\"\"\n        Aborts the current transaction, reverting all operations performed within the transaction.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/ISimiliarityQuery.py",
        "content": "```swarmauri/core/vector_stores/ISimiliarityQuery.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict\n\nclass ISimilarityQuery(ABC):\n    \n    @abstractmethod\n    def search_by_similarity_threshold(self, query_vector: List[float], similarity_threshold: float, space_name: str = None) -> List[Dict]:\n        \"\"\"\n        Search vectors exceeding a similarity threshold to a query vector within an optional vector space.\n\n        Args:\n            query_vector (List[float]): The high-dimensional query vector.\n            similarity_threshold (float): The similarity threshold for filtering results.\n            space_name (str, optional): The name of the vector space to search within.\n\n        Returns:\n            List[Dict]: A list of dictionaries with vector IDs, similarity scores, and optional metadata that meet the similarity threshold.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IGradient.py",
        "content": "```swarmauri/core/vector_stores/IGradient.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Callable\n\nclass IGradient(ABC):\n    \"\"\"\n    Interface for calculating the gradient of a scalar field.\n    \"\"\"\n\n    @abstractmethod\n    def calculate_gradient(self, scalar_field: Callable[[List[float]], float], point: List[float]) -> List[float]:\n        \"\"\"\n        Calculate the gradient of a scalar field at a specific point.\n\n        Parameters:\n        - scalar_field (Callable[[List[float]], float]): The scalar field represented as a function\n                                                         that takes a point and returns a scalar value.\n        - point (List[float]): The point at which the gradient is to be calculated.\n\n        Returns:\n        - List[float]: The gradient vector at the specified point.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IAngleBetweenVectors.py",
        "content": "```swarmauri/core/vector_stores/IAngleBetweenVectors.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IAngleBetweenVectors(ABC):\n    \"\"\"\n    Interface for calculating the angle between two vectors.\n    \"\"\"\n\n    @abstractmethod\n    def angle_between(self, vector_a: List[float], vector_b: List[float]) -> float:\n        \"\"\"\n        Method to calculate and return the angle in radians between two vectors.\n\n        Parameters:\n        - vector_a (List[float]): The first vector as a list of floats.\n        - vector_b (List[float]): The second vector as a list of floats.\n\n        Returns:\n        - float: The angle between vector_a and vector_b in radians.\n\n        Note: Implementations should handle the vectors' dimensionality and throw appropriate exceptions for incompatible vectors.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IDecompose.py",
        "content": "```swarmauri/core/vector_stores/IDecompose.py\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, List\nfrom swarmauri.core.vectors.IVector import IVector  # Assuming there's a base IVector interface for vector representations\n\nclass IDecompose(ABC):\n    \"\"\"\n    Interface for decomposing a vector into components along specified basis vectors.\n    This operation is essential in expressing a vector in different coordinate systems or reference frames.\n    \"\"\"\n\n    @abstractmethod\n    def decompose(self, vector: IVector, basis_vectors: List[IVector]) -> List[IVector]:\n        \"\"\"\n        Decompose the given vector into components along the specified basis vectors.\n\n        Parameters:\n        - vector (IVector): The vector to be decomposed.\n        - basis_vectors (List[IVector]): A list of basis vectors along which to decompose the given vector.\n\n        Returns:\n        - List[IVector]: A list of vectors, each representing the component of the decomposed vector along \n                         the corresponding basis vector in the `basis_vectors` list.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IDivergence.py",
        "content": "```swarmauri/core/vector_stores/IDivergence.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IDivergence(ABC):\n    \"\"\"\n    Interface for calculating the divergence of a vector field.\n    \"\"\"\n\n    @abstractmethod\n    def calculate_divergence(self, vector_field: List[List[float]], point: List[float]) -> float:\n        \"\"\"\n        Calculate the divergence of a vector field at a specific point.\n\n        Parameters:\n        - vector_field (List[List[float]]): A representation of the vector field as a list of vectors.\n        - point (List[float]): The point at which the divergence is to be calculated.\n\n        Returns:\n        - float: The divergence value at the specified point.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IOrthogonalProject.py",
        "content": "```swarmauri/core/vector_stores/IOrthogonalProject.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IOrthogonalProject(ABC):\n    \"\"\"\n    Interface for calculating the orthogonal projection of one vector onto another.\n    \"\"\"\n\n    @abstractmethod\n    def orthogonal_project(self, vector_a: List[float], vector_b: List[float]) -> List[float]:\n        \"\"\"\n        Calculates the orthogonal projection of vector_a onto vector_b.\n        \n        Args:\n            vector_a (List[float]): The vector to be projected.\n            vector_b (List[float]): The vector onto which vector_a is orthogonally projected.\n        \n        Returns:\n            List[float]: The orthogonal projection of vector_a onto vector_b.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IProject.py",
        "content": "```swarmauri/core/vector_stores/IProject.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IProject(ABC):\n    \"\"\"\n    Interface for projecting one vector onto another.\n    \"\"\"\n\n    @abstractmethod\n    def project(self, vector_a: List[float], vector_b: List[float]) -> List[float]:\n        \"\"\"\n        Projects vector_a onto vector_b.\n        \n        Args:\n            vector_a (List[float]): The vector to be projected.\n            vector_b (List[float]): The vector onto which vector_a is projected.\n        \n        Returns:\n            List[float]: The projection of vector_a onto vector_b.\n        \"\"\"\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IReflect.py",
        "content": "```swarmauri/core/vector_stores/IReflect.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IReflect(ABC):\n    \"\"\"\n    Interface for reflecting a vector across a specified plane or axis.\n    \"\"\"\n\n    @abstractmethod\n    def reflect_vector(self, vector: List[float], normal: List[float]) -> List[float]:\n        \"\"\"\n        Reflects a vector across a plane or axis defined by a normal vector.\n\n        Parameters:\n        - vector (List[float]): The vector to be reflected.\n        - normal (List[float]): The normal vector of the plane across which the vector will be reflected.\n\n        Returns:\n        - List[float]: The reflected vector.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/ISimilarity.py",
        "content": "```swarmauri/core/vector_stores/ISimilarity.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass ISimilarity(ABC):\n    \"\"\"\n    Interface to define operations for computing similarity and distance between vectors.\n    This interface is crucial for systems that need to perform similarity searches, clustering,\n    or any operations where vector similarity plays a key role.\n    \"\"\"\n\n    @abstractmethod\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Compute the similarity between two vectors. The definition of similarity (e.g., cosine similarity)\n        should be implemented in concrete classes.\n\n        Args:\n            vector_a (IVector): The first vector.\n            vector_b (IVector): The second vector to compare with the first vector.\n\n        Returns:\n            float: A similarity score between vector_a and vector_b.\n        \"\"\"\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorSpan.py",
        "content": "```swarmauri/core/vector_stores/IVectorSpan.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any\n\nclass IVectorSpan(ABC):\n    \"\"\"\n    Interface for determining if a vector is within the span of a set of vectors.\n    \"\"\"\n\n    @abstractmethod\n    def in_span(self, vector: Any, basis_vectors: List[Any]) -> bool:\n        \"\"\"\n        Checks if the given vector is in the span of the provided basis vectors.\n\n        Parameters:\n        - vector (Any): The vector to check.\n        - basis_vectors (List[Any]): A list of vectors that might span the vector.\n\n        Returns:\n        - bool: True if the vector is in the span of the basis_vectors, False otherwise.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorArithmetic.py",
        "content": "```swarmauri/core/vector_stores/IVectorArithmetic.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IVectorArithmetic(ABC):\n    @abstractmethod\n    def add(self, vector1: List[float], vector2: List[float]) -> List[float]:\n        \"\"\"\n        Vector addition of 'vector1' and 'vector2'.\n        \"\"\"\n        pass\n        \n    @abstractmethod\n    def subtract(self, vector1: List[float], vector2: List[float]) -> List[float]:\n        \"\"\"\n        Vector subtraction of 'vector1' - 'vector2'.\n        \"\"\"\n        pass\n   \n    @abstractmethod\n    def multiply(self, vector: List[float], scalar: float) -> List[float]:\n        \"\"\"\n        Scalar multiplication of 'vector' by 'scalar'.\n        \"\"\"\n        pass\n        \n    @abstractmethod\n    def divide(self, vector: List[float], scalar: float) -> List[float]:\n        \"\"\"\n        Scalar division of 'vector' by 'scalar'.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorLinearCombination.py",
        "content": "```swarmauri/core/vector_stores/IVectorLinearCombination.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any\n\nclass ILinearCombination(ABC):\n    \"\"\"\n    Interface for creating a vector as a linear combination of a set of vectors.\n    \"\"\"\n\n    @abstractmethod\n    def linear_combination(self, coefficients: List[float], vectors: List[Any]) -> Any:\n        \"\"\"\n        Computes the linear combination of the given vectors with the specified coefficients.\n\n        Parameters:\n        - coefficients (List[float]): A list of coefficients for the linear combination.\n        - vectors (List[Any]): A list of vectors to be combined.\n\n        Returns:\n        - Any: The resulting vector from the linear combination.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorNorm.py",
        "content": "```swarmauri/core/vector_stores/IVectorNorm.py\n# core/vectors/IVectorNorm.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union\n\nclass IVectorNorm(ABC):\n    \"\"\"\n    Interface for calculating vector norms.\n    Supports L1 norm, L2 norm, and Max norm calculations.\n    \"\"\"\n\n    @abstractmethod\n    def l1_norm(self, vector: List[Union[int, float]]) -> float:\n        \"\"\"\n        Calculate the L1 norm (Manhattan norm) of a vector.\n\n        Parameters:\n        - vector (List[Union[int, float]]): The vector for which to calculate the L1 norm.\n\n        Returns:\n        - float: The L1 norm of the vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def l2_norm(self, vector: List[Union[int, float]]) -> float:\n        \"\"\"\n        Calculate the L2 norm (Euclidean norm) of a vector.\n\n        Parameters:\n        - vector (List[Union[int, float]]): The vector for which to calculate the L2 norm.\n\n        Returns:\n        - float: The L2 norm of the vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def max_norm(self, vector: List[Union[int, float]]) -> float:\n        \"\"\"\n        Calculate the Max norm (infinity norm) of a vector.\n\n        Parameters:\n        - vector (List[Union[int, float]]): The vector for which to calculate the Max norm.\n\n        Returns:\n        - float: The Max norm of the vector.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorRotate.py",
        "content": "```swarmauri/core/vector_stores/IVectorRotate.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nclass IRotate(ABC):\n    \"\"\"\n    Interface for rotating a vector.\n    \"\"\"\n    \n    @abstractmethod\n    def rotate(self, vector: List[float], angle: float, axis: List[float] = None) -> List[float]:\n        \"\"\"\n        Rotate the given vector by a specified angle around an axis (for 3D) or in a plane (for 2D).\n\n        For 2D vectors, the axis parameter can be omitted.\n\n        Args:\n            vector (List[float]): The vector to rotate.\n            angle (float): The angle of rotation in degrees.\n            axis (List[float], optional): The axis of rotation (applicable in 3D).\n\n        Returns:\n            List[float]: The rotated vector.\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorBasisCheck.py",
        "content": "```swarmauri/core/vector_stores/IVectorBasisCheck.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any\n\nclass IVectorBasisCheck(ABC):\n    \"\"\"\n    Interface for checking if a given set of vectors forms a basis of the vector space.\n    \"\"\"\n\n    @abstractmethod\n    def is_basis(self, vectors: List[Any]) -> bool:\n        \"\"\"\n        Determines whether the given set of vectors forms a basis for their vector space.\n\n        Parameters:\n        - vectors (List[Any]): A list of vectors to be checked.\n\n        Returns:\n        - bool: True if the vectors form a basis, False otherwise.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/__init__.py",
        "content": "```swarmauri/core/vector_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorStore.py",
        "content": "```swarmauri/core/vector_stores/IVectorStore.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Union\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass IVectorStore(ABC):\n    \"\"\"\n    Interface for a vector store responsible for storing, indexing, and retrieving documents.\n    \"\"\"\n\n    @abstractmethod\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Stores a single document in the vector store.\n\n        Parameters:\n        - document (IDocument): The document to store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Stores multiple documents in the vector store.\n\n        Parameters:\n        - documents (List[IDocument]): The list of documents to store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document(self, doc_id: str) -> Union[IDocument, None]:\n        \"\"\"\n        Retrieves a document by its ID.\n\n        Parameters:\n        - doc_id (str): The unique identifier for the document.\n\n        Returns:\n        - Union[IDocument, None]: The requested document, or None if not found.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieves all documents stored in the vector store.\n\n        Returns:\n        - List[IDocument]: A list of all documents.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_document(self, doc_id: str) -> None:\n        \"\"\"\n        Deletes a document from the vector store by its ID.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to delete.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def clear_documents(self) -> None:\n        \"\"\"\n        Deletes all documents from the vector store\n\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Updates a document in the vector store.\n\n        Parameters:\n        - doc_id (str): The unique identifier for the document to update.\n        - updated_document (IDocument): The updated document object.\n\n        Note: It's assumed that the updated_document will retain the same doc_id but may have different content or metadata.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def document_count(self) -> int:\n        pass \n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorStoreRetrieve.py",
        "content": "```swarmauri/core/vector_stores/IVectorStoreRetrieve.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass IVectorStoreRetrieve(ABC):\n    \"\"\"\n    Abstract base class for document retrieval operations.\n    \n    This class defines the interface for retrieving documents based on a query or other criteria.\n    Implementations may use various indexing or search technologies to fulfill these retrievals.\n    \"\"\"\n\n    @abstractmethod\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the most relevant documents based on the given query.\n        \n        Parameters:\n            query (str): The query string used for document retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n            \n        Returns:\n            List[Document]: A list of the top_k most relevant documents.\n        \"\"\"\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/vector_stores/IVectorStoreSaveLoad.py",
        "content": "```swarmauri/core/vector_stores/IVectorStoreSaveLoad.py\nfrom abc import ABC, abstractmethod\n\nclass IVectorStoreSaveLoad(ABC):\n    \"\"\"\n    Interface to abstract the ability to save and load the state of a vector store.\n    This includes saving/loading the vectorizer's model as well as the documents or vectors.\n    \"\"\"\n\n    @abstractmethod\n    def save_store(self, directory_path: str) -> None:\n        \"\"\"\n        Saves the state of the vector store to the specified directory. This includes\n        both the vectorizer's model and the stored documents or vectors.\n\n        Parameters:\n        - directory_path (str): The directory path where the store's state will be saved.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_store(self, directory_path: str) -> None:\n        \"\"\"\n        Loads the state of the vector store from the specified directory. This includes\n        both the vectorizer's model and the stored documents or vectors.\n\n        Parameters:\n        - directory_path (str): The directory path from where the store's state will be loaded.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_parts(self, directory_path: str, chunk_size: int=10485760) -> None:\n        \"\"\"\n        Save the model in parts to handle large files by splitting them.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_parts(self, directory_path: str, file_pattern: str) -> None:\n        \"\"\"\n        Load and combine model parts from a directory.\n\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/document_stores/__init__.py",
        "content": "```swarmauri/core/document_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/document_stores/IDocumentRetrieve.py",
        "content": "```swarmauri/core/document_stores/IDocumentRetrieve.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass IDocumentRetrieve(ABC):\n    \"\"\"\n    Abstract base class for document retrieval operations.\n    \n    This class defines the interface for retrieving documents based on a query or other criteria.\n    Implementations may use various indexing or search technologies to fulfill these retrievals.\n    \"\"\"\n\n    @abstractmethod\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the most relevant documents based on the given query.\n        \n        Parameters:\n            query (str): The query string used for document retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n            \n        Returns:\n            List[Document]: A list of the top_k most relevant documents.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/document_stores/IDocumentStore.py",
        "content": "```swarmauri/core/document_stores/IDocumentStore.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass IDocumentStore(ABC):\n    \"\"\"\n    Interface for a Document Store responsible for storing, indexing, and retrieving documents.\n    \"\"\"\n\n    @abstractmethod\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Stores a single document in the document store.\n\n        Parameters:\n        - document (IDocument): The document to store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Stores multiple documents in the document store.\n\n        Parameters:\n        - documents (List[IDocument]): The list of documents to store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document(self, doc_id: str) -> Union[IDocument, None]:\n        \"\"\"\n        Retrieves a document by its ID.\n\n        Parameters:\n        - doc_id (str): The unique identifier for the document.\n\n        Returns:\n        - Union[IDocument, None]: The requested document, or None if not found.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieves all documents stored in the document store.\n\n        Returns:\n        - List[IDocument]: A list of all documents.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_document(self, doc_id: str) -> None:\n        \"\"\"\n        Deletes a document from the document store by its ID.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to delete.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Updates a document in the document store.\n\n        Parameters:\n        - doc_id (str): The unique identifier for the document to update.\n        - updated_document (IDocument): The updated document object.\n\n        Note: It's assumed that the updated_document will retain the same doc_id but may have different content or metadata.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def document_count(self) -> int:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chunkers/IChunker.py",
        "content": "```swarmauri/core/chunkers/IChunker.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union, Any\n\nclass IChunker(ABC):\n    \"\"\"\n    Interface for chunking text into smaller pieces.\n\n    This interface defines abstract methods for chunking texts. Implementing classes\n    should provide concrete implementations for these methods tailored to their specific\n    chunking algorithms.\n    \"\"\"\n\n    @abstractmethod\n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[Any]:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chunkers/__init__.py",
        "content": "```swarmauri/core/chunkers/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/vectors/IVectorMeta.py",
        "content": "```swarmauri/core/vectors/IVectorMeta.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, List\n\nclass IVectorMeta(ABC):\n    \"\"\"\n    Interface for a high-dimensional data vector. This interface defines the\n    basic structure and operations for interacting with vectors in various applications,\n    such as machine learning, information retrieval, and similarity search.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def id(self) -> str:\n        \"\"\"\n        Unique identifier for the vector. This ID can be used to reference the vector\n        in a database or a vector store.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def metadata(self) -> Dict[str, Any]:\n        \"\"\"\n        Optional metadata associated with the vector. Metadata can include additional information\n        useful for retrieval, categorization, or description of the vector data.\n        \"\"\"\n        pass\n\n\n```"
    },
    {
        "document_name": "swarmauri/core/vectors/IVectorTransform.py",
        "content": "```swarmauri/core/vectors/IVectorTransform.py\nfrom abc import ABC, abstractmethod\nfrom .IVector import IVector\n\nclass IVectorTransform(ABC):\n    \"\"\"\n    Interface for performing various transformations on vectors.\n    \"\"\"\n\n    @abstractmethod\n    def translate(self, translation_vector: IVector) -> IVector:\n        \"\"\"\n        Translate a vector by a given translation vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rotate(self, angle: float, axis: IVector) -> IVector:\n        \"\"\"\n        Rotate a vector around a given axis by a certain angle.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reflect(self, plane_normal: IVector) -> IVector:\n        \"\"\"\n        Reflect a vector across a plane defined by its normal vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def scale(self, scale_factor: float) -> IVector:\n        \"\"\"\n        Scale a vector by a given scale factor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def shear(self, shear_factor: float, direction: IVector) -> IVector:\n        \"\"\"\n        Shear a vector along a given direction by a shear factor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def project(self, plane_normal: IVector) -> IVector:\n        \"\"\"\n        Project a vector onto a plane defined by its normal vector.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vectors/__init__.py",
        "content": "```swarmauri/core/vectors/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/vectors/IVectorProduct.py",
        "content": "```swarmauri/core/vectors/IVectorProduct.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\n\nclass IVectorProduct(ABC):\n    \"\"\"\n    Interface for various vector products including dot product, cross product,\n    and triple products (vector and scalar).\n    \"\"\"\n\n    @abstractmethod\n    def dot_product(self, vector_a: List[float], vector_b: List[float]) -> float:\n        \"\"\"\n        Calculate the dot product of two vectors.\n\n        Parameters:\n        - vector_a (List[float]): The first vector.\n        - vector_b (List[float]): The second vector.\n\n        Returns:\n        - float: The dot product of the two vectors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def cross_product(self, vector_a: List[float], vector_b: List[float]) -> List[float]:\n        \"\"\"\n        Calculate the cross product of two vectors.\n\n        Parameters:\n        - vector_a (List[float]): The first vector.\n        - vector_b (List[float]): The second vector.\n\n        Returns:\n        - List[float]: The cross product as a new vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def vector_triple_product(self, vector_a: List[float], vector_b: List[float], vector_c: List[float]) -> List[float]:\n        \"\"\"\n        Calculate the vector triple product of three vectors.\n\n        Parameters:\n        - vector_a (List[float]): The first vector.\n        - vector_b (List[float]): The second vector.\n        - vector_c (List[float]): The third vector.\n\n        Returns:\n        - List[float]: The result of the vector triple product as a new vector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def scalar_triple_product(self, vector_a: List[float], vector_b: List[float], vector_c: List[float]) -> float:\n        \"\"\"\n        Calculate the scalar triple product of three vectors.\n\n        Parameters:\n        - vector_a (List[float]): The first vector.\n        - vector_b (List[float]): The second vector.\n        - vector_c (List[float]): The third vector.\n\n        Returns:\n        - float: The scalar value result of the scalar triple product.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/vectors/IVector.py",
        "content": "```swarmauri/core/vectors/IVector.py\nfrom abc import ABC, abstractmethod\n\nclass IVector(ABC):\n    \"\"\"\n    Interface for a high-dimensional data vector. This interface defines the\n    basic structure and operations for interacting with vectors in various applications,\n    such as machine learning, information retrieval, and similarity search.\n    \"\"\"\n\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/swarm_apis/__init__.py",
        "content": "```swarmauri/core/swarm_apis/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/swarm_apis/ISwarmAPI.py",
        "content": "```swarmauri/core/swarm_apis/ISwarmAPI.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass ISwarmAPI(ABC):\n    \"\"\"\n    Interface for managing the swarm's API endpoints.\n    \"\"\"\n    \n    @abstractmethod\n    def dispatch_request(self, request_data: Dict[str, Any]) -> Any:\n        \"\"\"\n        Dispatches an incoming user request to one or more suitable agents based on their capabilities.\n\n        Parameters:\n        - request_data (Dict[str, Any]): Data related to the incoming request.\n\n        Returns:\n        - Any: Response from processing the request.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def broadcast_request(self, request_data: Dict[str, Any]) -> Any:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/swarm_apis/IAgentRegistrationAPI.py",
        "content": "```swarmauri/core/swarm_apis/IAgentRegistrationAPI.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Optional\nfrom swarmauri.core.agents.IAgent import IAgent\n\nclass IAgentRegistrationAPI(ABC):\n    \"\"\"\n    Interface for registering agents with the swarm, designed to support CRUD operations on IAgent instances.\n    \"\"\"\n\n    @abstractmethod\n    def register_agent(self, agent: IAgent) -> bool:\n        \"\"\"\n        Register a new agent with the swarm.\n\n        Parameters:\n            agent (IAgent): An instance of IAgent representing the agent to register.\n\n        Returns:\n            bool: True if the registration succeeded; False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_agent(self, agent_id: str, updated_agent: IAgent) -> bool:\n        \"\"\"\n        Update the details of an existing agent. This could include changing the agent's configuration,\n        task assignment, or any other mutable attribute.\n\n        Parameters:\n            agent_id (str): The unique identifier for the agent.\n            updated_agent (IAgent): An updated IAgent instance to replace the existing one.\n\n        Returns:\n            bool: True if the update was successful; False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_agent(self, agent_id: str) -> bool:\n        \"\"\"\n        Remove an agent from the swarm based on its unique identifier.\n\n        Parameters:\n            agent_id (str): The unique identifier for the agent to be removed.\n\n        Returns:\n            bool: True if the removal was successful; False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_agent(self, agent_id: str) -> Optional[IAgent]:\n        \"\"\"\n        Retrieve an agent's instance from its unique identifier.\n\n        Parameters:\n            agent_id (str): The unique identifier for the agent of interest.\n\n        Returns:\n            Optional[IAgent]: The IAgent instance if found; None otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_agents(self) -> List[IAgent]:\n        \"\"\"\n        List all registered agents.\n\n        Returns:\n            List[IAgent]: A list containing instances of all registered IAgents.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/tracing/__init__.py",
        "content": "```swarmauri/core/tracing/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/tracing/ITraceContext.py",
        "content": "```swarmauri/core/tracing/ITraceContext.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\nclass ITraceContext(ABC):\n    \"\"\"\n    Interface for a trace context, representing a single trace instance.\n    This context carries the state and metadata of the trace across different system components.\n    \"\"\"\n\n    @abstractmethod\n    def get_trace_id(self) -> str:\n        \"\"\"\n        Retrieves the unique identifier for this trace.\n\n        Returns:\n            str: The unique trace identifier.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_attribute(self, key: str, value: Any):\n        \"\"\"\n        Adds or updates an attribute associated with this trace.\n\n        Args:\n            key (str): The attribute key or name.\n            value (Any): The value of the attribute.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/tracing/ITracer.py",
        "content": "```swarmauri/core/tracing/ITracer.py\nfrom swarmauri.core.tracing.ITraceContext import ITraceContext\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Dict, Any\n\n\nclass ITracer(ABC):\n    \"\"\"\n    Interface for implementing distributed tracing across different components of the system.\n    \"\"\"\n\n    @abstractmethod\n    def start_trace(self, name: str, initial_attributes: Optional[Dict[str, Any]] = None) -> ITraceContext:\n        \"\"\"\n        Starts a new trace with a given name and optional initial attributes.\n\n        Args:\n            name (str): Name of the trace, usually represents the operation being traced.\n            initial_attributes (Optional[Dict[str, Any]]): Key-value pairs to be attached to the trace initially.\n\n        Returns:\n            ITraceContext: A context object representing this particular trace instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def end_trace(self, trace_context: ITraceContext):\n        \"\"\"\n        Marks the end of a trace, completing its lifecycle and recording its details.\n\n        Args:\n            trace_context (ITraceContext): The trace context to be ended.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def annotate_trace(self, trace_context: ITraceContext, key: str, value: Any):\n        \"\"\"\n        Adds an annotation to an existing trace, enriching it with more detailed information.\n\n        Args:\n            trace_context (ITraceContext): The trace context to annotate.\n            key (str): The key or name of the annotation.\n            value (Any): The value of the annotation.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/tracing/IChainTracer.py",
        "content": "```swarmauri/core/tracing/IChainTracer.py\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Tuple, Dict, Any\n\nclass IChainTracer(ABC):\n    \"\"\"\n    Interface for a tracer supporting method chaining through a list of tuples.\n    Each tuple in the list contains: trace context, function, args, and kwargs.\n    \"\"\"\n\n    @abstractmethod\n    def process_chain(self, chain: List[Tuple[Any, Callable[..., Any], List[Any], Dict[str, Any]]]) -> \"IChainTracer\":\n        \"\"\"\n        Processes a sequence of operations defined in a chain.\n\n        Args:\n            chain (List[Tuple[Any, Callable[..., Any], List[Any], Dict[str, Any]]]): A list where each tuple contains:\n                - The trace context or reference required by the function.\n                - The function (method of IChainTracer) to execute.\n                - A list of positional arguments for the function.\n                - A dictionary of keyword arguments for the function.\n\n        Returns:\n            IChainTracer: Returns self to allow further method chaining.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chains/ICallableChain.py",
        "content": "```swarmauri/core/chains/ICallableChain.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, List, Tuple\n\nCallableDefinition = Tuple[Callable, List[Any], dict]\n\nclass ICallableChain(ABC):\n    @abstractmethod\n    def __call__(self, *initial_args: Any, **initial_kwargs: Any) -> Any:\n        \"\"\"Executes the chain of callables.\"\"\"\n        pass\n\n    @abstractmethod\n    def add_callable(self, func: Callable, args: List[Any] = None, kwargs: dict = None) -> None:\n        \"\"\"Adds a new callable to the chain.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chains/__init__.py",
        "content": "```swarmauri/core/chains/__init__.py\nfrom swarmauri.core.chains.ICallableChain import ICallableChain\n```"
    },
    {
        "document_name": "swarmauri/core/chains/IChain.py",
        "content": "```swarmauri/core/chains/IChain.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any, Dict\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass IChain(ABC):\n    \"\"\"\n    Defines the interface for a Chain within a system, facilitating the organized\n    execution of a sequence of tasks or operations. This interface is at the core of\n    orchestrating operations that require coordination between multiple steps, potentially\n    involving decision-making, branching, and conditional execution based on the outcomes\n    of previous steps or external data.\n\n    A chain can be thought of as a workflow or pipeline, where each step in the chain can\n    perform an operation, transform data, or make decisions that influence the flow of\n    execution.\n\n    Implementors of this interface are responsible for managing the execution order,\n    data flow between steps, and any dynamic adjustments to the execution based on\n    runtime conditions.\n\n    Methods:\n        add_step: Adds a step to the chain.\n        remove_step: Removes a step from the chain.\n        execute: Executes the chain, potentially returning a result.\n    \"\"\"\n\n    @abstractmethod\n    def add_step(self, step: IChainStep, **kwargs) -> None:\n        \"\"\"\n        Adds a new step to the chain. Steps are executed in the order they are added.\n        Each step is represented by a Callable, which can be a function or method, with\n        optional keyword arguments that specify execution aspects or data needed by the step.\n\n        Parameters:\n            step (IChainStep): The Callable representing the step to add to the chain.\n            **kwargs: Optional keyword arguments that provide additional data or configuration\n                      for the step when it is executed.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_step(self, step: IChainStep) -> None:\n        \"\"\"\n        Removes an existing step from the chain. This alters the chain's execution sequence\n        by excluding the specified step from subsequent executions of the chain.\n\n        Parameters:\n            step (IChainStep): The Callable representing the step to remove from the chain.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute(self, *args, **kwargs) -> Any:\n        \"\"\"\n        Initiates the execution of the chain. This involves invoking each step in the order\n        they have been added to the chain, passing control from one step to the next, and optionally\n        aggregating or transforming results along the way.\n\n        The execution process can incorporate branching, looping, or conditional logic based on the\n        implementation, allowing for complex workflows to be represented and managed within the chain.\n\n        Parameters:\n            *args: Positional arguments passed to the first step in the chain. These can be data inputs\n                   or other values required for the chain's execution.\n            **kwargs: Keyword arguments that provide additional context, data inputs, or configuration\n                      for the chain's execution. These can be passed to individual steps or influence\n                      the execution flow of the chain.\n\n        Returns:\n            Any: The outcome of executing the chain. This could be a value produced by the final\n                 step, a collection of outputs from multiple steps, or any other result type as\n                 determined by the specific chain implementation.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chains/IChainContext.py",
        "content": "```swarmauri/core/chains/IChainContext.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\n\nclass IChainContext(ABC):\n    \n    @abstractmethod\n    def update(self, **kwargs) -> None:\n        pass\n\n    def get_value(self, key: str) -> Any:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chains/IChainContextLoader.py",
        "content": "```swarmauri/core/chains/IChainContextLoader.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\n\nclass IChainContextLoader(ABC):\n    @abstractmethod\n    def load_context(self, context_id: str) -> Dict[str, Any]:\n        \"\"\"Load the execution context by its identifier.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chains/IChainDependencyResolver.py",
        "content": "```swarmauri/core/chains/IChainDependencyResolver.py\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, Dict, List, Optional\nfrom swarmauri.standard.chains.concrete.ChainStep import ChainStep\n\nclass IChainDependencyResolver(ABC):\n    @abstractmethod\n    def build_dependencies(self) -> List[ChainStep]:\n        \"\"\"\n        Builds the dependencies for a particular sequence in the matrix.\n\n        Args:\n            matrix (List[List[str]]): The prompt matrix.\n            sequence_index (int): The index of the sequence to build dependencies for.\n\n        Returns:\n            Tuple containing indegrees and graph dicts.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def resolve_dependencies(self, matrix: List[List[Optional[str]]], sequence_index: int) -> List[int]:\n        \"\"\"\n        Resolves the execution order based on the provided dependencies.\n\n        Args:\n            indegrees (Dict[int, int]): The indegrees of each node.\n            graph (Dict[int, List[int]]): The graph representing dependencies.\n\n        Returns:\n            List[int]: The resolved execution order.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/chains/IChainFactory.py",
        "content": "```swarmauri/core/chains/IChainFactory.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any, Dict\nfrom swarmauri.core.chains.IChain import IChain\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass IChainFactory(ABC):\n    \"\"\"\n    Interface for creating and managing execution chains within the system.\n    \"\"\"\n\n    @abstractmethod\n    def create_chain(self, steps: List[IChainStep] = None) -> IChain:\n        pass\n    \n    \n    @abstractmethod\n    def get_chain(self) -> IChain:\n        pass\n    \n    @abstractmethod\n    def set_chain(self, chain: IChain):\n        pass\n    \n    @abstractmethod\n    def reset_chain(self):\n        pass\n    \n    @abstractmethod\n    def get_chain_steps(self) -> List[IChainStep]:\n        pass\n    \n    @abstractmethod\n    def set_chain_steps(self, steps: List[IChainStep]):\n        pass\n    \n    @abstractmethod\n    def add_chain_step(self, step: IChainStep):\n        pass\n    \n    @abstractmethod\n    def remove_chain_step(self, key: str):\n        pass\n    \n    @abstractmethod\n    def get_configs(self) -> Dict[str, Any]:\n        pass\n    \n    @abstractmethod\n    def set_configs(self, **configs):\n        pass\n    \n    @abstractmethod\n    \n    def get_config(self, key: str) -> Any:\n        pass\n    \n    @abstractmethod\n    def set_config(self, key: str, value: Any):\n        pass\n    \n\n\n```"
    },
    {
        "document_name": "swarmauri/core/chains/IChainStep.py",
        "content": "```swarmauri/core/chains/IChainStep.py\nfrom typing import List, Dict, Any, Callable\n\nclass IChainStep:\n    \"\"\"\n    Represents a single step within an execution chain.\n    \"\"\"\n    pass\n```"
    },
    {
        "document_name": "swarmauri/core/distances/__init__.py",
        "content": "```swarmauri/core/distances/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/distances/IDistanceSimilarity.py",
        "content": "```swarmauri/core/distances/IDistanceSimilarity.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass IDistanceSimilarity(ABC):\n    \"\"\"\n    Interface for computing distances and similarities between high-dimensional data vectors. This interface\n    abstracts the method for calculating the distance and similarity, allowing for the implementation of various \n    distance metrics such as Euclidean, Manhattan, Cosine similarity, etc.\n    \"\"\"\n\n    @abstractmethod\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the distance between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The computed distance between vector_a and vector_b.\n        \"\"\"\n        pass\n    \n\n    @abstractmethod\n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> float:\n        pass\n\n\n    @abstractmethod\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Compute the similarity between two vectors. The definition of similarity (e.g., cosine similarity)\n        should be implemented in concrete classes.\n\n        Args:\n            vector_a (IVector): The first vector.\n            vector_b (IVector): The second vector to compare with the first vector.\n\n        Returns:\n            float: A similarity score between vector_a and vector_b.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> float:\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/metrics/__init__.py",
        "content": "```swarmauri/core/metrics/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/metrics/IMetric.py",
        "content": "```swarmauri/core/metrics/IMetric.py\nfrom typing import Any\nfrom abc import ABC, abstractmethod\n\nclass IMetric(ABC):\n    \"\"\"\n    Defines a general interface for metrics within the SwarmaURI system.\n    Metrics can be anything from system performance measurements to\n    machine learning model evaluation metrics.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"\n        Retrieves the current value of the metric.\n\n        Returns:\n            The current value of the metric.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/metrics/IMetricAggregate.py",
        "content": "```swarmauri/core/metrics/IMetricAggregate.py\nfrom typing import List, Any\nfrom abc import ABC, abstractmethod\n\nclass IMetricAggregate(ABC):\n\n    @abstractmethod\n    def add_measurement(self, measurement: Any) -> None:\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"\n        Reset or clear the metric's current state, starting fresh as if no data had been processed.\n        This is useful for metrics that might aggregate or average data over time and need to be reset.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/metrics/IMetricCalculate.py",
        "content": "```swarmauri/core/metrics/IMetricCalculate.py\nfrom typing import Any\nfrom abc import ABC, abstractmethod\n\nclass IMetricCalculate(ABC):\n\n    @abstractmethod\n    def calculate(self, **kwargs) -> Any:\n        \"\"\"\n        Calculate the metric based on the provided data.\n\n        Args:\n            *args: Variable length argument list that the metric calculation might require.\n            **kwargs: Arbitrary keyword arguments that the metric calculation might require.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, value) -> None:\n        \"\"\"\n        Update the metric value based on new information.\n\n        Args:\n            value: The new information used to update the metric. This could be a new\n            measurement or data point that affects the metric's current value.\n\n        Note:\n            This method is intended for internal use and should not be publicly accessible.\n        \"\"\"\n        pass\n        \n```"
    },
    {
        "document_name": "swarmauri/core/metrics/IThreshold.py",
        "content": "```swarmauri/core/metrics/IThreshold.py\nfrom abc import ABC, abstractmethod\n\nclass IThreshold(ABC):\n    pass\n\n```"
    },
    {
        "document_name": "swarmauri/core/experiment_stores/__init__.py",
        "content": "```swarmauri/core/experiment_stores/__init__.py\n# core/experiment_stores/IExperimentStore.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Union\nfrom swarmauri.core.documents.IExperimentDocument import IExperimentDocument\n\nclass IExperimentStore(ABC):\n    \"\"\"\n    Interface for an Experiment Store that manages experimental documents and supports\n    operations related to experimenting, evaluating, testing, and benchmarking.\n    \"\"\"\n    @abstractmethod\n    def add_experiment(self, experiment: IExperimentDocument) -> None:\n        \"\"\"\n        Stores a single experiment in the experiment store.\n\n        Parameters:\n        - experiment (IExperimentDocument): The experimental document to be stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_experiments(self, experiments: List[IExperimentDocument]) -> None:\n        \"\"\"\n        Stores multiple experiments in the experiment store.\n\n        Parameters:\n        - experiments (List[IExperimentDocument]): The list of experimental documents to be stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_experiment(self, experiment_id: str) -> Union[IExperimentDocument, None]:\n        \"\"\"\n        Retrieves an experimental document by its ID.\n\n        Parameters:\n        - id (str): The unique identifier of the experiment.\n\n        Returns:\n        - Union[IExperimentDocument, None]: The requested experimental document, or None if not found.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_experiments(self) -> List[IExperimentDocument]:\n        \"\"\"\n        Retrieves all experimental documents stored in the experiment store.\n\n        Returns:\n        - List[IExperimentDocument]: A list of all experimental documents.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_experiment(self, experiment_id: str, updated_experiment: IExperimentDocument) -> None:\n        \"\"\"\n        Updates an experimental document in the experiment store.\n\n        Parameters:\n        - id (str): The unique identifier of the experiment to update.\n        - updated_experiment (IExperimentDocument): The updated experimental document.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_experiment(self, experiment_id: str) -> None:\n        \"\"\"\n        Deletes an experimental document from the experiment store by its ID.\n\n        Parameters:\n        - id (str): The unique identifier of the experimental document to be deleted.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def evaluate_experiments(self, evaluation_criteria: Dict[str, Any]) -> Any:\n        \"\"\"\n        Evaluates the experiments stored in the experiment store based on given criteria and metrics.\n\n        Parameters:\n        - evaluation_criteria (Dict[str, Any]): The criteria and metrics to evaluate the experiments.\n\n        Returns:\n        - Any: The evaluation results, which may vary depending on the evaluation criteria.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def benchmark_experiments(self, benchmarking_data: Dict[str, Any]) -> Any:\n        \"\"\"\n        Benchmarks the experiments against each other or predefined standards.\n\n        Parameters:\n        - benchmarking_data (Dict[str, Any]): Data and parameters for benchmarking the experiments.\n\n        Returns:\n        - Any: The benchmark results, which may vary depending on the benchmarking methodology.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/experiment_stores/IExperimentStore.py",
        "content": "```swarmauri/core/experiment_stores/IExperimentStore.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Union\nfrom swarmauri.core.documents.IExperimentDocument import IExperimentDocument\n\nclass IExperimentStore(ABC):\n    \"\"\"\n    Interface for an Experiment Store that manages experimental documents and supports\n    operations related to experimenting, evaluating, testing, and benchmarking.\n    \"\"\"\n    @abstractmethod\n    def add_experiment(self, experiment: IExperimentDocument) -> None:\n        \"\"\"\n        Stores a single experiment in the experiment store.\n\n        Parameters:\n        - experiment (IExperimentDocument): The experimental document to be stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_experiments(self, experiments: List[IExperimentDocument]) -> None:\n        \"\"\"\n        Stores multiple experiments in the experiment store.\n\n        Parameters:\n        - experiments (List[IExperimentDocument]): The list of experimental documents to be stored.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_experiment(self, experiment_id: str) -> Union[IExperimentDocument, None]:\n        \"\"\"\n        Retrieves an experimental document by its ID.\n\n        Parameters:\n        - experiment_id (str): The unique identifier of the experiment.\n\n        Returns:\n        - Union[IExperimentDocument, None]: The requested experimental document, or None if not found.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_experiments(self) -> List[IExperimentDocument]:\n        \"\"\"\n        Retrieves all experimental documents stored in the experiment store.\n\n        Returns:\n        - List[IExperimentDocument]: A list of all experimental documents.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_experiment(self, experiment_id: str, updated_experiment: IExperimentDocument) -> None:\n        \"\"\"\n        Updates an experimental document in the experiment store.\n\n        Parameters:\n        - experiment_id (str): The unique identifier of the experiment to update.\n        - updated_experiment (IExperimentDocument): The updated experimental document.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_experiment(self, experiment_id: str) -> None:\n        \"\"\"\n        Deletes an experimental document from the experiment store by its ID.\n\n        Parameters:\n        - experiment_id (str): The unique identifier of the experimental document to be deleted.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/agent_factories/IAgentFactory.py",
        "content": "```swarmauri/core/agent_factories/IAgentFactory.py\nfrom abc import ABC, abstractmethod\nfrom typing import Type, Any\nfrom datetime import datetime\n\nclass IAgentFactory(ABC):\n    \"\"\"\n    Interface for Agent Factories, extended to include properties like ID, name, type,\n    creation date, and last modification date.\n    \"\"\"\n\n    @abstractmethod\n    def create_agent(self, agent_type: str, **kwargs) -> Any:\n        pass\n\n    @abstractmethod\n    def register_agent(self, agent_type: str, constructor: Type[Any]) -> None:\n        pass\n\n    # Abstract properties and setters\n    @property\n    @abstractmethod\n    def id(self) -> str:\n        \"\"\"Unique identifier for the factory instance.\"\"\"\n        pass\n\n    @id.setter\n    @abstractmethod\n    def id(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Name of the factory.\"\"\"\n        pass\n\n    @name.setter\n    @abstractmethod\n    def name(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def type(self) -> str:\n        \"\"\"Type of agents this factory produces.\"\"\"\n        pass\n\n    @type.setter\n    @abstractmethod\n    def type(self, value: str) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def date_created(self) -> datetime:\n        \"\"\"The creation date of the factory instance.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def last_modified(self) -> datetime:\n        \"\"\"Date when the factory was last modified.\"\"\"\n        pass\n\n    @last_modified.setter\n    @abstractmethod\n    def last_modified(self, value: datetime) -> None:\n        pass\n\n    def __hash__(self):\n        \"\"\"\n        The __hash__ method allows objects of this class to be used in sets and as dictionary keys.\n        __hash__ should return an integer and be defined based on immutable properties.\n        This is generally implemented directly in concrete classes rather than in the interface,\n        but it's declared here to indicate that implementing classes must provide it.\n        \"\"\"\n        pass\n\n   \n```"
    },
    {
        "document_name": "swarmauri/core/agent_factories/__init__.py",
        "content": "```swarmauri/core/agent_factories/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/core/agent_factories/IExportConf.py",
        "content": "```swarmauri/core/agent_factories/IExportConf.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict\n\nclass IExportConf(ABC):\n    \"\"\"\n    Interface for exporting configurations related to agent factories.\n    \n    Implementing classes are expected to provide functionality for representing\n    the factory's configuration as a dictionary, JSON string, or exporting to a file.\n    \"\"\"\n\n    @abstractmethod\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Serializes the agent factory's configuration to a dictionary.\n        \n        Returns:\n            Dict[str, Any]: A dictionary representation of the factory's configuration.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def to_json(self) -> str:\n        \"\"\"\n        Serializes the agent factory's configuration to a JSON string.\n        \n        Returns:\n            str: A JSON string representation of the factory's configuration.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def to_file(self, file_path: str) -> None:\n        \"\"\"\n        Exports the agent factory's configuration to a file in a suitable format.\n        \n        Parameters:\n            file_path (str): The path to the file where the configuration should be saved.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/embeddings/IFeature.py",
        "content": "```swarmauri/core/embeddings/IFeature.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any\n\nclass IFeature(ABC):\n\n    @abstractmethod\n    def extract_features(self) -> List[Any]:\n        pass\n    \n\n```"
    },
    {
        "document_name": "swarmauri/core/embeddings/ISaveModel.py",
        "content": "```swarmauri/core/embeddings/ISaveModel.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\nclass ISaveModel(ABC):\n    \"\"\"\n    Interface to abstract the ability to save and load models.\n    \"\"\"\n\n    @abstractmethod\n    def save_model(self, path: str) -> None:\n        \"\"\"\n        Saves the model to the specified directory.\n\n        Parameters:\n        - path (str): The directory path where the model will be saved.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_model(self, path: str) -> Any:\n        \"\"\"\n        Loads a model from the specified directory.\n\n        Parameters:\n        - path (str): The directory path from where the model will be loaded.\n\n        Returns:\n        - Returns an instance of the loaded model.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/core/embeddings/IVectorize.py",
        "content": "```swarmauri/core/embeddings/IVectorize.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union, Any\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass IVectorize(ABC):\n    \"\"\"\n    Interface for converting text to vectors. \n    Implementations of this interface transform input text into numerical \n    vectors that can be used in machine learning models, similarity calculations, \n    and other vector-based operations.\n    \"\"\"\n    @abstractmethod\n    def fit(self, data: Union[str, Any]) -> None:\n        pass\n    \n    @abstractmethod\n    def transform(self, data: Union[str, Any]) -> List[IVector]:\n        pass\n\n    @abstractmethod\n    def fit_transform(self, data: Union[str, Any]) -> List[IVector]:\n        pass\n\n    @abstractmethod\n    def infer_vector(self, data: Union[str, Any], *args, **kwargs) -> IVector:\n        pass \n```"
    },
    {
        "document_name": "swarmauri/core/embeddings/__init__.py",
        "content": "```swarmauri/core/embeddings/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/core/schema_converters/ISchemaConvert.py",
        "content": "```swarmauri/core/schema_converters/ISchemaConvert.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict\nfrom swarmauri.core.tools.ITool import ITool\n\nclass ISchemaConvert(ABC):\n\n    @abstractmethod\n    def convert(self, tool: ITool) -> Dict[str, Any]:\n        raise NotImplementedError(\"Subclasses must implement the convert method.\")\n\n```"
    },
    {
        "document_name": "swarmauri/core/schema_converters/__init__.py",
        "content": "```swarmauri/core/schema_converters/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/__init__.py",
        "content": "```swarmauri/experimental/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/RemoteUniversalBase.py",
        "content": "```swarmauri/experimental/RemoteUniversalBase.py\nimport requests\nimport hashlib\nfrom functools import wraps\nfrom uuid import uuid4\nimport inspect\n\ndef remote_local_transport(cls):\n    original_init = cls.__init__\n    def init_wrapper(self, *args, **kwargs):\n        host = kwargs.pop('host', None)\n        resource = kwargs.get('resource', cls.__name__)\n        owner = kwargs.get('owner')\n        name = kwargs.get('name')\n        id = kwargs.get('id')\n        #path = kwargs.get('path')\n        if host:\n            #self.is_remote = True\n            self.host = host\n            self.resource = resource\n            self.owner = owner\n            self.id = id\n            #self.path = path\n            url = f\"{host}/{owner}/{resource}/{id}\"\n            data = {\"class_name\": cls.__name__, \"owner\": owner, \"name\": name, **kwargs}\n            response = requests.post(url, json=data)\n            if not response.ok:\n                raise Exception(f\"Failed to initialize remote {cls.__name__}: {response.text}\")\n        else:\n            original_init(self, owner, name, **kwargs)  # Ensure proper passing of positional arguments\n\n    setattr(cls, '__init__', init_wrapper)\n\n    for attr_name, attr_value in cls.__dict__.items():\n        if callable(attr_value) and not attr_name.startswith(\"_\"):\n            setattr(cls, attr_name, method_wrapper(attr_value))\n        elif isinstance(attr_value, property):\n            prop_get = attr_value.fget and method_wrapper(attr_value.fget)\n            prop_set = attr_value.fset and method_wrapper(attr_value.fset)\n            prop_del = attr_value.fdel and method_wrapper(attr_value.fdel)\n            setattr(cls, attr_name, property(prop_get, prop_set, prop_del, attr_value.__doc__))\n    return cls\n\n\ndef method_wrapper(method):\n    @wraps(method)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        if getattr(self, 'host'):\n            print('[x] Executing remote call...')\n            url = f\"{self.path}\".lower()\n            response = requests.post(url, json={\"args\": args[1:], \"kwargs\": kwargs})\n            if response.ok:\n                return response.json()\n            else:\n                raise Exception(f\"Remote method call failed: {response.text}\")\n        else:\n            return method(*args, **kwargs)\n    return wrapper\n\nclass RemoteLocalMeta(type):\n    def __new__(metacls, name, bases, class_dict):\n        cls = super().__new__(metacls, name, bases, class_dict)\n        if bases:  # This prevents BaseComponent itself from being decorated\n            cls = remote_local_transport(cls)\n        cls.class_hash = cls._calculate_class_hash()\n        return cls\n\n    def _calculate_class_hash(cls):\n        sig_hash = hashlib.sha256()\n        for attr_name, attr_value in cls.__dict__.items():\n            if callable(attr_value) and not attr_name.startswith(\"_\"):\n                sig = inspect.signature(attr_value)\n                sig_hash.update(str(sig).encode())\n        return sig_hash.hexdigest()\n    \n\nclass BaseComponent(metaclass=RemoteLocalMeta):\n    version = \"1.0.0\"  # Semantic versioning initialized here\n    def __init__(self, owner, name, host=None, members=[], resource=None):\n        self.id = uuid4()\n        self.owner = owner\n        self.name = name\n        self.host = host  \n        #self.is_remote = bool(self.host) \n        self.members = members\n        self.resource = resource if resource else self.__class__.__name__\n        self.path = f\"{self.host if self.host else ''}/{self.owner}/{self.resource}/{self.id}\"\n\n    @property\n    def is_remote(self):\n        return bool(self.host)\n\n    @classmethod\n    def public_interfaces(cls):\n        methods = []\n        for attr_name in dir(cls):\n            # Retrieve the attribute\n            attr_value = getattr(cls, attr_name)\n            # Check if it's callable or a property and not a private method\n            if (callable(attr_value) and not attr_name.startswith(\"_\")) or isinstance(attr_value, property):\n                methods.append(attr_name)\n        return methods\n\n    @classmethod\n    def is_method_registered(cls, method_name):\n        \"\"\"\n        Checks if a public method with the given name is registered on the class.\n        Args:\n            method_name (str): The name of the method to check.\n        Returns:\n            bool: True if the method is registered, False otherwise.\n        \"\"\"\n        return method_name in cls.public_interfaces()\n\n    @classmethod\n    def method_with_signature(cls, input_signature):\n        \"\"\"\n        Checks if there is a method with the given signature available in the class.\n        \n        Args:\n            input_signature (str): The string representation of the method signature to check.\n        \n        Returns:\n            bool: True if a method with the input signature exists, False otherwise.\n        \"\"\"\n        for method_name in cls.public_interfaces():\n            method = getattr(cls, method_name)\n            if callable(method):\n                sig = str(inspect.signature(method))\n                if sig == input_signature:\n                    return True\n        return False\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/__init__.py",
        "content": "```swarmauri/experimental/tools/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/CypherQueryTool.py",
        "content": "```swarmauri/experimental/tools/CypherQueryTool.py\nimport json\nfrom neo4j import GraphDatabase\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass CypherQueryTool(ToolBase):\n    def __init__(self, uri: str, user: str, password: str):\n        self.uri = uri\n        self.user = user\n        self.password = password\n        \n        # Define only the 'query' parameter since uri, user, and password are set at initialization\n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"The Cypher query to execute.\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"CypherQueryTool\",\n                         description=\"Executes a Cypher query against a Neo4j database.\",\n                         parameters=parameters)\n\n    def _get_connection(self):\n        return GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n\n    def __call__(self, query) -> str:\n        # Establish connection to the database\n        driver = self._get_connection()\n        session = driver.session()\n\n        # Execute the query\n        result = session.run(query)\n        records = result.data()\n\n        # Close the connection\n        session.close()\n        driver.close()\n\n        # Convert records to JSON string, assuming it's JSON serializable\n        return json.dumps(records)\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/FileDownloaderTool.py",
        "content": "```swarmauri/experimental/tools/FileDownloaderTool.py\nimport requests\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass FileDownloaderTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"The URL of the file to download\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"FileDownloaderTool\",\n                         description=\"Downloads a file from a specified URL into memory.\",\n                         parameters=parameters)\n    \n    def __call__(self, url: str) -> bytes:\n        \"\"\"\n        Downloads a file from the given URL into memory.\n        \n        Parameters:\n        - url (str): The URL of the file to download.\n        \n        Returns:\n        - bytes: The content of the downloaded file.\n        \"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raises an HTTPError if the request resulted in an error\n            return response.content\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Failed to download file from '{url}'. Error: {e}\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/LinkedInArticleTool.py",
        "content": "```swarmauri/experimental/tools/LinkedInArticleTool.py\nimport requests\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass LinkedInArticleTool(ToolBase):\n    \"\"\"\n    A tool to post articles on LinkedIn using the LinkedIn API.\n    \"\"\"\n    def __init__(self, access_token):\n        \"\"\"\n        Initializes the LinkedInArticleTool with the necessary access token.\n        \n        Args:\n            access_token (str): The OAuth access token for authenticating with the LinkedIn API.\n        \"\"\"\n        super().__init__(name=\"LinkedInArticleTool\",\n                         description=\"A tool for posting articles on LinkedIn.\",\n                         parameters=[\n                             Parameter(name=\"title\", type=\"string\", description=\"The title of the article\", required=True),\n                             Parameter(name=\"text\", type=\"string\", description=\"The body text of the article\", required=True),\n                             Parameter(name=\"visibility\", type=\"string\", description=\"The visibility of the article\", required=True, enum=[\"anyone\", \"connectionsOnly\"])\n                         ])\n        self.access_token = access_token\n        \n    def __call__(self, title: str, text: str, visibility: str = \"anyone\") -> str:\n        \"\"\"\n        Posts an article on LinkedIn.\n\n        Args:\n            title (str): The title of the article.\n            text (str): The body text of the article.\n            visibility (str): The visibility of the article, either \"anyone\" or \"connectionsOnly\".\n\n        Returns:\n            str: A message indicating the success or failure of the post operation.\n        \"\"\"\n        # Construct the request URL and payload according to LinkedIn API documentation\n        url = 'https://api.linkedin.com/v2/ugcPosts'\n        headers = {\n            'Authorization': f'Bearer {self.access_token}',\n            'X-Restli-Protocol-Version': '2.0.0',\n            'Content-Type': 'application/json'\n        }\n        \n        payload = {\n            \"author\": \"urn:li:person:YOUR_PERSON_ID_HERE\",\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\n                        \"text\": text\n                    },\n                    \"shareMediaCategory\": \"ARTICLE\",\n                    \"media\": [\n                        {\n                            \"status\": \"READY\",\n                            \"description\": {\n                                \"text\": title\n                            },\n                            \"originalUrl\": \"URL_OF_THE_ARTICLE_OR_IMAGE\",\n                            \"visibility\": {\n                                \"com.linkedin.ugc.MemberNetworkVisibility\": visibility.upper()\n                            }\n                        }\n                    ]\n                }\n            },\n            \"visibility\": {\n                \"com.linkedin.ugc.MemberNetworkVisibility\": visibility.upper()\n            }\n        }\n     \n        # Make the POST request to LinkedIn's API\n        response = requests.post(url, headers=headers, json=payload)\n        \n        if response.status_code == 201:\n            return f\"Article posted successfully: {response.json().get('id')}\"\n        else:\n            return f\"Failed to post the article. Status Code: {response.status_code} - {response.text}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/OutlookSendMailTool.py",
        "content": "```swarmauri/experimental/tools/OutlookSendMailTool.py\nimport requests\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass OutlookSendMailTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"recipient\",\n                type=\"string\",\n                description=\"The email address of the recipient\",\n                required=True\n            ),\n            Parameter(\n                name=\"subject\",\n                type=\"string\",\n                description=\"The subject of the email\",\n                required=True\n            ),\n            Parameter(\n                name=\"body\",\n                type=\"string\",\n                description=\"The HTML body of the email\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"OutlookSendMailTool\", \n                         description=\"Sends an email using the Outlook service.\",\n                         parameters=parameters)\n\n        # Add your Microsoft Graph API credentials and endpoint URL here\n        self.tenant_id = \"YOUR_TENANT_ID\"\n        self.client_id = \"YOUR_CLIENT_ID\"\n        self.client_secret = \"YOUR_CLIENT_SECRET\"\n        self.scope = [\"https://graph.microsoft.com/.default\"]\n        self.token_url = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/v2.0/token\"\n        self.graph_endpoint = \"https://graph.microsoft.com/v1.0\"\n\n    def get_access_token(self):\n        data = {\n            \"client_id\": self.client_id,\n            \"scope\": \" \".join(self.scope),\n            \"client_secret\": self.client_secret,\n            \"grant_type\": \"client_credentials\"\n        }\n        response = requests.post(self.token_url, data=data)\n        response.raise_for_status()\n        return response.json().get(\"access_token\")\n\n    def __call__(self, recipient, subject, body):\n        access_token = self.get_access_token()\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        email_data = {\n            \"message\": {\n                \"subject\": subject,\n                \"body\": {\n                    \"contentType\": \"HTML\",\n                    \"content\": body\n                },\n                \"toRecipients\": [\n                    {\n                        \"emailAddress\": {\n                            \"address\": recipient\n                        }\n                    }\n                ]\n            }\n        }\n\n        send_mail_endpoint = f\"{self.graph_endpoint}/users/{self.client_id}/sendMail\"\n        response = requests.post(send_mail_endpoint, json=email_data, headers=headers)\n        if response.status_code == 202:\n            return \"Email sent successfully\"\n        else:\n            return f\"Failed to send email, status code {response.status_code}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/SQLite3QueryTool.py",
        "content": "```swarmauri/experimental/tools/SQLite3QueryTool.py\nimport sqlite3\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass SQLite3QueryTool(ToolBase):\n    def __init__(self, db_name: str):\n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"SQL query to execute\",\n                required=True\n            )\n        ]\n        super().__init__(name=\"SQLQueryTool\", \n                         description=\"Executes an SQL query and returns the results.\", \n                         parameters=parameters)\n        self.db_name = db_name\n\n    def __call__(self, query) -> str:\n        \"\"\"\n        Execute the provided SQL query.\n\n        Parameters:\n        - query (str): The SQL query to execute.\n\n        Returns:\n        - str: The results of the SQL query as a string.\n        \"\"\"\n        try:\n            connection = sqlite3.connect(self.db_name)  # Connect to the specific database file\n            cursor = connection.cursor()\n            \n            cursor.execute(query)\n            rows = cursor.fetchall()\n            result = \"\\n\".join(str(row) for row in rows)\n        except Exception as e:\n            result = f\"Error executing query: {e}\"\n        finally:\n            connection.close()\n        \n        return f\"Query Result:\\n{result}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/TwitterPostTool.py",
        "content": "```swarmauri/experimental/tools/TwitterPostTool.py\nfrom tweepy import Client\n\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass TwitterPostTool(ToolBase):\n    def __init__(self, bearer_token):\n        # Initialize parameters necessary for posting a tweet\n        parameters = [\n            Parameter(\n                name=\"status\",\n                type=\"string\",\n                description=\"The status message to post on Twitter\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"TwitterPostTool\", description=\"Post a status update on Twitter\", parameters=parameters)\n        \n        # Initialize Twitter API Client\n        self.client = Client(bearer_token=bearer_token)\n\n    def __call__(self, status: str) -> str:\n        \"\"\"\n        Posts a status on Twitter.\n\n        Args:\n            status (str): The status message to post.\n\n        Returns:\n            str: A confirmation message including the tweet's URL if successful.\n        \"\"\"\n        try:\n            # Using Tweepy to send a tweet\n            response = self.client.create_tweet(text=status)\n            tweet_id = response.data['id']\n            # Constructing URL to the tweet - Adjust the URL to match Twitter API v2 structure if needed\n            tweet_url = f\"https://twitter.com/user/status/{tweet_id}\"\n            return f\"Tweet successful: {tweet_url}\"\n        except Exception as e:\n            return f\"An error occurred: {e}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/__init__.py",
        "content": "```swarmauri/experimental/conversations/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/SemanticConversation.py",
        "content": "```swarmauri/experimental/conversations/SemanticConversation.py\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Dict, Union\nfrom ...core.messages.IMessage import IMessage\nfrom ...core.conversations.IConversation import IConversation\n\nclass SemanticConversation(IConversation, ABC):\n    \"\"\"\n    A concrete implementation of the Conversation class that includes semantic routing.\n    Semantic routing involves analyzing the content of messages to understand their intent\n    or category and then routing them to appropriate handlers based on that analysis.\n\n    This class requires subclasses to implement the _analyze_message method for semantic analysis.\n    \"\"\"\n\n\n    @abstractmethod\n    def register_handler(self, category: str, handler: Callable[[IMessage], None]):\n        \"\"\"\n        Registers a message handler for a specific semantic category.\n\n        Args:\n            category (str): The category of messages this handler should process.\n            handler (Callable[[Message], None]): The function to call for messages of the specified category.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history and routes it to the appropriate handler based on its semantic category.\n\n        Args:\n            message (Message): The message to be added and processed.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _analyze_message(self, message: IMessage) -> Union[str, None]:\n        \"\"\"\n        Analyzes the content of a message to determine its semantic category.\n\n        This method must be implemented by subclasses to provide specific logic for semantic analysis.\n\n        Args:\n            message (Message): The message to analyze.\n\n        Returns:\n            Union[str, None]: The semantic category of the message, if determined; otherwise, None.\n\n        Raises:\n            NotImplementedError: If the method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the _analyze_message method to provide semantic analysis.\")\n\n    # Additional methods as needed for message retrieval, history management, etc., inherited from Conversation\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/ConsensusBuildingConversation.py",
        "content": "```swarmauri/experimental/conversations/ConsensusBuildingConversation.py\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.messages.IMessage import IMessage\n\n\nclass ConsensusBuildingMessage(IMessage):\n    def __init__(self, sender_id: str, content: str, message_type: str):\n        self._sender_id = sender_id\n        self._content = content\n        self._role = 'consensus_message'\n        self._message_type = message_type\n\n    @property\n    def role(self) -> str:\n        return self._role\n\n    @property\n    def content(self) -> str:\n        return self._content\n\n    def as_dict(self) -> dict:\n        return {\n            \"sender_id\": self._sender_id,\n            \"content\": self._content,\n            \"message_type\": self._message_type\n        }\n\n\nclass ConsensusBuildingConversation(IConversation):\n    def __init__(self, topic: str, participants: list):\n        self.topic = topic\n        self.participants = participants  # List of agent IDs\n        self._history = []  # Stores all messages exchanged in the conversation\n        self.proposal_votes = {}  # Tracks votes for each proposal\n\n    @property\n    def history(self) -> list:\n        return self._history\n\n    def add_message(self, message: IMessage):\n        if not isinstance(message, ConsensusBuildingMessage):\n            raise ValueError(\"Only instances of ConsensusBuildingMessage are accepted\")\n        self._history.append(message)\n\n    def get_last(self) -> IMessage:\n        if self._history:\n            return self._history[-1]\n        return None\n\n    def clear_history(self) -> None:\n        self._history.clear()\n\n    def as_dict(self) -> list:\n        return [message.as_dict() for message in self._history]\n\n    def initiate_consensus(self, initiator_id: str, proposal=None):\n        \"\"\"Starts the conversation with an initial proposal, if any.\"\"\"\n        initiate_message = ConsensusBuildingMessage(initiator_id, proposal, \"InitiateConsensusMessage\")\n        self.add_message(initiate_message)\n\n    def add_proposal(self, sender_id: str, proposal: str):\n        \"\"\"Adds a proposal to the conversation.\"\"\"\n        proposal_message = ConsensusBuildingMessage(sender_id, proposal, \"ProposalMessage\")\n        self.add_message(proposal_message)\n\n    def add_comment(self, sender_id: str, comment: str):\n        \"\"\"Adds a comment or feedback regarding a proposal.\"\"\"\n        comment_message = ConsensusBuildingMessage(sender_id, comment, \"CommentMessage\")\n        self.add_message(comment_message)\n\n    def vote(self, sender_id: str, vote: str):\n        \"\"\"Registers a vote for a given proposal.\"\"\"\n        vote_message = ConsensusBuildingMessage(sender_id, vote, \"VoteMessage\")\n        self.add_message(vote_message)\n        # Count the vote\n        self.proposal_votes[vote] = self.proposal_votes.get(vote, 0) + 1\n\n    def check_agreement(self):\n        \"\"\"\n        Checks if there is a consensus on any proposal.\n        A simple majority (>50% of the participants) is required for consensus.\n        \"\"\"\n        consensus_threshold = len(self.participants) / 2  # Define consensus as a simple majority\n\n        for proposal, votes in self.proposal_votes.items():\n            if votes > consensus_threshold:\n                # A consensus has been reached\n                return True, f\"Consensus reached on proposal: {proposal} with {votes} votes.\"\n\n        # If no consensus is reached\n        return False, \"No consensus reached.\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/SharedConversation.py",
        "content": "```swarmauri/experimental/conversations/SharedConversation.py\nimport inspect\nfrom threading import Lock\nfrom typing import Optional, Dict, List, Tuple\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.standard.messages.concrete.HumanMessage import HumanMessage\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n\nclass SharedConversation(ConversationBase):\n    \"\"\"\n    A thread-safe conversation class that supports individual system contexts for each SwarmAgent.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = Lock()  # A lock to ensure thread safety\n        self._agent_system_contexts: Dict[str, SystemMessage] = {}  # Store system contexts for each agent\n        self._history: List[Tuple[str, IMessage]] = []  # Stores tuples of (sender_id, IMessage)\n\n\n    @property\n    def history(self):\n        history = []\n        for each in self._history:\n            history.append((each[0], each[1]))\n        return history\n\n    def add_message(self, message: IMessage, sender_id: str):\n        with self._lock:\n            self._history.append((sender_id, message))\n\n    def reset_messages(self) -> None:\n        self._history = []\n        \n\n    def _get_caller_name(self) -> Optional[str]:\n        for frame_info in inspect.stack():\n            # Check each frame for an instance with a 'name' attribute in its local variables\n            local_variables = frame_info.frame.f_locals\n            for var_name, var_value in local_variables.items():\n                if hasattr(var_value, 'name'):\n                    # Found an instance with a 'name' attribute. Return its value.\n                    return getattr(var_value, 'name')\n        # No suitable caller found\n        return None\n\n    def as_dict(self) -> List[Dict]:\n        caller_name = self._get_caller_name()\n        history = []\n\n        with self._lock:\n            # If Caller is not one of the agents, then give history\n            if caller_name not in self._agent_system_contexts.keys():\n                for sender_id, message in self._history:\n                    history.append((sender_id, message.as_dict()))\n                \n                \n            else:\n                system_context = self.get_system_context(caller_name)\n                #print(caller_name, system_context, type(system_context))\n                if type(system_context) == str:\n                    history.append(SystemMessage(system_context).as_dict())\n                else:\n                    history.append(system_context.as_dict())\n                    \n                for sender_id, message in self._history:\n                    #print(caller_name, sender_id, message, type(message))\n                    if sender_id == caller_name:\n                        if message.__class__.__name__ == 'AgentMessage' or 'FunctionMessage':\n                            # The caller is the sender; treat as AgentMessage\n                            history.append(message.as_dict())\n                            \n                            # Print to see content that is empty.\n                            #if not message.content:\n                                #print('\\n\\t\\t\\t=>', message, message.content)\n                    else:\n                        if message.content:\n                            # The caller is not the sender; treat as HumanMessage\n                            history.append(HumanMessage(message.content).as_dict())\n        return history\n    \n    def get_last(self) -> IMessage:\n        with self._lock:\n            return super().get_last()\n\n\n    def clear_history(self):\n        with self._lock:\n            super().clear_history()\n\n\n        \n\n    def set_system_context(self, agent_id: str, context: SystemMessage):\n        \"\"\"\n        Sets the system context for a specific agent.\n\n        Args:\n            agent_id (str): Unique identifier for the agent.\n            context (SystemMessage): The context message to be set for the agent.\n        \"\"\"\n        with self._lock:\n            self._agent_system_contexts[agent_id] = context\n\n    def get_system_context(self, agent_id: str) -> Optional[SystemMessage]:\n        \"\"\"\n        Retrieves the system context for a specific agent.\n\n        Args:\n            agent_id (str): Unique identifier for the agent.\n\n        Returns:\n            Optional[SystemMessage]: The context message of the agent, or None if not found.\n        \"\"\"\n        return self._agent_system_contexts.get(agent_id, None)\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/__init__.py",
        "content": "```swarmauri/experimental/models/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/SageMaker.py",
        "content": "```swarmauri/experimental/models/SageMaker.py\nimport json\nimport boto3\nfrom ...core.models.IModel import IModel\n\n\nclass AWSSageMakerModel(IModel):\n    def __init__(self, access_key: str, secret_access_key: str, region_name: str, model_name: str):\n        \"\"\"\n        Initialize the AWS SageMaker model with AWS credentials, region, and the model name.\n\n        Parameters:\n        - access_key (str): AWS access key ID.\n        - secret_access_key (str): AWS secret access key.\n        - region_name (str): The region where the SageMaker model is deployed.\n        - model_name (str): The name of the SageMaker model.\n        \"\"\"\n        self.access_key = access_key\n        self.secret_access_key = secret_access_key\n        self.region_name = region_name\n        self.client = boto3.client('sagemaker-runtime',\n                                   aws_access_key_id=access_key,\n                                   aws_secret_access_key=secret_access_key,\n                                   region_name=region_name)\n        super().__init__(model_name)\n\n    def predict(self, payload: str, content_type: str='application/json') -> dict:\n        \"\"\"\n        Generate predictions using the AWS SageMaker model.\n\n        Parameters:\n        - payload (str): Input data in JSON format.\n        - content_type (str): The MIME type of the input data (default: 'application/json').\n        \n        Returns:\n        - dict: The predictions returned by the model.\n        \"\"\"\n        endpoint_name = self.model_name  # Assuming the model name is also the endpoint name\n        response = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                               Body=payload,\n                                               ContentType=content_type)\n        result = json.loads(response['Body'].read().decode())\n        return result\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/HierarchicalAttentionModel.py",
        "content": "```swarmauri/experimental/models/HierarchicalAttentionModel.py\nimport tensorflow as tf\nfrom swarmauri.core.models.IModel import IModel\nfrom typing import Any\n\nclass HierarchicalAttentionModel(IModel):\n    def __init__(self, model_name: str):\n        self._model_name = model_name\n        self._model = None  # This will hold the TensorFlow model with attention\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n\n    @model_name.setter\n    def model_name(self, value: str) -> None:\n        self._model_name = value\n\n    def load_model(self) -> None:\n        \"\"\"\n        Here, we define and compile the TensorFlow model described earlier.\n        \"\"\"\n        # The following code is adapted from the attention model example provided earlier\n        vocab_size = 10000  # Size of the vocabulary\n        embedding_dim = 256  # Dimension of the embedding layer\n        sentence_length = 100  # Max length of a sentence\n        num_sentences = 10  # Number of sentences in a document\n        units = 128  # Dimensionality of the output space of GRU\n        \n        # Word-level attention layer\n        word_input = tf.keras.layers.Input(shape=(sentence_length,), dtype='int32')\n        embedded_word = tf.keras.layers.Embedding(vocab_size, embedding_dim)(word_input)\n        word_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))(embedded_word)\n        word_attention_layer = tf.keras.layers.Attention(use_scale=True, return_attention_scores=True)\n        word_attention_output, word_attention_weights = word_attention_layer([word_gru, word_gru], return_attention_scores=True)\n        word_encoder_with_attention = tf.keras.Model(inputs=word_input, outputs=[word_attention_output, word_attention_weights])\n        \n        # Sentence-level attention layer\n        sentence_input = tf.keras.layers.Input(shape=(num_sentences, sentence_length), dtype='int32')\n        sentence_encoder_with_attention = tf.keras.layers.TimeDistributed(word_encoder_with_attention)(sentence_input)\n        sentence_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))(sentence_encoder_with_attention[0])\n        sentence_attention_layer = tf.keras.layers.Attention(use_scale=True, return_attention_scores=True)\n        sentence_attention_output, sentence_attention_weights = sentence_attention_layer([sentence_gru, sentence_gru], return_attention_scores=True)\n        doc_representation = tf.keras.layers.Dense(units, activation='tanh')(sentence_attention_output)\n        \n        # Classifier\n        classifier = tf.keras.layers.Dense(1, activation='sigmoid')(doc_representation)\n        \n        # The model\n        self._model = tf.keras.Model(inputs=sentence_input, outputs=[classifier, sentence_attention_weights])\n        self._model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    def predict(self, input_data: Any) -> Any:\n        \"\"\"\n        Predict method to use the loaded model for making predictions.\n\n        This example assumes `input_data` is preprocessed appropriately for the model's expected input.\n        \"\"\"\n        if self._model is None:\n            raise ValueError(\"Model is not loaded. Call `load_model` before prediction.\")\n            \n        # Predicting with the model\n        predictions, attention_weights = self._model.predict(input_data)\n        \n        # Additional logic to handle and package the predictions and attention weights could be added here\n        \n        return predictions, attention_weights\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/__init__.py",
        "content": "```swarmauri/experimental/utils/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/get_last_frame.py",
        "content": "```swarmauri/experimental/utils/get_last_frame.py\nimport inspect\n\ndef child_function(arg):\n    # Get the stack frame of the caller\n    caller_frame = inspect.currentframe().f_back\n    # Get the name of the caller function\n    caller_name = caller_frame.f_code.co_name\n    # Inspect the arguments of the caller function\n    args, _, _, values = inspect.getargvalues(caller_frame)\n    # Assuming the caller has only one argument for simplicity\n    arg_name = args[0]\n    arg_value = values[arg_name]\n    print(f\"Caller Name: {caller_name}, Argument Name: {arg_name}, Argument Value: {arg_value}\")\n\ndef caller_function(l):\n    child_function(l)\n\n# Example usage\ncaller_function(\"Hello\")\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/save_schema.py",
        "content": "```swarmauri/experimental/utils/save_schema.py\nimport inspect\nimport random\n\nclass Storage:\n    def __init__(self):\n        self.logs = []\n\n    def log(self, log_data):\n        self.logs.append(log_data)\n\n    def print_logs(self):\n        for log in self.logs:\n            print(log)\n\nclass Loggable:\n    def __init__(self, name, storage):\n        self.name = name\n        self.storage = storage\n\n    def log_call(self, *args, **kwargs):\n        # Inspect the call stack to get the caller's details\n        caller_frame = inspect.stack()[2]\n        caller_name = inspect.currentframe().f_back.f_code.co_name\n        #caller_name = caller_frame.function\n        module = inspect.getmodule(caller_frame[0])\n        module_name = module.__name__ if module else 'N/A'\n\n        # Log all relevant details\n        log_data = {\n            'caller_name': caller_name,\n            'module_name': module_name,\n            'called_name': self.name,\n            'called_function': caller_frame[3], # The function in which log_call was invoked\n            'args': args,\n            'kwargs': kwargs\n        }\n        self.storage.log(log_data)\n\nclass Caller(Loggable):\n    def __init__(self, name, storage, others):\n        super().__init__(name, storage)\n        self.others = others\n\n    def __call__(self, *args, **kwargs):\n        if len(self.storage.logs)<10:\n            self.log_call(*args, **kwargs)\n            # Randomly call another without causing recursive calls\n            if args:  # Ensures it's not the first call without actual target\n                next_caller_name = random.choice([name for name in self.others if name != self.name])\n                self.others[next_caller_name](self.name)\n\n# Initialize storage and callers\nstorage = Storage()\nothers = {}\n\n# Creating callers\nalice = Caller('Alice', storage, others)\nbob = Caller('Bob', storage, others)\ncharlie = Caller('Charlie', storage, others)\ndan = Caller('Dan', storage, others)\n\nothers['Alice'] = alice\nothers['Bob'] = bob\nothers['Charlie'] = charlie\nothers['Dan'] = dan\n\n# Simulate the calls\ndan(1, taco=23)\n\n# Print the logs\nstorage.print_logs()\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/ISerializable.py",
        "content": "```swarmauri/experimental/utils/ISerializable.py\n\n# class Serializable:\n#     def serialize(self):\n#         raise NotImplementedError(\"Serialization method not implemented\")\n    \n#     @classmethod\n#     def deserialize(cls, data):\n#         raise NotImplementedError(\"Deserialization method not implemented\")\n        \n        \n# class ToolAgent(Serializable):\n#     def serialize(self):\n#         # Simplified example, adapt according to actual attributes\n#         return {\"type\": self.__class__.__name__, \"state\": {\"model_name\": self.model.model_name}}\n\n#     @classmethod\n#     def deserialize(cls, data):\n#         # This method should instantiate the object based on the serialized state.\n#         # Example assumes the presence of model_name in the serialized state.\n#         model = OpenAIToolModel(api_key=\"api_key_placeholder\", model_name=data[\"state\"][\"model_name\"])\n#         return cls(model=model, conversation=None, toolkit=None)  # Simplify, omit optional parameters for illustration\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/log_prompt_response.py",
        "content": "```swarmauri/experimental/utils/log_prompt_response.py\nimport sqlite3\nfrom functools import wraps\n\ndef log_prompt_response(db_path):\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extracting the 'message' parameter from args which is assumed to be the first argument\n            message = args[0]  \n            response = await func(*args, **kwargs)\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n            \n            # Create table if it doesn't exist\n            cursor.execute('''CREATE TABLE IF NOT EXISTS prompts_responses\n                            (id INTEGER PRIMARY KEY AUTOINCREMENT, \n                             prompt TEXT, \n                             response TEXT)''')\n            \n            # Insert a new record\n            cursor.execute('''INSERT INTO prompts_responses (prompt, response) \n                            VALUES (?, ?)''', (message, response))\n            conn.commit()\n            conn.close()\n            return response\n        \n        return wrapper\n    return decorator\n```"
    },
    {
        "document_name": "swarmauri/experimental/parsers/__init__.py",
        "content": "```swarmauri/experimental/parsers/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/parsers/PDFToTextParser.py",
        "content": "```swarmauri/experimental/parsers/PDFToTextParser.py\nimport fitz  # PyMuPDF\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.ConcreteDocument import ConcreteDocument\n\nclass PDFtoTextParser(IParser):\n    \"\"\"\n    A parser to extract text from PDF files.\n    \"\"\"\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses a PDF file and extracts its text content as Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The path to the PDF file.\n\n        Returns:\n        - List[IDocument]: A list with a single IDocument instance containing the extracted text.\n        \"\"\"\n        # Ensure data is a valid str path to a PDF file\n        if not isinstance(data, str):\n            raise ValueError(\"PDFtoTextParser expects a file path in str format.\")\n\n        try:\n            # Open the PDF file\n            doc = fitz.open(data)\n            text = \"\"\n\n            # Extract text from each page\n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                text += page.get_text()\n\n            # Create a document with the extracted text\n            document = ConcreteDocument(doc_id=str(hash(data)), content=text, metadata={\"source\": data})\n            return [document]\n        \n        except Exception as e:\n            print(f\"An error occurred while parsing the PDF: {e}\")\n            return []\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/__init__.py",
        "content": "```swarmauri/experimental/vector_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/Word2VecDocumentStore.py",
        "content": "```swarmauri/experimental/vector_stores/Word2VecDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nimport gensim.downloader as api\n\nclass Word2VecDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self):\n        \"\"\"\n        Initializes the Word2VecDocumentStore.\n\n        Parameters:\n        - word2vec_model_path (Optional[str]): File path to a pre-trained Word2Vec model. \n                                               Leave None to use Gensim's pre-trained model.\n        - pre_trained (bool): If True, loads a pre-trained Word2Vec model. If False, an uninitialized model is used that requires further training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)  # Example parameters; adjust as needed\n        self.documents = []\n        self.metric = CosineDistance()\n\n    def add_document(self, document: EmbeddedDocument) -> None:\n        # Check if the document already has an embedding, if not generate one using _average_word_vectors\n        if not hasattr(document, 'embedding') or document.embedding is None:\n            words = document.content.split()  # Simple tokenization, consider using a better tokenizer\n            embedding = self._average_word_vectors(words)\n            document.embedding = embedding\n            print(document.embedding)\n        self.documents.append(document)\n        \n    def add_documents(self, documents: List[EmbeddedDocument]) -> None:\n        self.documents.extend(documents)\n        \n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n        \n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str, updated_document: EmbeddedDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n    def _average_word_vectors(self, words: List[str]) -> np.ndarray:\n        \"\"\"\n        Generate document vector by averaging its word vectors.\n        \"\"\"\n        word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n        print(word_vectors)\n        if word_vectors:\n            return np.mean(word_vectors, axis=0)\n        else:\n            return np.zeros(self.model.vector_size)\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[EmbeddedDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string based on Word2Vec embeddings.\n        \"\"\"\n        query_vector = self._average_word_vectors(query.split())\n        print('query_vector', query_vector)\n        # Compute similarity scores between the query and each document's stored embedding\n        similarities = self.metric.similarities(SimpleVector(query_vector), [SimpleVector(doc.embedding) for doc in self.documents if doc.embedding])\n        print('similarities', similarities)\n        # Retrieve indices of top_k most similar documents\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n        print('top_k_indices', top_k_indices)\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/TriplesDocumentStore.py",
        "content": "```swarmauri/experimental/vector_stores/TriplesDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom rdflib import Graph, URIRef, Literal, BNode\nfrom ampligraph.latent_features import ComplEx\nfrom ampligraph.evaluation import train_test_split_no_unseen\nfrom ampligraph.latent_features import EmbeddingModel\nfrom ampligraph.utils import save_model, restore_model\n\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nfrom swarmauri.standard.vectorizers.concrete.AmpligraphVectorizer import AmpligraphVectorizer\n\n\nclass TriplesDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self, rdf_file_path: str, model_path: Optional[str] = None):\n        \"\"\"\n        Initializes the TriplesDocumentStore.\n        \"\"\"\n        self.graph = Graph()\n        self.rdf_file_path = rdf_file_path\n        self.graph.parse(rdf_file_path, format='turtle')\n        self.documents = []\n        self.vectorizer = AmpligraphVectorizer()\n        self.model_path = model_path\n        if model_path:\n            self.model = restore_model(model_path)\n        else:\n            self.model = None\n        self.metric = CosineDistance()\n        self._load_documents()\n        if not self.model:\n            self._train_model()\n\n    def _train_model(self):\n        \"\"\"\n        Trains a model based on triples in the graph.\n        \"\"\"\n        # Extract triples for embedding model\n        triples = np.array([[str(s), str(p), str(o)] for s, p, o in self.graph])\n        # Split data\n        train, test = train_test_split_no_unseen(triples, test_size=0.1)\n        self.model = ComplEx(batches_count=100, seed=0, epochs=20, k=150, eta=1,\n                             optimizer='adam', optimizer_params={'lr': 1e-3},\n                             loss='pairwise', regularizer='LP', regularizer_params={'p': 3, 'lambda': 1e-5},\n                             verbose=True)\n        self.model.fit(train)\n        if self.model_path:\n            save_model(self.model, self.model_path)\n\n    def _load_documents(self):\n        \"\"\"\n        Load documents into the store from the RDF graph.\n        \"\"\"\n        for subj, pred, obj in self.graph:\n            doc_id = str(hash((subj, pred, obj)))\n            content = f\"{subj} {pred} {obj}\"\n            document = Document(content=content, doc_id=doc_id, metadata={})\n            self.documents.append(document)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Adds a single RDF triple document.\n        \"\"\"\n        subj, pred, obj = document.content.split()  # Splitting content into RDF components\n        self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.append(document)\n        self._train_model()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Adds multiple RDF triple documents.\n        \"\"\"\n        for document in documents:\n            subj, pred, obj = document.content.split()  # Assuming each document's content is \"subj pred obj\"\n            self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.extend(documents)\n        self._train_model()\n\n    # Implementation for get_document, get_all_documents, delete_document, update_document remains same as before\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string.\n        \"\"\"\n        if not self.model:\n            self._train_model()\n        query_vector = self.vectorizer.infer_vector(model=self.model, samples=[query])[0]\n        document_vectors = [self.vectorizer.infer_vector(model=self.model, samples=[doc.content])[0] for doc in self.documents]\n        similarities = self.metric.distances(SimpleVector(data=query_vector), [SimpleVector(vector) for vector in document_vectors])\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/ScannVectorStore.py",
        "content": "```swarmauri/experimental/vector_stores/ScannVectorStore.py\nimport numpy as np\nimport scann\nfrom typing import List, Dict, Union\n\nfrom swarmauri.core.vector_stores.IVectorStore import IVectorStore\nfrom swarmauri.core.vector_stores.ISimiliarityQuery import ISimilarityQuery\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\n\nclass ScannVectorStore(IVectorStore, ISimilarityQuery):\n    \"\"\"\n    A vector store that utilizes ScaNN (Scalable Nearest Neighbors) for efficient similarity searches.\n    \"\"\"\n\n    def __init__(self, dimension: int, num_leaves: int = 100, num_leaves_to_search: int = 10, reordering_num_neighbors: int = 100):\n        \"\"\"\n        Initialize the ScaNN vector store with given parameters.\n\n        Parameters:\n        - dimension (int): The dimensionality of the vectors being stored.\n        - num_leaves (int): The number of leaves for the ScaNN partitioning tree.\n        - num_leaves_to_search (int): The number of leaves to search for query time. Must be <= num_leaves.\n        - reordering_num_neighbors (int): The number of neighbors to re-rank based on the exact distance after searching leaves.\n        \"\"\"\n        self.dimension = dimension\n        self.num_leaves = num_leaves\n        self.num_leaves_to_search = num_leaves_to_search\n        self.reordering_num_neighbors = reordering_num_neighbors\n\n        self.searcher = None  # Placeholder for the ScaNN searcher initialized during building\n        self.dataset_vectors = []\n        self.id_to_metadata = {}\n\n    def _build_scann_searcher(self):\n        \"\"\"Build the ScaNN searcher based on current dataset vectors.\"\"\"\n        self.searcher = scann.ScannBuilder(np.array(self.dataset_vectors, dtype=np.float32), num_neighbors=self.reordering_num_neighbors, distance_measure=\"dot_product\").tree(\n            num_leaves=self.num_leaves, num_leaves_to_search=self.num_leaves_to_search, training_sample_size=25000\n        ).score_ah(\n            dimensions_per_block=2\n        ).reorder(self.reordering_num_neighbors).build()\n\n    def add_vector(self, vector_id: str, vector: Union[np.ndarray, List[float]], metadata: Dict = None) -> None:\n        \"\"\"\n        Adds a vector along with its identifier and optional metadata to the store.\n\n        Args:\n            vector_id (str): Unique identifier for the vector.\n            vector (Union[np.ndarray, List[float]]): The high-dimensional vector to be stored.\n            metadata (Dict, optional): Optional metadata related to the vector.\n        \"\"\"\n        if not isinstance(vector, np.ndarray):\n            vector = np.array(vector, dtype=np.float32)\n        \n        if self.searcher is None:\n            self.dataset_vectors.append(vector)\n        else:\n            raise Exception(\"Cannot add vectors after building the index. Rebuild the index to include new vectors.\")\n\n        if metadata is None:\n            metadata = {}\n        self.id_to_metadata[vector_id] = metadata\n\n    def build_index(self):\n        \"\"\"Builds or rebuilds the ScaNN searcher to reflect the current dataset vectors.\"\"\"\n        self._build_scann_searcher()\n\n    def get_vector(self, vector_id: str) -> Union[IVector, None]:\n        \"\"\"\n        Retrieve a vector by its identifier.\n\n        Args:\n            vector_id (str): The unique identifier for the vector.\n\n        Returns:\n            Union[IVector, None]: The vector associated with the given id, or None if not found.\n        \"\"\"\n        if vector_id in self.id_to_metadata:\n            metadata = self.id_to_metadata[vector_id]\n            return SimpleVector(data=metadata.get('vector'), metadata=metadata)\n        return None\n\n    def delete_vector(self, vector_id: str) -> None:\n        \"\"\"\n        Deletes a vector from the ScannVectorStore and marks the index for rebuilding.\n        Note: For simplicity, this function assumes vectors are uniquely identifiable by their metadata.\n\n        Args:\n            vector_id (str): The unique identifier for the vector to be deleted.\n        \"\"\"\n        if vector_id in self.id_to_metadata:\n            # Identify index of the vector to be deleted\n            vector = self.id_to_metadata[vector_id]['vector']\n            index = self.dataset_vectors.index(vector)\n\n            # Remove vector and its metadata\n            del self.dataset_vectors[index]\n            del self.id_to_metadata[vector_id]\n\n            # Since vector order is important for matching ids, rebuild the searcher to reflect deletion\n            self.searcher = None\n        else:\n            # Handle case where vector_id is not found\n            print(f\"Vector ID {vector_id} not found.\")\n\n    def update_vector(self, vector_id: str, new_vector: Union[np.ndarray, List[float]], new_metadata: Dict = None) -> None:\n        \"\"\"\n        Updates an existing vector in the ScannVectorStore and marks the index for rebuilding.\n\n        Args:\n            vector_id (str): The unique identifier for the vector to be updated.\n            new_vector (Union[np.ndarray, List[float]]): The updated vector.\n            new_metadata (Dict, optional): Optional updated metadata for the vector.\n        \"\"\"\n        # Ensure new_vector is numpy array for consistency\n        if not isinstance(new_vector, np.ndarray):\n            new_vector = np.array(new_vector, dtype=np.float32)\n\n        if vector_id in self.id_to_metadata:\n            # Update operation follows delete then add strategy because vector order matters in ScaNN\n            self.delete_vector(vector_id)\n            self.add_vector(vector_id, new_vector, new_metadata)\n        else:\n            # Handle case where vector_id is not found\n            print(f\"Vector ID {vector_id} not found.\")\n\n\n\n    def search_by_similarity_threshold(self, query_vector: Union[np.ndarray, List[float]], similarity_threshold: float, space_name: str = None) -> List[Dict]:\n        \"\"\"\n        Search vectors exceeding a similarity threshold to a query vector within an optional vector space.\n\n        Args:\n            query_vector (Union[np.ndarray, List[float]]): The high-dimensional query vector.\n            similarity_threshold (float): The similarity threshold for filtering results.\n            space_name (str, optional): The name of the vector space to search within. Not used in this implementation.\n\n        Returns:\n            List[Dict]: A list of dictionaries with vector IDs, similarity scores, and optional metadata that meet the similarity threshold.\n        \"\"\"\n        if not isinstance(query_vector, np.ndarray):\n            query_vector = np.array(query_vector, dtype=np.float32)\n        \n        if self.searcher is None:\n            self._build_scann_searcher()\n        \n        _, indices = self.searcher.search(query_vector, final_num_neighbors=self.reordering_num_neighbors)\n        results = [{\"id\": str(idx), \"metadata\": self.id_to_metadata.get(str(idx), {})} for idx in indices if idx < similarity_threshold]\n        return results\n```"
    },
    {
        "document_name": "swarmauri/experimental/tracing/RemoteTrace.py",
        "content": "```swarmauri/experimental/tracing/RemoteTrace.py\nfrom __future__ import ITraceContext\n\nimport requests\nimport json\nimport uuid\nfrom datetime import datetime\n\nfrom swarmauri.core.tracing.ITracer import ITracer\nfrom swarmauri.core.tracing.ITraceContext import ITraceContext\n\n# Implementing the RemoteTraceContext class\nclass RemoteTraceContext(ITraceContext):\n    def __init__(self, trace_id: str, name: str):\n        self.trace_id = trace_id\n        self.name = name\n        self.start_time = datetime.now()\n        self.attributes = {}\n        self.annotations = {}\n\n    def get_trace_id(self) -> str:\n        return self.trace_id\n\n    def add_attribute(self, key: str, value):\n        self.attributes[key] = value\n        \n    def add_annotation(self, key: str, value):\n        self.annotations[key] = value\n\n# Implementing the RemoteAPITracer class\nclass RemoteAPITracer(ITracer):\n    def __init__(self, api_endpoint: str):\n        self.api_endpoint = api_endpoint\n\n    def start_trace(self, name: str, initial_attributes=None) -> 'RemoteTraceContext':\n        trace_id = str(uuid.uuid4())\n        context = RemoteTraceContext(trace_id, name)\n        if initial_attributes:\n            for key, value in initial_attributes.items():\n                context.add_attribute(key, value)\n        return context\n\n    def end_trace(self, trace_context: 'RemoteTraceContext'):\n        trace_context.end_time = datetime.now()\n        # Pretending to serialize the context information to JSON\n        trace_data = {\n            \"trace_id\": trace_context.get_trace_id(),\n            \"name\": trace_context.name,\n            \"start_time\": str(trace_context.start_time),\n            \"end_time\": str(trace_context.end_time),\n            \"attributes\": trace_context.attributes,\n            \"annotations\": trace_context.annotations\n        }\n        json_data = json.dumps(trace_data)\n        # POST the serialized data to the remote REST API\n        response = requests.post(self.api_endpoint, json=json_data)\n        if not response.ok:\n            raise Exception(f\"Failed to send trace data to {self.api_endpoint}. Status code: {response.status_code}\")\n\n    def annotate_trace(self, trace_context: 'RemoteTraceContext', key: str, value):\n        trace_context.add_annotation(key, value)\n```"
    },
    {
        "document_name": "swarmauri/experimental/tracing/__init__.py",
        "content": "```swarmauri/experimental/tracing/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/TypeAgnosticCallableChain.py",
        "content": "```swarmauri/experimental/chains/TypeAgnosticCallableChain.py\nfrom typing import Any, Callable, List, Dict, Optional, Tuple, Union\n\nCallableDefinition = Tuple[Callable, List[Any], Dict[str, Any], Union[str, Callable, None]]\n\nclass TypeAgnosticCallableChain:\n    def __init__(self, callables: Optional[List[CallableDefinition]] = None):\n        self.callables = callables if callables is not None else []\n\n    @staticmethod\n    def _ignore_previous(_previous_result, *args, **kwargs):\n        return args, kwargs\n\n    @staticmethod\n    def _use_first_arg(previous_result, *args, **kwargs):\n        return [previous_result] + list(args), kwargs\n\n    @staticmethod\n    def _use_all_previous_args_first(previous_result, *args, **kwargs):\n        if not isinstance(previous_result, (list, tuple)):\n            previous_result = [previous_result]\n        return list(previous_result) + list(args), kwargs\n\n    @staticmethod\n    def _use_all_previous_args_only(previous_result, *_args, **_kwargs):\n        if not isinstance(previous_result, (list, tuple)):\n            previous_result = [previous_result]\n        return list(previous_result), {}\n\n    @staticmethod\n    def _add_previous_kwargs_overwrite(previous_result, args, kwargs):\n        if not isinstance(previous_result, dict):\n            raise ValueError(\"Previous result is not a dictionary.\")\n        return args, {**kwargs, **previous_result}\n\n    @staticmethod\n    def _add_previous_kwargs_no_overwrite(previous_result, args, kwargs):\n        if not isinstance(previous_result, dict):\n            raise ValueError(\"Previous result is not a dictionary.\")\n        return args, {**previous_result, **kwargs}\n\n    @staticmethod\n    def _use_all_args_all_kwargs_overwrite(previous_result_args, previous_result_kwargs, *args, **kwargs):\n        combined_args = list(previous_result_args) + list(args) if isinstance(previous_result_args, (list, tuple)) else list(args)\n        combined_kwargs = previous_result_kwargs if isinstance(previous_result_kwargs, dict) else {}\n        combined_kwargs.update(kwargs)\n        return combined_args, combined_kwargs\n\n    @staticmethod\n    def _use_all_args_all_kwargs_no_overwrite(previous_result_args, previous_result_kwargs, *args, **kwargs):\n        combined_args = list(previous_result_args) + list(args) if isinstance(previous_result_args, (list, tuple)) else list(args)\n        combined_kwargs = kwargs if isinstance(kwargs, dict) else {}\n        combined_kwargs = {**combined_kwargs, **(previous_result_kwargs if isinstance(previous_result_kwargs, dict) else {})}\n        return combined_args, combined_kwargs\n\n    def add_callable(self, func: Callable, args: List[Any] = None, kwargs: Dict[str, Any] = None, input_handler: Union[str, Callable, None] = None) -> None:\n        if isinstance(input_handler, str):\n            # Map the string to the corresponding static method\n            input_handler_method = getattr(self, f\"_{input_handler}\", None)\n            if input_handler_method is None:\n                raise ValueError(f\"Unknown input handler name: {input_handler}\")\n            input_handler = input_handler_method\n        elif input_handler is None:\n            input_handler = self._ignore_previous\n        self.callables.append((func, args or [], kwargs or {}, input_handler))\n\n    def __call__(self, *initial_args, **initial_kwargs) -> Any:\n        result = None\n        for func, args, kwargs, input_handler in self.callables:\n            if isinstance(input_handler, str):\n                # Map the string to the corresponding static method\n                input_handler_method = getattr(self, f\"_{input_handler}\", None)\n                if input_handler_method is None:\n                    raise ValueError(f\"Unknown input handler name: {input_handler}\")\n                input_handler = input_handler_method\n            elif input_handler is None:\n                input_handler = self._ignore_previous\n                \n            args, kwargs = input_handler(result, *args, **kwargs) if result is not None else (args, kwargs)\n            result = func(*args, **kwargs)\n        return result\n\n    def __or__(self, other: \"TypeAgnosticCallableChain\") -> \"TypeAgnosticCallableChain\":\n        if not isinstance(other, TypeAgnosticCallableChain):\n            raise TypeError(\"Operand must be an instance of TypeAgnosticCallableChain\")\n        \n        new_chain = TypeAgnosticCallableChain(self.callables + other.callables)\n        return new_chain\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/__init__.py",
        "content": "```swarmauri/experimental/chains/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainFormatter.py",
        "content": "```swarmauri/experimental/chains/IChainFormatter.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass IChainFormatter(ABC):\n    @abstractmethod\n    def format_output(self, step: IChainStep, output: Any) -> str:\n        \"\"\"Format the output of a specific chain step.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainNotification.py",
        "content": "```swarmauri/experimental/chains/IChainNotification.py\nfrom abc import ABC, abstractmethod\n\nclass IChainNotifier(ABC):\n    @abstractmethod\n    def send_notification(self, message: str) -> None:\n        \"\"\"Send a notification message based on chain execution results.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainPersistence.py",
        "content": "```swarmauri/experimental/chains/IChainPersistence.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\nfrom swarmauri.core.chains.IChain import IChain\n\nclass IChainPersistence(ABC):\n    @abstractmethod\n    def save_state(self, chain: IChain, state: Dict[str, Any]) -> None:\n        \"\"\"Save the state of the given chain.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_state(self, chain_id: str) -> Dict[str, Any]:\n        \"\"\"Load the state of a chain by its identifier.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainScheduler.py",
        "content": "```swarmauri/experimental/chains/IChainScheduler.py\nfrom abc import ABC, abstractmethod\nfrom swarmauri.core.chains.IChain import IChain\n\nclass IChainScheduler(ABC):\n    @abstractmethod\n    def schedule_chain(self, chain: IChain, schedule: str) -> None:\n        \"\"\"Schedule the execution of the given chain.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/TriplesDocumentStore.py",
        "content": "```swarmauri/experimental/document_stores/TriplesDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom rdflib import Graph, URIRef, Literal, BNode\nfrom ampligraph.latent_features import ComplEx\nfrom ampligraph.evaluation import train_test_split_no_unseen\nfrom ampligraph.latent_features import EmbeddingModel\nfrom ampligraph.utils import save_model, restore_model\n\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nfrom swarmauri.standard.vectorizers.concrete.AmpligraphVectorizer import AmpligraphVectorizer\n\n\nclass TriplesDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self, rdf_file_path: str, model_path: Optional[str] = None):\n        \"\"\"\n        Initializes the TriplesDocumentStore.\n        \"\"\"\n        self.graph = Graph()\n        self.rdf_file_path = rdf_file_path\n        self.graph.parse(rdf_file_path, format='turtle')\n        self.documents = []\n        self.vectorizer = AmpligraphVectorizer()\n        self.model_path = model_path\n        if model_path:\n            self.model = restore_model(model_path)\n        else:\n            self.model = None\n        self.metric = CosineDistance()\n        self._load_documents()\n        if not self.model:\n            self._train_model()\n\n    def _train_model(self):\n        \"\"\"\n        Trains a model based on triples in the graph.\n        \"\"\"\n        # Extract triples for embedding model\n        triples = np.array([[str(s), str(p), str(o)] for s, p, o in self.graph])\n        # Split data\n        train, test = train_test_split_no_unseen(triples, test_size=0.1)\n        self.model = ComplEx(batches_count=100, seed=0, epochs=20, k=150, eta=1,\n                             optimizer='adam', optimizer_params={'lr': 1e-3},\n                             loss='pairwise', regularizer='LP', regularizer_params={'p': 3, 'lambda': 1e-5},\n                             verbose=True)\n        self.model.fit(train)\n        if self.model_path:\n            save_model(self.model, self.model_path)\n\n    def _load_documents(self):\n        \"\"\"\n        Load documents into the store from the RDF graph.\n        \"\"\"\n        for subj, pred, obj in self.graph:\n            doc_id = str(hash((subj, pred, obj)))\n            content = f\"{subj} {pred} {obj}\"\n            document = Document(content=content, doc_id=doc_id, metadata={})\n            self.documents.append(document)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Adds a single RDF triple document.\n        \"\"\"\n        subj, pred, obj = document.content.split()  # Splitting content into RDF components\n        self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.append(document)\n        self._train_model()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Adds multiple RDF triple documents.\n        \"\"\"\n        for document in documents:\n            subj, pred, obj = document.content.split()  # Assuming each document's content is \"subj pred obj\"\n            self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.extend(documents)\n        self._train_model()\n\n    # Implementation for get_document, get_all_documents, delete_document, update_document remains same as before\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string.\n        \"\"\"\n        if not self.model:\n            self._train_model()\n        query_vector = self.vectorizer.infer_vector(model=self.model, samples=[query])[0]\n        document_vectors = [self.vectorizer.infer_vector(model=self.model, samples=[doc.content])[0] for doc in self.documents]\n        similarities = self.metric.distances(SimpleVector(data=query_vector), [SimpleVector(vector) for vector in document_vectors])\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/Word2VecDocumentStore.py",
        "content": "```swarmauri/experimental/document_stores/Word2VecDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nimport gensim.downloader as api\n\nclass Word2VecDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self):\n        \"\"\"\n        Initializes the Word2VecDocumentStore.\n\n        Parameters:\n        - word2vec_model_path (Optional[str]): File path to a pre-trained Word2Vec model. \n                                               Leave None to use Gensim's pre-trained model.\n        - pre_trained (bool): If True, loads a pre-trained Word2Vec model. If False, an uninitialized model is used that requires further training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)  # Example parameters; adjust as needed\n        self.documents = []\n        self.metric = CosineDistance()\n\n    def add_document(self, document: EmbeddedDocument) -> None:\n        # Check if the document already has an embedding, if not generate one using _average_word_vectors\n        if not hasattr(document, 'embedding') or document.embedding is None:\n            words = document.content.split()  # Simple tokenization, consider using a better tokenizer\n            embedding = self._average_word_vectors(words)\n            document.embedding = embedding\n            print(document.embedding)\n        self.documents.append(document)\n        \n    def add_documents(self, documents: List[EmbeddedDocument]) -> None:\n        self.documents.extend(documents)\n        \n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n        \n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str, updated_document: EmbeddedDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n    def _average_word_vectors(self, words: List[str]) -> np.ndarray:\n        \"\"\"\n        Generate document vector by averaging its word vectors.\n        \"\"\"\n        word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n        print(word_vectors)\n        if word_vectors:\n            return np.mean(word_vectors, axis=0)\n        else:\n            return np.zeros(self.model.vector_size)\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[EmbeddedDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string based on Word2Vec embeddings.\n        \"\"\"\n        query_vector = self._average_word_vectors(query.split())\n        print('query_vector', query_vector)\n        # Compute similarity scores between the query and each document's stored embedding\n        similarities = self.metric.similarities(SimpleVector(query_vector), [SimpleVector(doc.embedding) for doc in self.documents if doc.embedding])\n        print('similarities', similarities)\n        # Retrieve indices of top_k most similar documents\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n        print('top_k_indices', top_k_indices)\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/__init__.py",
        "content": "```swarmauri/experimental/document_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SSASimilarity.py",
        "content": "```swarmauri/experimental/distances/SSASimilarity.py\nfrom typing import Set, List, Dict\nfrom ....core.vector_stores.ISimilarity import ISimilarity\nfrom ....core.vectors.IVector import IVector\n\n\nclass SSASimilarity(ISimilarity):\n    \"\"\"\n    Implements the State Similarity in Arity (SSA) similarity measure to\n    compare states (sets of variables) for their similarity.\n    \"\"\"\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Calculate the SSA similarity between two documents by comparing their metadata,\n        assumed to represent states as sets of variables.\n\n        Args:\n        - vector_a (IDocument): The first document.\n        - vector_b (IDocument): The second document to compare with the first document.\n\n        Returns:\n        - float: The SSA similarity measure between vector_a and vector_b, ranging from 0 to 1\n                 where 0 represents no similarity and 1 represents identical states.\n        \"\"\"\n        state_a = set(vector_a.metadata.keys())\n        state_b = set(vector_b.metadata.keys())\n\n        return self.calculate_ssa(state_a, state_b)\n\n    @staticmethod\n    def calculate_ssa(state_a: Set[str], state_b: Set[str]) -> float:\n        \"\"\"\n        Calculate the State Similarity in Arity (SSA) between two states.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n\n        Returns:6\n        - float: The SSA similarity measure, ranging from 0 (no similarity) to 1 (identical states).\n        \"\"\"\n        # Calculate the intersection (shared variables) between the two states\n        shared_variables = state_a.intersection(state_b)\n        \n        # Calculate the union (total unique variables) of the two states\n        total_variables = state_a.union(state_b)\n        \n        # Calculate the SSA measure as the ratio of shared to total variables\n        ssa = len(shared_variables) / len(total_variables) if total_variables else 1\n        \n        return ssa\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SSIVSimilarity.py",
        "content": "```swarmauri/experimental/distances/SSIVSimilarity.py\nfrom typing import List, Dict, Set\nfrom ....core.vector_stores.ISimilarity import ISimilarity\n\nclass SSIVSimilarity(ISimilarity):\n    \"\"\"\n    Concrete class that implements ISimilarity interface using\n    State Similarity of Important Variables (SSIV) as the similarity measure.\n    \"\"\"\n\n    def similarity(self, state_a: Set[str], state_b: Set[str], importance_a: Dict[str, float], importance_b: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate the SSIV between two states represented by sets of variables.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n        - importance_a (Dict[str, float]): A dictionary where keys are variables in state A and values are their importance weights.\n        - importance_b (Dict[str, float]): A dictionary where keys are variables in state B and values are their importance weights.\n\n        Returns:\n        - float: The SSIV similarity measure, ranging from 0 to 1.\n        \"\"\"\n        return self.calculate_ssiv(state_a, state_b, importance_a, importance_b)\n\n    @staticmethod\n    def calculate_ssiv(state_a: Set[str], state_b: Set[str], importance_a: Dict[str, float], importance_b: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate the State Similarity of Important Variables (SSIV) between two states.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n        - importance_a (Dict[str, float]): A dictionary where keys are variables in state A and values are their importance weights.\n        - importance_b (Dict[str, float]): A dictionary where keys are variables in state B and values are their importance weights.\n\n        Returns:\n        - float: The SSIV similarity measure, ranging from 0 to 1.\n        \n        Note: It is assumed that the importance weights are non-negative.\n        \"\"\"\n        shared_variables = state_a.intersection(state_b)\n        \n        # Calculate the summed importance of shared variables\n        shared_importance_sum = sum(importance_a[var] for var in shared_variables) + sum(importance_b[var] for var in shared_variables)\n        \n        # Calculate the total importance of all variables in both states\n        total_importance_sum = sum(importance_a.values()) + sum(importance_b.values())\n        \n        # Calculate and return the SSIV\n        ssiv = (2 * shared_importance_sum) / total_importance_sum if total_importance_sum != 0 else 0\n        return ssiv\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/__init__.py",
        "content": "```swarmauri/experimental/distances/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/apis/CeleryAgentCommands.py",
        "content": "```swarmauri/experimental/apis/CeleryAgentCommands.py\nfrom celery import Celery\nfrom swarmauri.core.agent_apis.IAgentCommands import IAgentCommands\nfrom typing import Callable, Any, Dict\n\nclass CeleryAgentCommands(IAgentCommands):\n    def __init__(self, broker_url: str, backend_url: str):\n        \"\"\"\n        Initializes the Celery application with the specified broker and backend URLs.\n        \"\"\"\n        self.app = Celery('swarmauri_agent_tasks', broker=broker_url, backend=backend_url)\n\n    def register_command(self, command_name: str, function: Callable[..., Any], *args, **kwargs) -> None:\n        \"\"\"\n        Registers a new command as a Celery task.\n        \"\"\"\n        self.app.task(name=command_name, bind=True)(function)\n\n    def execute_command(self, command_name: str, *args, **kwargs) -> Any:\n        \"\"\"\n        Executes a registered command by name asynchronously.\n        \"\"\"\n        result = self.app.send_task(command_name, args=args, kwargs=kwargs)\n        return result.get()\n\n    def get_status(self, task_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Fetches the status of a command execution via its task ID.\n        \"\"\"\n        async_result = self.app.AsyncResult(task_id)\n        return {\"status\": async_result.status, \"result\": async_result.result if async_result.ready() else None}\n\n    def revoke_command(self, task_id: str) -> None:\n        \"\"\"\n        Revokes or terminates a command execution by its task ID.\n        \"\"\"\n        self.app.control.revoke(task_id, terminate=True)\n```"
    },
    {
        "document_name": "swarmauri/experimental/embeddings/SpatialDocEmbedding.py",
        "content": "```swarmauri/experimental/embeddings/SpatialDocEmbedding.py\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom torch import nn\nimport numpy as np\nfrom typing import Literal\nfrom pydantic import PrivateAttr\n\nfrom swarmauri.standard.embeddings.base.EmbeddingBase import EmbeddingBase\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\nclass SpatialDocEmbedding(EmbeddingBase):\n    _special_tokens_dict = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _model = PrivateAttr()\n    _device = PrivateAttr()\n    type: Literal['SpatialDocEmbedding'] = 'SpatialDocEmbedding'\n    \n    def __init__(self, special_tokens_dict=None, **kwargs):\n        super().__init__(**kwargs)\n        self._special_tokens_dict = special_tokens_dict or {\n            'additional_special_tokens': [\n                '[DIR]', '[TYPE]', '[SECTION]', '[PATH]',\n                '[PARAGRAPH]', '[SUBPARAGRAPH]', '[CHAPTER]', '[TITLE]', '[SUBSECTION]'\n            ]\n        }\n        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self._tokenizer.add_special_tokens(self._special_tokens_dict)\n        self._model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n        self._model.resize_token_embeddings(len(self._tokenizer))\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._model.to(self._device)\n\n    def add_metadata(self, text, metadata_dict):\n        metadata_components = []\n        for key, value in metadata_dict.items():\n            if f\"[{key.upper()}]\" in self._special_tokens_dict['additional_special_tokens']:\n                token = f\"[{key.upper()}={value}]\"\n                metadata_components.append(token)\n        metadata_str = ' '.join(metadata_components)\n        return metadata_str + ' ' + text if metadata_components else text\n\n    def tokenize_and_encode(self, text):\n        inputs = self._tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        # Move the input tensors to the same device as the model\n        inputs = {key: value.to(self._device) for key, value in inputs.items()}\n        outputs = self._model(**inputs)\n        return outputs.pooler_output\n\n    def enhance_embedding_with_positional_info(self, embeddings, doc_position, total_docs):\n        position_effect = torch.sin(torch.tensor(doc_position / total_docs, dtype=torch.float))\n        enhanced_embeddings = embeddings + position_effect\n        return enhanced_embeddings\n\n    def vectorize_document(self, chunks, metadata_list=None):\n        all_embeddings = []\n        total_chunks = len(chunks)\n        if not metadata_list:\n            # Default empty metadata if none provided\n            metadata_list = [{} for _ in chunks]\n        \n        for i, (chunk, metadata) in enumerate(zip(chunks, metadata_list)):\n            # Use add_metadata to include any available metadata dynamically\n            embedded_text = self.add_metadata(chunk, metadata)\n            embeddings = self.tokenize_and_encode(embedded_text)\n            enhanced_embeddings = self.enhance_embedding_with_positional_info(embeddings, i, total_chunks)\n            all_embeddings.append(enhanced_embeddings)\n\n        return all_embeddings\n\n\n\n    def fit(self, data):\n        # Although this vectorizer might not need to be fitted in the traditional sense,\n        # this method placeholder allows integration into pipelines that expect a fit method.\n        pass\n\n    def transform(self, data):\n        print(data)\n        if isinstance(data, list):\n            return [self.infer_vector(text).value for text in data]\n        else:\n            return self.infer_vector(data).value\n\n    def fit_transform(self, data):\n        #self.fit(data)\n        return self.transform(data)\n\n    def infer_vector(self, data, *args, **kwargs):\n        print(data)\n        inputs = self.tokenize_and_encode(data)\n        print(inputs)\n        inputs = inputs.cpu().detach().numpy().tolist()\n        print(inputs)\n        return Vector(value=[1,2,3]) # Placeholder\n\n    def save_model(self, path):\n        torch.save({\n            'model_state_dict': self._model.state_dict(),\n            'tokenizer': self._tokenizer\n        }, path)\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self._model.load_state_dict(checkpoint['model_state_dict'])\n        self._tokenizer = checkpoint['tokenizer']\n\n    def extract_features(self, text):\n        inputs = self.tokenize_and_encode(text)\n        return Vector(value=inputs.cpu().detach().numpy().tolist())\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/embeddings/__init__.py",
        "content": "```swarmauri/experimental/embeddings/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vectors/__init__.py",
        "content": "```swarmauri/experimental/vectors/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/llms/ShuttleAIModel.py",
        "content": "```swarmauri/experimental/llms/ShuttleAIModel.py\nimport logging\nimport json\nfrom typing import List, Dict, Literal\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase \n\nimport requests \n\nclass ShuttleAIModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n    \"shuttle-2-turbo\", \"shuttle-turbo\", \"gpt-4o-2024-05-13\", \"gpt-4-turbo-2024-04-09\",\n    \"gpt-4-0125-preview\", \"gpt-4-1106-preview\", \"gpt-4-1106-vision-preview\", \"gpt-4-0613\",\n    \"gpt-4-bing\", \"gpt-4-turbo-bing\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-0125\",\n    \"gpt-3.5-turbo-1106\", \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\",\n    \"claude-2.1\", \"claude-2.0\", \"claude-instant-1.2\", \"claude-instant-1.1\",\n    \"claude-instant-1.0\", \"meta-llama-3-70b-instruct\", \"meta-llama-3-8b-instruct\", \"llama-3-sonar-large-32k-online\",\n    \"llama-3-sonar-small-32k-online\", \"llama-3-sonar-large-32k-chat\", \"llama-3-sonar-small-32k-chat\", \"blackbox\",\n    \"blackbox-code\", \"wizardlm-2-8x22b\", \"wizardlm-2-70b\", \"dolphin-2.6-mixtral-8x7b\",\n    \"codestral-latest\", \"mistral-large\", \"mistral-next\", \"mistral-medium\",\n    \"mistral-small\", \"mistral-tiny\", \"mixtral-8x7b-instruct-v0.1\", \"mixtral-8x22b-instruct-v0.1\",\n    \"mistral-7b-instruct-v0.2\", \"mistral-7b-instruct-v0.1\", \"nous-hermes-2-mixtral-8x7b\", \"gemini-1.5-pro-latest\",\n    \"gemini-1.0-pro-latest\", \"gemini-1.0-pro-vision\", \"lzlv-70b\", \"figgs-rp\", \"cinematika-7b\"\n    ]\n    name: str = \"shuttle-2-turbo\"\n    type: Literal['ShuttleAIModel'] = 'ShuttleAIModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n       # Get only the properties that we require\n        message_properties = [\"content\", \"role\"]\n\n        # Exclude FunctionMessages\n        formatted_messages = [message.model_dump(include=message_properties) for message in messages]\n        return formatted_messages\n\n    \n    def predict(self, \n            conversation, \n            temperature=0.7, \n            max_tokens=256, \n            top_p=1, \n            internet=False, \n            citations=False, \n            tone='precise', \n            raw=False, \n            image=None): \n\n            formatted_messages = self._format_messages(conversation.history) \n\n            url = \"https://api.shuttleai.app/v1/chat/completions\" \n            payload = { \n                \"model\": self.name, \n                \"messages\": formatted_messages, \n                \"max_tokens\": max_tokens, \n                \"temperature\": temperature, \n                \"top_p\": top_p\n            } \n\n            if raw:\n                payload['raw'] = True\n\n            if internet:\n                payload['internet'] = True\n\n            # Only include the 'image' field if it's not None\n            if image is not None:\n                payload[\"image\"] = image\n\n            if self.name in ['gpt-4-bing', 'gpt-4-turbo-bing']: \n                payload['tone'] = tone\n                \n                # Include citations only if citations is True\n                if citations:\n                    payload['citations'] = True\n\n            headers = { \n                \"Authorization\": f\"Bearer {self.api_key}\", \n                \"Content-Type\": \"application/json\", \n            }\n\n            # Log payload for debugging\n            logging.info(f\"Payload being sent: {payload}\")\n\n            # Send the request\n            response = requests.post(url, json=payload, headers=headers)\n\n            # Log response for debugging\n            logging.info(f\"Response received: {response.text}\")\n\n            # Parse response JSON safely\n            try:\n                message_content = response.json()['choices'][0]['message']['content']\n            except KeyError as e:\n                logging.info(f\"Error parsing response: {response.text}\")\n                raise e\n\n            conversation.add_message(AgentMessage(content=message_content))  \n            return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/llms/ShuttleAIToolModel.py",
        "content": "```swarmauri/experimental/llms/ShuttleAIToolModel.py\nimport json\nimport logging\nfrom typing import List, Literal, Dict, Any\nimport requests\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.ShuttleAISchemaConverter import (\n    ShuttleAISchemaConverter,\n)\n\n\nclass ShuttleAIToolModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n        \"shuttle-2-turbo\",\n        \"gpt-4-turbo-2024-04-09\",\n        \"gpt-4-0125-preview\",\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0613\",\n        \"gpt-3.5-turbo-0125\",\n        \"gpt-3.5-turbo-1106\",\n        \"claude-instant-1.1\",\n        \"wizardlm-2-8x22b\",\n        \"mistral-7b-instruct-v0.2\",\n        \"gemini-1.5-pro-latest\",\n        \"gemini-1.0-pro-latest\",\n    ]\n    name: str = \"shuttle-2-turbo\"\n    type: Literal[\"ShuttleAIToolModel\"] = \"ShuttleAIToolModel\"\n\n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [ShuttleAISchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(\n        self, messages: List[SubclassUnion[MessageBase]]\n    ) -> List[Dict[str, str]]:\n        message_properties = [\"content\", \"role\", \"name\", \"tool_call_id\", \"tool_calls\"]\n        formatted_messages = [\n            message.model_dump(include=message_properties, exclude_none=True)\n            for message in messages\n        ]\n        return formatted_messages\n\n    def predict(\n        self,\n        conversation,\n        toolkit=None,\n        tool_choice=\"auto\",\n        temperature=0.7,\n        max_tokens=1024,\n        top_p=1.0,\n        internet=False,\n        raw=False,\n        image=None,\n        citations=False,\n        tone=\"precise\",\n    ):\n        formatted_messages = self._format_messages(conversation.history)\n\n        if toolkit and not tool_choice:\n            tool_choice = \"auto\"\n\n        url = \"https://api.shuttleai.app/v1/chat/completions\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        payload = {\n            \"model\": self.name,\n            \"messages\": formatted_messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"tool_choice\": tool_choice,\n            \"tools\": self._schema_convert_tools(toolkit.tools),\n        }\n\n        if raw:\n            payload[\"raw\"] = True\n\n        if internet:\n            payload[\"internet\"] = True\n\n        if image is not None:\n            payload[\"image\"] = image\n\n        if self.name in [\"gpt-4-bing\", \"gpt-4-turbo-bing\"]:\n            payload[\"tone\"] = tone\n            # Include citations only if citations is True\n            if citations:\n                payload['citations'] = True\n\n\n        logging.info(f\"payload: {payload}\")\n        \n        # First we ask agent to give us a response\n        agent_response = requests.request(\"POST\", url, json=payload, headers=headers)\n\n        logging.info(f\"agent response {agent_response.json()}\")\n\n        try:\n            messages = [\n                formatted_messages[-1],\n                agent_response.json()[\"choices\"][0][\"message\"][\"content\"],\n            ]\n        except Exception as error:\n            logging.warn(error)\n\n        tool_calls = agent_response.json()[\"choices\"][0][\"message\"].get(\n            \"tool_calls\", None\n        )\n\n\n        # If agent responds with tool call, then we execute the functions\n        if tool_calls:\n            for tool_call in tool_calls:\n                func_name = tool_call[\"function\"][\"name\"]\n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call[\"function\"][\"arguments\"])\n                func_result = func_call(**func_args)\n                func_message = FunctionMessage(content=func_result, \n                                               name=func_name, \n                                               tool_call_id=tool_call['id'])\n                conversation.add_message(func_message)\n\n\n\n        logging.info(f\"conversation: {conversation.history}\")\n\n\n        # After executing the functions, we present the results to the Agent\n        payload['messages'] = self._format_messages(conversation.history)\n\n        logging.info(f\"payload: {payload}\")\n\n        agent_response = requests.request(\"POST\", url, json=payload, headers=headers)\n        logging.info(f\"agent response {agent_response.json()}\")\n        \n        agent_message = AgentMessage(\n            content=agent_response.json()[\"choices\"][0][\"message\"][\"content\"]\n        )\n\n        conversation.add_message(agent_message)\n        logging.info(f\"conversation: {conversation.history}\")\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/llms/__init__.py",
        "content": "```swarmauri/experimental/llms/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/__init__.py",
        "content": "```swarmauri/standard/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/__init__.py",
        "content": "```swarmauri/standard/agents/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentBase.py",
        "content": "```swarmauri/standard/agents/base/AgentBase.py\nfrom typing import Any, Optional, Dict, Union, Literal\nfrom pydantic import ConfigDict, Field, field_validator\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.core.agents.IAgent import IAgent\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass AgentBase(IAgent, ComponentBase):\n    llm: SubclassUnion[LLMBase]\n    resource: ResourceTypes =  Field(default=ResourceTypes.AGENT.value)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['AgentBase'] = 'AgentBase'\n\n    def exec(self, input_str: Optional[Union[str, IMessage]] = \"\", llm_kwargs: Optional[Dict] = {}) -> Any:\n        raise NotImplementedError('The `exec` function has not been implemeneted on this class.')\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentConversationMixin.py",
        "content": "```swarmauri/standard/agents/base/AgentConversationMixin.py\nfrom pydantic import BaseModel, ConfigDict\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.core.agents.IAgentConversation import IAgentConversation\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\n\nclass AgentConversationMixin(IAgentConversation, BaseModel):\n    conversation: SubclassUnion[ConversationBase] # \u00f0\u0178\u0161\u00a7  Placeholder\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentRetrieveMixin.py",
        "content": "```swarmauri/standard/agents/base/AgentRetrieveMixin.py\nfrom abc import ABC\nfrom typing import List\nfrom pydantic import BaseModel, ConfigDict, field_validator, Field\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.core.agents.IAgentRetrieve import IAgentRetrieve\n\nclass AgentRetrieveMixin(IAgentRetrieve, BaseModel):\n    last_retrieved: List[Document] = Field(default_factory=list)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentSystemContextMixin.py",
        "content": "```swarmauri/standard/agents/base/AgentSystemContextMixin.py\nfrom typing import Union\nfrom pydantic import BaseModel, field_validator\n\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\nfrom swarmauri.core.agents.IAgentSystemContext import IAgentSystemContext\n\n\nclass AgentSystemContextMixin(IAgentSystemContext, BaseModel):\n    system_context:  Union[SystemMessage, str]\n\n    @field_validator('system_context', mode='before')\n    def set_system_context(cls, value: Union[str, SystemMessage]) -> SystemMessage:\n        if isinstance(value, str):\n            return SystemMessage(content=value)\n        return value\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentToolMixin.py",
        "content": "```swarmauri/standard/agents/base/AgentToolMixin.py\nfrom pydantic import BaseModel, ConfigDict\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.toolkits.base.ToolkitBase import ToolkitBase\nfrom swarmauri.core.agents.IAgentToolkit import IAgentToolkit\n\nclass AgentToolMixin(IAgentToolkit, BaseModel):\n    toolkit: SubclassUnion[ToolkitBase]\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    \n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentVectorStoreMixin.py",
        "content": "```swarmauri/standard/agents/base/AgentVectorStoreMixin.py\nfrom pydantic import BaseModel, ConfigDict\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.core.agents.IAgentVectorStore import IAgentVectorStore\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\n\nclass AgentVectorStoreMixin(IAgentVectorStore, BaseModel):\n    vector_store: SubclassUnion[VectorStoreBase]\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/__init__.py",
        "content": "```swarmauri/standard/agents/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/RagAgent.py",
        "content": "```swarmauri/standard/agents/concrete/RagAgent.py\nfrom typing import Any, Optional, Union, Dict, Literal\nfrom swarmauri.core.messages import IMessage\n\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\nfrom swarmauri.standard.agents.base.AgentRetrieveMixin import AgentRetrieveMixin\nfrom swarmauri.standard.agents.base.AgentConversationMixin import AgentConversationMixin\nfrom swarmauri.standard.agents.base.AgentVectorStoreMixin import AgentVectorStoreMixin\nfrom swarmauri.standard.agents.base.AgentSystemContextMixin import AgentSystemContextMixin\n\nfrom swarmauri.standard.messages.concrete import (HumanMessage, \n                                                  SystemMessage,\n                                                  AgentMessage)\n\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\n\nclass RagAgent(AgentRetrieveMixin, \n               AgentVectorStoreMixin, \n               AgentSystemContextMixin, \n               AgentConversationMixin, \n               AgentBase):\n    \"\"\"\n    RagAgent (Retriever-And-Generator Agent) extends DocumentAgentBase,\n    specialized in retrieving documents based on input queries and generating responses.\n    \"\"\"\n    llm: SubclassUnion[LLMBase]\n    conversation: SubclassUnion[ConversationBase]\n    vector_store: SubclassUnion[VectorStoreBase]\n    system_context:  Union[SystemMessage, str]\n    type: Literal['RagAgent'] = 'RagAgent'\n    \n    def _create_preamble_context(self):\n        substr = self.system_context.content\n        substr += '\\n\\n'\n        substr += '\\n'.join([doc.content for doc in self.last_retrieved])\n        return substr\n\n    def _create_post_context(self):\n        substr = '\\n'.join([doc.content for doc in self.last_retrieved])\n        substr += '\\n\\n'\n        substr += self.system_context.content\n        return substr\n\n    def exec(self, \n             input_data: Optional[Union[str, IMessage]] = \"\", \n             top_k: int = 5, \n             preamble: bool = True,\n             fixed: bool = False,\n             llm_kwargs: Optional[Dict] = {}\n             ) -> Any:\n        try:\n            # Check if the input is a string, then wrap it in a HumanMessage\n            if isinstance(input_data, str):\n                human_message = HumanMessage(content=input_data)\n            elif isinstance(input_data, IMessage):\n                human_message = input_data\n            else:\n                raise TypeError(\"Input data must be a string or an instance of Message.\")\n            \n            # Add the human message to the conversation\n            self.conversation.add_message(human_message)\n\n            # Retrieval and set new substr for system context\n            if top_k > 0 and len(self.vector_store.documents) > 0:\n                self.last_retrieved = self.vector_store.retrieve(query=input_data, top_k=top_k)\n\n                if preamble:\n                    substr = self._create_preamble_context()\n                else:\n                    substr = self._create_post_context()\n\n            else:\n                if fixed:\n                    if preamble:\n                        substr = self._create_preamble_context()\n                    else:\n                        substr = self._create_post_context()\n                else:\n                    substr = self.system_context.content\n                    self.last_retrieved = []\n                \n            # Use substr to set system context\n            system_context = SystemMessage(content=substr)\n            self.conversation.system_context = system_context\n            \n\n            # Retrieve the conversation history and predict a response\n            if llm_kwargs:\n                self.llm.predict(conversation=self.conversation, **llm_kwargs)\n            else:\n                self.llm.predict(conversation=self.conversation)\n                \n            return self.conversation.get_last().content\n\n        except Exception as e:\n            print(f\"RagAgent error: {e}\")\n            raise e\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/SimpleConversationAgent.py",
        "content": "```swarmauri/standard/agents/concrete/SimpleConversationAgent.py\nfrom typing import Any, Optional, Dict, Literal\n\nfrom swarmauri.core.conversations.IConversation import IConversation\n\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\nfrom swarmauri.standard.agents.base.AgentConversationMixin import AgentConversationMixin\nfrom swarmauri.standard.messages.concrete import HumanMessage, AgentMessage, FunctionMessage\n\nfrom swarmauri.core.typing import SubclassUnion # \u00f0\u0178\u0161\u00a7  Placeholder\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase # \u00f0\u0178\u0161\u00a7  Placeholder\n\nclass SimpleConversationAgent(AgentConversationMixin, AgentBase):\n    conversation: SubclassUnion[ConversationBase] # \u00f0\u0178\u0161\u00a7  Placeholder\n    type: Literal['SimpleConversationAgent'] = 'SimpleConversationAgent'\n    \n    def exec(self, \n        input_str: Optional[str] = \"\",\n        llm_kwargs: Optional[Dict] = {} \n        ) -> Any:\n        \n        if input_str:\n            human_message = HumanMessage(content=input_str)\n            self.conversation.add_message(human_message)\n        \n        self.llm.predict(conversation=self.conversation, **llm_kwargs)\n        return self.conversation.get_last().content\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/ToolAgent.py",
        "content": "```swarmauri/standard/agents/concrete/ToolAgent.py\nfrom pydantic import ConfigDict\nfrom typing import Any, Optional, Union, Dict, Literal\nimport json\nimport logging\nfrom swarmauri.core.messages import IMessage\n\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\nfrom swarmauri.standard.agents.base.AgentConversationMixin import AgentConversationMixin\nfrom swarmauri.standard.agents.base.AgentToolMixin import AgentToolMixin\nfrom swarmauri.standard.messages.concrete import HumanMessage, AgentMessage, FunctionMessage\n\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.toolkits.concrete.Toolkit import Toolkit\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\n\nclass ToolAgent(AgentToolMixin, AgentConversationMixin, AgentBase):\n    llm: SubclassUnion[LLMBase]\n    toolkit: SubclassUnion[Toolkit]\n    conversation: SubclassUnion[ConversationBase] # \u00f0\u0178\u0161\u00a7  Placeholder\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['ToolAgent'] = 'ToolAgent'\n    \n    def exec(self, \n        input_data: Optional[Union[str, IMessage]] = \"\",  \n        llm_kwargs: Optional[Dict] = {}) -> Any:\n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(content=input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n\n        # Add the human message to the conversation\n        self.conversation.add_message(human_message)\n\n        #predict a response        \n        self.conversation = self.llm.predict(\n            conversation=self.conversation, \n            toolkit=self.toolkit, \n            **llm_kwargs)\n\n        logging.info(self.conversation.get_last().content)\n\n        return self.conversation.get_last().content\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/__init__.py",
        "content": "```swarmauri/standard/agents/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/QAAgent.py",
        "content": "```swarmauri/standard/agents/concrete/QAAgent.py\nfrom typing import Any, Optional, Dict, Literal\n\nfrom swarmauri.standard.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\nfrom swarmauri.standard.messages.concrete.HumanMessage import HumanMessage\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\n\nclass QAAgent(AgentBase):\n    conversation: MaxSystemContextConversation = MaxSystemContextConversation(max_size=2)\n    type: Literal['QAAgent'] = 'QAAgent'\n    \n    def exec(self, \n        input_str: Optional[str] = \"\",\n        llm_kwargs: Optional[Dict] = {} \n        ) -> Any:\n        \n        self.conversation.add_message(HumanMessage(content=input_str))\n        self.llm.predict(conversation=self.conversation, **llm_kwargs)\n        \n        return self.conversation.get_last().content\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/__init__.py",
        "content": "```swarmauri/standard/utils/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/apply_metaclass.py",
        "content": "```swarmauri/standard/utils/apply_metaclass.py\ndef apply_metaclass_to_cls(cls, metaclass):\n    # Create a new class using the metaclass, with the same name, bases, and attributes as the original class\n    new_class = metaclass(cls.__name__, cls.__bases__, dict(cls.__dict__))\n    return new_class\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/decorate.py",
        "content": "```swarmauri/standard/utils/decorate.py\ndef decorate_cls(cls, decorator_fn):\n    import types\n    for attr_name in dir(cls):\n        attr = getattr(cls, attr_name)\n        if isinstance(attr, types.FunctionType):\n            setattr(cls, attr_name, decorator_fn(attr))\n    return cls\n\ndef decorate_instance(instance, decorator_fn):\n    import types\n    for attr_name in dir(instance):\n        attr = getattr(instance, attr_name)\n        if isinstance(attr, types.MethodType):\n            setattr(instance, attr_name, decorator_fn(attr.__func__).__get__(instance))\n\ndef decorate_instance_method(instance, method_name, decorator_fn):\n    # Get the method from the instance\n    original_method = getattr(instance, method_name)\n    \n    # Decorate the method\n    decorated_method = decorator_fn(original_method)\n    \n    # Rebind the decorated method to the instance\n    setattr(instance, method_name, decorated_method.__get__(instance, instance.__class__))\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/get_class_hash.py",
        "content": "```swarmauri/standard/utils/get_class_hash.py\nimport hashlib\nimport inspect\n\ndef get_class_hash(cls):\n    \"\"\"\n    Generates a unique hash value for a given class.\n\n    This function uses the built-in `hashlib` and `inspect` modules to create a hash value based on the class' methods\n    and properties. The members of the class are first sorted to ensure a consistent order, and then the hash object is\n    updated with each member's name and signature.\n\n    Parameters:\n    - cls (type): The class object to calculate the hash for.\n\n    Returns:\n    - str: The generated hexadecimal hash value.\n    \"\"\"\n    hash_obj = hashlib.sha256()\n\n    # Get the list of methods and properties of the class\n    members = inspect.getmembers(cls, predicate=inspect.isfunction)\n    members += inspect.getmembers(cls, predicate=inspect.isdatadescriptor)\n\n    # Sort members to ensure consistent order\n    members.sort()\n\n    # Update the hash with each member's name and signature\n    for name, member in members:\n        hash_obj.update(name.encode('utf-8'))\n        if inspect.isfunction(member):\n            sig = inspect.signature(member)\n            hash_obj.update(str(sig).encode('utf-8'))\n\n    # Return the hexadecimal digest of the hash\n    return hash_obj.hexdigest()\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/json_validator.py",
        "content": "```swarmauri/standard/utils/json_validator.py\n# swarmauri/standard/utils/json_validator.py\nimport json\nimport jsonschema\nfrom jsonschema import validate\n\ndef load_json_file(file_path: str) -> dict:\n    with open(file_path, 'r') as file:\n        return json.load(file)\n\ndef validate_json(data: dict, schema_file: str) -> bool:\n    schema = load_json_file(schema_file)\n    try:\n        validate(instance=data, schema=schema)\n    except jsonschema.exceptions.ValidationError as err:\n        print(f\"JSON validation error: {err.message}\")\n        return False\n    return True\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/load_documents_from_json.py",
        "content": "```swarmauri/standard/utils/load_documents_from_json.py\nimport json\nfrom typing import List\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\n\ndef load_documents_from_json_file(json_file_path):\n    documents = []\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n\n    documents = [\n        EmbeddedDocument(id=str(_), \n        content=doc['content'], \n        metadata={\"document_name\": doc['document_name']}) \n        for _, doc in enumerate(data) if doc['content']\n        ]\n\n    return documents\n\ndef load_documents_from_json(json):\n    documents = []\n    data = json.loads(json)\n    documents = [\n        EmbeddedDocument(id=str(_), \n        content=doc['content'], \n        metadata={\"document_name\": doc['document_name']}) \n        for _, doc in enumerate(data) if doc['content']\n        ]\n    return documents\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/memoize.py",
        "content": "```swarmauri/standard/utils/memoize.py\ndef memoize(func):\n    cache = {}\n    def memoized_func(*args):\n        if args in cache:\n            return cache[args]\n        result = func(*args)\n        cache[args] = result\n        return result\n    return memoized_func\n    \nclass MemoizingMeta(type):\n    def __new__(cls, name, bases, dct):\n        for key, value in dct.items():\n            if callable(value) and not key.startswith('__'):\n                dct[key] = memoize(value)\n        return super().__new__(cls, name, bases, dct)\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/sql_log.py",
        "content": "```swarmauri/standard/utils/sql_log.py\nimport sqlite3\nfrom datetime import datetime\nimport asyncio\n\n\ndef sql_log(self, db_path: str, conversation_id, model_name, prompt, response, start_datetime, end_datetime):\n    try:\n        duration = (end_datetime - start_datetime).total_seconds()\n        start_datetime = start_datetime.isoformat()\n        end_datetime = end_datetime.isoformat()\n        conversation_id = conversation_id\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS conversations\n                        (id INTEGER PRIMARY KEY AUTOINCREMENT, \n                        conversation_id TEXT, \n                        model_name TEXT, \n                        prompt TEXT, \n                        response TEXT, \n                        start_datetime TEXT, \n                        end_datetime TEXT,\n                        duration NUMERIC)''')\n        cursor.execute('''INSERT INTO conversations (\n                        conversation_id, \n                        model_name, \n                        prompt, \n                        response, \n                        start_datetime, \n                        end_datetime,\n                        duration) VALUES (?, ?, ?, ?, ?, ?, ?)''', \n                       (conversation_id, \n                        model_name, \n                        prompt, \n                        response, \n                        start_datetime, \n                        end_datetime, \n                        duration))\n        conn.commit()\n        conn.close()\n    except:\n        raise\n\n\n\ndef sql_log_decorator(func):\n    async def wrapper(self, *args, **kwargs):\n        start_datetime = datetime.now()\n        try:\n            # Execute the function\n            result = await func(self, *args, **kwargs)\n        except Exception as e:\n            # Handle errors within the decorated function\n            self.agent.conversation._history.pop(0)\n            print(f\"chatbot_function error: {e}\")\n            return \"\", [], kwargs['history']  \n\n        end_datetime = datetime.now()\n        \n        # SQL logging\n        # Unpacking the history and other required parameters from kwargs if they were used\n        history = kwargs.get('history', [])\n        message = kwargs.get('message', '')\n        response = result[1]  # Assuming the response is the second item in the returned tuple\n        model_name = kwargs.get('model_name', '')\n        conversation_id = str(self.agent.conversation.id)\n        sql_log(conversation_id, model_name, message, response, start_datetime, end_datetime)\n        return result\n    return wrapper\n\n\nclass SqlLogMeta(type):\n    def __new__(cls, name, bases, dct):\n        for key, value in dct.items():\n            if callable(value) and not key.startswith('__'):\n                dct[key] = sql_log(value)\n        return super().__new__(cls, name, bases, dct)\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/__init__.py",
        "content": "```swarmauri/standard/conversations/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/base/__init__.py",
        "content": "```swarmauri/standard/conversations/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/base/ConversationSystemContextMixin.py",
        "content": "```swarmauri/standard/conversations/base/ConversationSystemContextMixin.py\nfrom abc import ABC\nfrom typing import Optional, Literal\nfrom pydantic import BaseModel\nfrom swarmauri.core.conversations.ISystemContext import ISystemContext\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n\nclass ConversationSystemContextMixin(ISystemContext, BaseModel):\n    system_context: Optional[SystemMessage]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/base/ConversationBase.py",
        "content": "```swarmauri/standard/conversations/base/ConversationBase.py\nfrom typing import List, Union, Literal\nfrom pydantic import Field, PrivateAttr, ConfigDict\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.conversations.IConversation import IConversation\n\n\nclass ConversationBase(IConversation, ComponentBase):\n    \"\"\"\n    Concrete implementation of IConversation, managing conversation history and operations.\n    \"\"\"\n\n    _history: List[SubclassUnion[MessageBase]] = PrivateAttr(default_factory=list)\n    resource: ResourceTypes = Field(default=ResourceTypes.CONVERSATION.value)\n    model_config = ConfigDict(extra=\"forbid\", arbitrary_types_allowed=True)\n    type: Literal[\"ConversationBase\"] = \"ConversationBase\"\n\n    @property\n    def history(self) -> List[SubclassUnion[MessageBase]]:\n        \"\"\"\n        Provides read-only access to the conversation history.\n        \"\"\"\n        return self._history\n\n    def add_message(self, message: SubclassUnion[MessageBase]):\n        self._history.append(message)\n\n    def add_messages(self, messages: List[SubclassUnion[MessageBase]]):\n        for message in messages:\n            self._history.append(message)\n\n    def get_last(self) -> Union[SubclassUnion[MessageBase], None]:\n        if self._history:\n            return self._history[-1]\n        return None\n\n    def clear_history(self):\n        self._history.clear()\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/__init__.py",
        "content": "```swarmauri/standard/conversations/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/Conversation.py",
        "content": "```swarmauri/standard/conversations/concrete/Conversation.py\nfrom typing import Literal\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\n\nclass Conversation(ConversationBase):\n    \"\"\"\n    Concrete implementation of ConversationBase, managing conversation history and operations.\n    \"\"\"    \n    type: Literal['Conversation'] = 'Conversation'\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/MaxSizeConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/MaxSizeConversation.py\nfrom typing import Literal\nfrom pydantic import Field\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.core.conversations.IMaxSize import IMaxSize\n\nclass MaxSizeConversation(IMaxSize, ConversationBase):\n    max_size: int = Field(default=2, gt=1)\n    type: Literal['MaxSizeConversation'] = 'MaxSizeConversation'\n\n    def add_message(self, message: IMessage):\n        \"\"\"Adds a message and ensures the conversation does not exceed the max size.\"\"\"\n        super().add_message(message)\n        self._enforce_max_size_limit()\n\n    def _enforce_max_size_limit(self):\n        \"\"\"\n        Enforces the maximum size limit of the conversation history.\n        If the current history size exceeds the maximum size, the oldest messages are removed.\n        We pop two messages (one for the user's prompt, one for the assistant's response)\n        \"\"\"\n        while len(self._history) > self.max_size:\n            \n            self._history.pop(0)\n            self._history.pop(0)\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/MaxSystemContextConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/MaxSystemContextConversation.py\nfrom typing import Optional, Union, List, Literal\nfrom pydantic import Field, ConfigDict, field_validator\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.core.conversations.IMaxSize import IMaxSize\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.standard.conversations.base.ConversationSystemContextMixin import ConversationSystemContextMixin\nfrom swarmauri.standard.messages.concrete import SystemMessage, AgentMessage, HumanMessage\nfrom swarmauri.standard.exceptions.concrete import IndexErrorWithContext\n\nclass MaxSystemContextConversation(IMaxSize, ConversationSystemContextMixin, ConversationBase):\n    system_context: Optional[SystemMessage] = SystemMessage(content=\"\")\n    max_size: int = Field(default=2, gt=1)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['MaxSystemContextConversation'] = 'MaxSystemContextConversation'\n    \n    @field_validator('system_context', mode='before')\n    def set_system_context(cls, value: Union[str, SystemMessage]) -> SystemMessage:\n        if isinstance(value, str):\n            return SystemMessage(content=value)\n        return value\n    \n    @property\n    def history(self) -> List[IMessage]:\n        \"\"\"\n        Get the conversation history, ensuring it starts with a 'user' message and alternates correctly between 'user' and 'assistant' roles.\n        The maximum number of messages returned does not exceed max_size + 1.\n        \"\"\"\n        res = []  # Start with an empty list to build the proper history\n\n        # Attempt to find the first 'user' message in the history.\n        user_start_index = -1\n        for index, message in enumerate(self._history):\n            if isinstance(message, HumanMessage):  # Identify user message\n                user_start_index = index\n                break\n\n        # If no 'user' message is found, just return the system context.\n        if user_start_index == -1:\n            return [self.system_context]\n\n        # Build history from the first 'user' message ensuring alternating roles.\n        res.append(self.system_context)\n        alternating = True\n        count = 0 \n        for message in self._history[user_start_index:]:\n            if count >= self.max_size: # max size\n                break\n            if alternating and isinstance(message, HumanMessage) or not alternating and isinstance(message, AgentMessage):\n                res.append(message)\n                alternating = not alternating\n                count += 1\n            elif not alternating and isinstance(message, HumanMessage):\n                # If we find two 'user' messages in a row when expecting an 'assistant' message, we skip this 'user' message.\n                continue\n            else:\n                # If there is no valid alternate message to append, break the loop\n                break\n\n        return res\n\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history and ensures history does not exceed the max size.\n        \"\"\"\n        if isinstance(message, SystemMessage):\n            raise ValueError(f\"System context cannot be set through this method on {self.__class_name__}.\")\n        elif isinstance(message, IMessage):\n            self._history.append(message)\n        else:\n            raise ValueError(f\"Must use a subclass of IMessage\")\n        self._enforce_max_size_limit()\n        \n    def _enforce_max_size_limit(self):\n        \"\"\"\n        Remove messages from the beginning of the conversation history if the limit is exceeded.\n        We add one to max_size to account for the system context message\n        \"\"\"\n        try:\n            while len(self._history) > self.max_size + 1:\n                self._history.pop(0)\n                self._history.pop(0)\n        except IndexError as e:\n            raise IndexErrorWithContext(e)\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/SessionCacheConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/SessionCacheConversation.py\nfrom typing import Optional, Union, List, Literal\nfrom pydantic import Field, ConfigDict\nfrom collections import deque\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.core.conversations.IMaxSize import IMaxSize\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.standard.conversations.base.ConversationSystemContextMixin import ConversationSystemContextMixin\nfrom swarmauri.standard.messages.concrete import SystemMessage, AgentMessage, HumanMessage, FunctionMessage\nfrom swarmauri.standard.exceptions.concrete import IndexErrorWithContext\n\n\nclass SessionCacheConversation(IMaxSize, ConversationSystemContextMixin, ConversationBase):\n    max_size: int = Field(default=2, gt=1)\n    system_context: Optional[SystemMessage] = None\n    session_max_size: int = Field(default=-1)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['SessionCacheConversation'] = 'SessionCacheConversation'\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        if self.session_max_size == -1:\n            self.session_max_size = self.max_size\n\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history and ensures history does not exceed the max size.\n        This only allows system context to be set through the system context method.\n        We are forcing the SystemContext to be a preamble only.\n        \"\"\"\n        if isinstance(message, SystemMessage):\n            raise ValueError(f\"System context cannot be set through this method on {self.__class_name__}.\")\n        if not self._history and not isinstance(message, HumanMessage):\n            raise ValueError(\"The first message in the history must be an HumanMessage.\")\n        if self._history and isinstance(self._history[-1], HumanMessage) and isinstance(message, HumanMessage):\n            raise ValueError(\"Cannot have two repeating HumanMessages.\")\n        \n        super().add_message(message)\n\n\n    def session_to_dict(self) -> List[dict]:\n        \"\"\"\n        Converts session messages to a list of dictionaries.\n        \"\"\"\n        included_fields = {\"role\", \"content\"}\n        return [message.dict(include=included_fields) for message in self.session]\n    \n    @property\n    def session(self) -> List[IMessage]:\n        return self._history[-self.session_max_size:]\n\n    @property\n    def history(self):\n        res = []\n        if not self._history or self.max_size == 0:\n            if self.system_context:\n                return [self.system_context]\n            else:\n                return []\n\n        # Initialize alternating with the expectation to start with HumanMessage\n        alternating = True\n        count = 0\n\n        for message in self._history[-self.max_size:]:\n            if isinstance(message, HumanMessage) and alternating:\n                res.append(message)\n                alternating = not alternating  # Switch to expecting AgentMessage\n                count += 1\n            elif isinstance(message, AgentMessage) and not alternating:\n                res.append(message)\n                alternating = not alternating  # Switch to expecting HumanMessage\n                count += 1\n\n            if count >= self.max_size:\n                break\n                \n        if self.system_context:\n            res = [self.system_context] + res\n            \n        return res\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/__init__.py",
        "content": "```swarmauri/standard/documents/__init__.py\nfrom .concrete import *\nfrom .base import *\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/base/__init__.py",
        "content": "```swarmauri/standard/documents/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/base/DocumentBase.py",
        "content": "```swarmauri/standard/documents/base/DocumentBase.py\nfrom typing import Dict, Optional, Literal\nfrom pydantic import Field, ConfigDict\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\n\nclass DocumentBase(IDocument, ComponentBase):\n    content: str\n    metadata: Dict = {}\n    embedding: Optional[Vector] = None\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    resource: Optional[str] =  Field(default=ResourceTypes.DOCUMENT.value, frozen=True)\n    type: Literal['DocumentBase'] = 'DocumentBase'\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/concrete/Document.py",
        "content": "```swarmauri/standard/documents/concrete/Document.py\nfrom typing import Literal\nfrom swarmauri.standard.documents.base.DocumentBase import DocumentBase\n\nclass Document(DocumentBase):\n    type: Literal['Document'] = 'Document'\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/concrete/__init__.py",
        "content": "```swarmauri/standard/documents/concrete/__init__.py\nfrom .Document import Document\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/__init__.py",
        "content": "```swarmauri/standard/messages/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/base/__init__.py",
        "content": "```swarmauri/standard/messages/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/base/MessageBase.py",
        "content": "```swarmauri/standard/messages/base/MessageBase.py\nfrom typing import Optional, Tuple, Literal\nfrom pydantic import PrivateAttr, ConfigDict, Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.messages.IMessage import IMessage\n\nclass MessageBase(IMessage, ComponentBase):\n    content: str\n    role: str\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    resource: Optional[str] =  Field(default=ResourceTypes.MESSAGE.value, frozen=True)\n    type: Literal['MessageBase'] = 'MessageBase'\n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/__init__.py",
        "content": "```swarmauri/standard/messages/concrete/__init__.py\nfrom .HumanMessage import HumanMessage\nfrom .AgentMessage import AgentMessage\nfrom .FunctionMessage import FunctionMessage\nfrom .SystemMessage import SystemMessage\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/AgentMessage.py",
        "content": "```swarmauri/standard/messages/concrete/AgentMessage.py\nfrom typing import Optional, Any, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\nclass AgentMessage(MessageBase):\n    content: Optional[str] = None\n    role: str = Field(default='assistant')\n    #tool_calls: Optional[Any] = None\n    name: Optional[str] = None\n    type: Literal['AgentMessage'] = 'AgentMessage'\n    usage: Optional[Any] = None # \u00f0\u0178\u0161\u00a7 Placeholder for CompletionUsage(input_tokens, output_tokens, completion time, etc)\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/FunctionMessage.py",
        "content": "```swarmauri/standard/messages/concrete/FunctionMessage.py\nfrom typing import Literal, Optional, Any\nfrom pydantic import Field\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\nclass FunctionMessage(MessageBase):\n    content: str\n    role: str = Field(default='tool')\n    tool_call_id: str\n    name: str\n    type: Literal['FunctionMessage'] = 'FunctionMessage'\n    usage: Optional[Any] = None # \u00f0\u0178\u0161\u00a7 Placeholder for CompletionUsage(input_tokens, output_tokens, completion time, etc)\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/HumanMessage.py",
        "content": "```swarmauri/standard/messages/concrete/HumanMessage.py\nfrom typing import Optional, Any, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\nclass HumanMessage(MessageBase):\n    content: str\n    role: str = Field(default='user')\n    name: Optional[str] = None\n    type: Literal['HumanMessage'] = 'HumanMessage'    \n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/SystemMessage.py",
        "content": "```swarmauri/standard/messages/concrete/SystemMessage.py\nfrom typing import Optional, Any, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\nclass SystemMessage(MessageBase):\n    content: str\n    role: str = Field(default='system')\n    type: Literal['SystemMessage'] = 'SystemMessage'\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/__init__.py",
        "content": "```swarmauri/standard/parsers/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/base/__init__.py",
        "content": "```swarmauri/standard/parsers/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/base/ParserBase.py",
        "content": "```swarmauri/standard/parsers/base/ParserBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Union, List, Any, Literal\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass ParserBase(ComponentBase, ABC):\n    \"\"\"\n    Interface for chunking text into smaller pieces.\n\n    This interface defines abstract methods for chunking texts. Implementing classes\n    should provide concrete implementations for these methods tailored to their specific\n    chunking algorithms.\n    \"\"\"\n    resource: Optional[str] =  Field(default=ResourceTypes.PARSER.value)\n    type: Literal['ParserBase'] = 'ParserBase'\n    \n    @abstractmethod\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Public method to parse input data (either a str or a Message) into a list of Document instances.\n        \n        This method leverages the abstract _parse_data method which must be\n        implemented by subclasses to define specific parsing logic.\n        \"\"\"\n        pass\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/__init__.py",
        "content": "```swarmauri/standard/parsers/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/BERTEmbeddingParser.py",
        "content": "```swarmauri/standard/parsers/concrete/BERTEmbeddingParser.py\nfrom typing import List, Union, Any, Literal\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom pydantic import PrivateAttr\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass BERTEmbeddingParser(ParserBase):\n    \"\"\"\n    A parser that transforms input text into document embeddings using BERT.\n    \n    This parser tokenizes the input text, passes it through a pre-trained BERT model,\n    and uses the resulting embeddings as the document content.\n    \"\"\"\n    parser_model_name: str = 'bert-base-uncased'\n    _model: Any = PrivateAttr()\n    type: Literal['BERTEmbeddingParser'] = 'BERTEmbeddingParser'\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the BERTEmbeddingParser with a specific BERT model.\n        \n        Parameters:\n        - model_name (str): The name of the pre-trained BERT model to use.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.tokenizer = BertTokenizer.from_pretrained(self.parser_model_name)\n        self._model = BertModel.from_pretrained(self.parser_model_name)\n        self._model.eval()  # Set model to evaluation mode\n\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Tokenizes input data and generates embeddings using a BERT model.\n\n        Parameters:\n        - data (Union[str, Any]): Input data, expected to be a single string or batch of strings.\n\n        Returns:\n        - List[IDocument]: A list containing a single IDocument instance with BERT embeddings as content.\n        \"\"\"\n        \n        # Tokenization\n        inputs = self.tokenizer(data, return_tensors='pt', padding=True, truncation=True, max_length=512)\n\n        # Generate embeddings\n        with torch.no_grad():\n            outputs = self._model(**inputs)\n\n        # Use the last hidden state as document embeddings (batch_size, sequence_length, hidden_size)\n        embeddings = outputs.last_hidden_state\n        \n        # Convert to list of numpy arrays\n        embeddings = embeddings.detach().cpu().numpy()\n        \n        # For simplicity, let's consider the mean of embeddings across tokens to represent the document\n        doc_embeddings = embeddings.mean(axis=1)\n        \n        # Creating document object(s)\n        documents = [Document(doc_id=str(i), content=emb, metadata={\"source\": \"BERTEmbeddingParser\"}) for i, emb in enumerate(doc_embeddings)]\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/CSVParser.py",
        "content": "```swarmauri/standard/parsers/concrete/CSVParser.py\nimport csv\nfrom io import StringIO\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass CSVParser(ParserBase):\n    \"\"\"\n    Concrete implementation of IParser for parsing CSV formatted text into Document instances.\n\n    The parser can handle input as a CSV formatted string or from a file, with each row\n    represented as a separate Document. Assumes the first row is the header which will\n    be used as keys for document metadata.\n    \"\"\"\n    type: Literal['CSVParser'] = 'CSVParser'\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the given CSV data into a list of Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The input data to parse, either as a CSV string or file path.\n\n        Returns:\n        - List[IDocument]: A list of documents parsed from the CSV data.\n        \"\"\"\n        # Prepare an in-memory string buffer if the data is provided as a string\n        if isinstance(data, str):\n            data_stream = StringIO(data)\n        else:\n            raise ValueError(\"Data provided is not a valid CSV string\")\n\n        # Create a list to hold the parsed documents\n        documents: List[IDocument] = []\n\n        # Read CSV content row by row\n        reader = csv.DictReader(data_stream)\n        for row in reader:\n            # Each row represents a document, where the column headers are metadata fields\n            document = Document(doc_id=row.get('id', None), \n                                        content=row.get('content', ''), \n                                        metadata=row)\n            documents.append(document)\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/EntityRecognitionParser.py",
        "content": "```swarmauri/standard/parsers/concrete/EntityRecognitionParser.py\nimport spacy\nfrom typing import List, Union, Any, Literal\nfrom pydantic import PrivateAttr\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass EntityRecognitionParser(ParserBase):\n    \"\"\"\n    EntityRecognitionParser leverages NER capabilities to parse text and \n    extract entities with their respective tags such as PERSON, LOCATION, ORGANIZATION, etc.\n    \"\"\"\n    _nlp: Any = PrivateAttr()\n    type: Literal['EntityRecognitionParser'] = 'EntityRecognitionParser'\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Load a SpaCy model. The small model is used for demonstration; larger models provide improved accuracy.\n        self._nlp = spacy.load(\"en_core_web_sm\")\n    \n    def parse(self, text: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the input text, identifies entities, and returns a list of documents with entities tagged.\n\n        Parameters:\n        - text (Union[str, Any]): The input text to be parsed and analyzed for entities.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances representing the identified entities in the text.\n        \"\"\"\n        # Ensure the input is a string type before processing\n        if not isinstance(text, str):\n            text = str(text)\n        \n        # Apply the NER model\n        doc = self._nlp(text)\n\n        # Compile identified entities into documents\n        entities_docs = []\n        for ent in doc.ents:\n            # Create a document for each entity with metadata carrying entity type\n            entity_doc = Document(doc_id=ent.text, content=ent.text, metadata={\"entity_type\": ent.label_})\n            entities_docs.append(entity_doc)\n        \n        return entities_docs\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/HTMLTagStripParser.py",
        "content": "```swarmauri/standard/parsers/concrete/HTMLTagStripParser.py\nimport html\nimport re\nfrom typing import Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass HTMLTagStripParser(ParserBase):\n    \"\"\"\n    A concrete parser that removes HTML tags and unescapes HTML content,\n    leaving plain text.\n    \"\"\"\n    type: Literal['HTMLTagStripParser'] = 'HTMLTagStripParser'\n\n    def parse(self, data: str):\n        \"\"\"\n        Strips HTML tags from input data and unescapes HTML content.\n        \n        Args:\n            data (str): The HTML content to be parsed.\n        \n        Returns:\n            List[IDocument]: A list containing a single IDocument instance of the stripped text.\n        \"\"\"\n\n        # Ensure that input is a string\n        if not isinstance(data, str):\n            raise ValueError(\"HTMLTagStripParser expects input data to be of type str.\")\n        \n        # Remove HTML tags\n        text = re.sub('<[^<]+?>', '', data)  # Matches anything in < > and replaces it with empty string\n        \n        # Unescape HTML entities\n        text = html.unescape(text)\n\n        # Wrap the cleaned text into a Document and return it in a list\n        document = Document(content=text, metadata={\"original_length\": len(data)})\n        \n        return [document]\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/KeywordExtractorParser.py",
        "content": "```swarmauri/standard/parsers/concrete/KeywordExtractorParser.py\nimport yake\nfrom typing import List, Union, Any, Literal\nfrom pydantic import ConfigDict, PrivateAttr\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass KeywordExtractorParser(ParserBase):\n    \"\"\"\n    Extracts keywords from text using the YAKE keyword extraction library.\n    \"\"\"\n    lang: str = 'en'\n    num_keywords: int = 10\n    _kw_extractor: yake.KeywordExtractor = PrivateAttr(default=None)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['KeywordExtractorParser'] = 'KeywordExtractorParser'\n    \n    def __init__(self, **data):\n        super().__init__(**data)\n        self._kw_extractor = yake.KeywordExtractor(lan=self.lang,\n                                                   n=3, \n                                                   dedupLim=0.9, \n                                                   dedupFunc='seqm', \n                                                   windowsSize=1, \n                                                   top=self.num_keywords, \n                                                   features=None)\n    \n\n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Extract keywords from input text and return as list of Document instances containing keyword information.\n\n        Parameters:\n        - data (Union[str, Any]): The input text from which to extract keywords.\n\n        Returns:\n        - List[Document]: A list of Document instances, each containing information about an extracted keyword.\n        \"\"\"\n        # Ensure data is in string format for analysis\n        text = str(data) if not isinstance(data, str) else data\n\n        # Extract keywords using YAKE\n        keywords = self._kw_extractor.extract_keywords(text)\n\n        # Create Document instances for each keyword\n        documents = [Document(content=keyword, metadata={\"score\": score}) for index, (keyword, score) in enumerate(keywords)]\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/Md2HtmlParser.py",
        "content": "```swarmauri/standard/parsers/concrete/Md2HtmlParser.py\nimport re\nfrom typing import List, Tuple, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\n\nclass Md2HtmlParser(ParserBase):\n    \"\"\"\n    A concrete implementation of the IParser interface that parses Markdown text.\n    \n    This parser takes Markdown formatted text, converts it to HTML using Python's\n    markdown library, and then uses BeautifulSoup to extract plain text content. The\n    resulting plain text is then wrapped into Document instances.\n    \"\"\"\n    rules: List[Tuple[str, str]] = [\n            (r'###### (.*)', r'<h6>\\1</h6>'),\n            (r'##### (.*)', r'<h5>\\1</h5>'),\n            (r'#### (.*)', r'<h4>\\1</h4>'),\n            (r'### (.*)', r'<h3>\\1</h3>'),\n            (r'## (.*)', r'<h2>\\1</h2>'),\n            (r'# (.*)', r'<h1>\\1</h1>'),\n            (r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>'),\n            (r'\\*(.*?)\\*', r'<em>\\1</em>'),\n            (r'!\\[(.*?)\\]\\((.*?)\\)', r'<img alt=\"\\1\" src=\"\\2\" />'),\n            (r'\\[(.*?)\\]\\((.*?)\\)', r'<a href=\"\\2\">\\1</a>'),\n            (r'\\n\\n', r'<p>'),\n            (r'\\n', r'<br>'),\n        ]\n    type: Literal['Md2HtmlParser'] = 'Md2HtmlParser'\n\n    def parse(self, data: str) -> List[Document]:\n        documents = []\n        for pattern, repl in self.rules:\n            data = re.sub(pattern, repl, data)\n        documents.append( Document(content=data, metadata={} ))\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/OpenAPISpecParser.py",
        "content": "```swarmauri/standard/parsers/concrete/OpenAPISpecParser.py\nimport yaml\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass OpenAPISpecParser(ParserBase):\n    \"\"\"\n    A parser that processes OpenAPI Specification files (YAML or JSON format)\n    and extracts information into structured Document instances. \n    This is useful for building documentation, APIs inventory, or analyzing the API specification.\n    \"\"\"\n    type: Literal['OpenAPISpecParser'] = 'OpenAPISpecParser'\n    \n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parses an OpenAPI Specification from a YAML or JSON string into a list of Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The OpenAPI specification in YAML or JSON format as a string.\n\n        Returns:\n        - List[IDocument]: A list of Document instances representing the parsed information.\n        \"\"\"\n        try:\n            # Load the OpenAPI spec into a Python dictionary\n            spec_dict = yaml.safe_load(data)\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Failed to parse the OpenAPI specification: {e}\")\n        \n        documents = []\n        # Iterate over paths in the OpenAPI spec to extract endpoint information\n        for path, path_item in spec_dict.get(\"paths\", {}).items():\n            for method, operation in path_item.items():\n                # Create a Document instance for each operation\n                content = yaml.dump(operation)\n                metadata = {\n                    \"path\": path,\n                    \"method\": method.upper(),\n                    \"operationId\": operation.get(\"operationId\", \"\")\n                }\n                document = Document(content=content, metadata=metadata)\n                documents.append(document)\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/PhoneNumberExtractorParser.py",
        "content": "```swarmauri/standard/parsers/concrete/PhoneNumberExtractorParser.py\nimport re\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass PhoneNumberExtractorParser(ParserBase):\n    \"\"\"\n    A parser that extracts phone numbers from the input text.\n    Utilizes regular expressions to identify phone numbers in various formats.\n    \"\"\"\n    type: Literal['PhoneNumberExtractorParser'] = 'PhoneNumberExtractorParser'\n    \n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parses the input data, looking for phone numbers employing a regular expression.\n        Each phone number found is contained in a separate IDocument instance.\n\n        Parameters:\n        - data (Union[str, Any]): The input text to be parsed for phone numbers.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances, each containing a phone number.\n        \"\"\"\n        # Define a regular expression for phone numbers.\n        # This is a simple example and might not capture all phone number formats accurately.\n        phone_regex = r'\\+?\\d[\\d -]{8,}\\d'\n\n        # Find all occurrences of phone numbers in the text\n        phone_numbers = re.findall(phone_regex, str(data))\n\n        # Create a new IDocument for each phone number found\n        documents = [Document(content=phone_number, metadata={}) for index, phone_number in enumerate(phone_numbers)]\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/PythonParser.py",
        "content": "```swarmauri/standard/parsers/concrete/PythonParser.py\nimport ast\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass PythonParser(ParserBase):\n    \"\"\"\n    A parser that processes Python source code to extract structural elements\n    such as functions, classes, and their docstrings.\n    \n    This parser utilizes the `ast` module to parse the Python code into an abstract syntax tree (AST)\n    and then walks the tree to extract relevant information.\n    \"\"\"\n    type: Literal['PythonParser'] = 'PythonParser'\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the given Python source code to extract structural elements.\n\n        Args:\n            data (Union[str, Any]): The input Python source code as a string.\n\n        Returns:\n            List[IDocument]: A list of IDocument objects, each representing a structural element \n                             extracted from the code along with its metadata.\n        \"\"\"\n        if not isinstance(data, str):\n            raise ValueError(\"PythonParser expects a string input.\")\n        \n        documents = []\n        tree = ast.parse(data)\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):\n                element_name = node.name\n                docstring = ast.get_docstring(node)\n                \n                # Get the source code snippet\n                source_code = ast.get_source_segment(data, node)\n                \n                # Create a metadata dictionary\n                metadata = {\n                    \"type\": \"function\" if isinstance(node, ast.FunctionDef) else \"class\",\n                    \"name\": element_name,\n                    \"docstring\": docstring,\n                    \"source_code\": source_code\n                }\n                \n                # Create a Document for each structural element\n                document = Document(content=docstring, metadata=metadata)\n                documents.append(document)\n                \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/RegExParser.py",
        "content": "```swarmauri/standard/parsers/concrete/RegExParser.py\nimport re\nfrom typing import List, Union, Any, Literal, Pattern\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass RegExParser(ParserBase):\n    \"\"\"\n    A parser that uses a regular expression to extract information from text.\n    \"\"\"\n    pattern: Pattern = re.compile(r'\\d+')\n    type: Literal['RegExParser'] = 'RegExParser'\n    \n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parses the input data using the specified regular expression pattern and\n        returns a list of IDocument instances containing the extracted information.\n\n        Parameters:\n        - data (Union[str, Any]): The input data to be parsed. It can be a string or any format \n                                   that the concrete implementation can handle.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances containing the parsed information.\n        \"\"\"\n        # Ensure data is a string\n        if not isinstance(data, str):\n            data = str(data)\n\n        # Use the regular expression pattern to find all matches in the text\n        matches = self.pattern.findall(data)\n\n        # Create a Document for each match and collect them into a list\n        documents = [Document(content=match, metadata={}) for i, match in enumerate(matches)]\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/URLExtractorParser.py",
        "content": "```swarmauri/standard/parsers/concrete/URLExtractorParser.py\nimport re\nfrom urllib.parse import urlparse\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass URLExtractorParser(ParserBase):\n    \"\"\"\n    A concrete implementation of IParser that extracts URLs from text.\n    \n    This parser scans the input text for any URLs and creates separate\n    documents for each extracted URL. It utilizes regular expressions\n    to identify URLs within the given text.\n    \"\"\"\n    type: Literal['URLExtractorParser'] = 'URLExtractorParser'\n\n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parse input data (string) and extract URLs, each URL is then represented as a document.\n        \n        Parameters:\n        - data (Union[str, Any]): The input data to be parsed for URLs.\n        \n        Returns:\n        - List[IDocument]: A list of documents, each representing an extracted URL.\n        \"\"\"\n        if not isinstance(data, str):\n            raise ValueError(\"URLExtractorParser expects input data to be of type str.\")\n\n        # Regular expression for finding URLs\n        url_regex = r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\"\n        \n        # Find all matches in the text\n        urls = re.findall(url_regex, data)\n        \n        # Create a document for each extracted URL\n        documents = [Document(content=url, metadata={\"source\": \"URLExtractor\"}) for i, url in enumerate(urls)]\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/XMLParser.py",
        "content": "```swarmauri/standard/parsers/concrete/XMLParser.py\nimport xml.etree.ElementTree as ET\nfrom typing import List, Union, Any, Literal\n\nfrom pydantic import Field\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\nclass XMLParser(ParserBase):\n    \"\"\"\n    A parser that extracts information from XML data and converts it into IDocument objects.\n    This parser assumes a simple use-case where each targeted XML element represents a separate document.\n    \"\"\"\n    element_tag: str = Field(default=\"root\")\n    type: Literal['XMLParser'] = 'XMLParser'\n\n    \n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parses XML data and converts elements with the specified tag into IDocument instances.\n\n        Parameters:\n        - data (Union[str, Any]): The XML data as a string to be parsed.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances created from the XML elements.\n        \"\"\"\n        if isinstance(data, str):\n            root = ET.fromstring(data)  # Parse the XML string into an ElementTree element\n        else:\n            raise TypeError(\"Data for XMLParser must be a string containing valid XML.\")\n\n        documents = []\n        for element in root.findall(self.element_tag):\n            # Extracting content and metadata from each element\n            content = \"\".join(element.itertext())  # Get text content\n            metadata = {child.tag: child.text for child in element}  # Extract child elements as metadata\n\n            # Create a Document instance for each element\n            doc = Document(content=content, metadata=metadata)\n            documents.append(doc)\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/TextBlobNounParser.py",
        "content": "```swarmauri/standard/parsers/concrete/TextBlobNounParser.py\nfrom textblob import TextBlob\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\n\nclass TextBlobNounParser(ParserBase):\n    \"\"\"\n    A concrete implementation of IParser using TextBlob for Natural Language Processing tasks.\n\n    This parser leverages TextBlob's functionalities such as noun phrase extraction,\n    sentiment analysis, classification, language translation, and more for parsing texts.\n    \"\"\"\n\n    type: Literal[\"TextBlobNounParser\"] = \"TextBlobNounParser\"\n\n    def __init__(self, **kwargs):\n        import nltk\n\n        nltk.download(\"punkt_tab\")\n        super().__init__(**kwargs)\n\n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parses the input data using TextBlob to perform basic NLP tasks\n        and returns a list of documents with the parsed information.\n\n        Parameters:\n        - data (Union[str, Any]): The input data to parse, expected to be text data for this parser.\n\n        Returns:\n        - List[IDocument]: A list of documents with metadata generated from the parsing process.\n        \"\"\"\n        # Ensure the data is a string\n        if not isinstance(data, str):\n            raise ValueError(\"TextBlobParser expects a string as input data.\")\n\n        # Use TextBlob for NLP tasks\n        blob = TextBlob(data)\n\n        # Extracts noun phrases to demonstrate one of TextBlob's capabilities.\n        # In practice, this parser could be expanded to include more sophisticated processing.\n        noun_phrases = list(blob.noun_phrases)\n\n        # Example: Wrap the extracted noun phrases into an IDocument instance\n        # In real scenarios, you might want to include more details, like sentiment, POS tags, etc.\n        document = Document(content=data, metadata={\"noun_phrases\": noun_phrases})\n\n        return [document]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/TextBlobSentenceParser.py",
        "content": "```swarmauri/standard/parsers/concrete/TextBlobSentenceParser.py\nfrom textblob import TextBlob\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.parsers.base.ParserBase import ParserBase\n\n\nclass TextBlobSentenceParser(ParserBase):\n    \"\"\"\n    A parser that leverages TextBlob to break text into sentences.\n\n    This parser uses the natural language processing capabilities of TextBlob\n    to accurately identify sentence boundaries within large blocks of text.\n    \"\"\"\n\n    type: Literal[\"TextBlobSentenceParser\"] = \"TextBlobSentenceParser\"\n\n    def __init__(self, **kwargs):\n        import nltk\n\n        nltk.download(\"punkt_tab\")\n        super().__init__(**kwargs)\n\n    def parse(self, data: Union[str, Any]) -> List[Document]:\n        \"\"\"\n        Parses the input text into sentence-based document chunks using TextBlob.\n\n        Args:\n            data (Union[str, Any]): The input text to be parsed.\n\n        Returns:\n            List[IDocument]: A list of IDocument instances, each representing a sentence.\n        \"\"\"\n        # Ensure the input is a string\n        if not isinstance(data, str):\n            data = str(data)\n\n        # Utilize TextBlob for sentence tokenization\n        blob = TextBlob(data)\n        sentences = blob.sentences\n\n        # Create a document instance for each sentence\n        documents = [\n            Document(\n                content=str(sentence), metadata={\"parser\": \"TextBlobSentenceParser\"}\n            )\n            for index, sentence in enumerate(sentences)\n        ]\n\n        return documents\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/__init__.py",
        "content": "```swarmauri/standard/prompts/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/base/__init__.py",
        "content": "```swarmauri/standard/prompts/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/base/PromptBase.py",
        "content": "```swarmauri/standard/prompts/base/PromptBase.py\nfrom typing import Optional, Literal\nfrom pydantic import ConfigDict, Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.prompts.IPrompt import IPrompt\n\nclass PromptBase(IPrompt, ComponentBase):\n    \"\"\"\n    The ChatPrompt class represents a simple, chat-like prompt system where a \n    message can be set and retrieved as needed. It's particularly useful in \n    applications involving conversational agents, chatbots, or any system that \n    requires dynamic text-based interactions.\n    \"\"\"\n    prompt: str = \"\"\n    resource: Optional[str] =  Field(default=ResourceTypes.PROMPT.value, frozen=True)\n    type: Literal['PromptBase'] = 'PromptBase'\n\n    def __call__(self):\n        \"\"\"\n        Enables the instance to be callable, allowing direct retrieval of the message. \n        This method facilitates intuitive access to the prompt's message, mimicking callable \n        behavior seen in functional programming paradigms.\n        \n        Returns:\n        - str: The current message stored in the prompt.\n        \"\"\"\n        return self.prompt\n\n    def set_prompt(self, prompt: str):\n        \"\"\"\n        Updates the internal message of the chat prompt. This method provides a way to change \n        the content of the prompt dynamically, reflecting changes in the conversational context \n        or user inputs.\n        \n        Parameters:\n        - message (str): The new message to set for the prompt.\n        \"\"\"\n        self.prompt = prompt\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/base/PromptGeneratorBase.py",
        "content": "```swarmauri/standard/prompts/base/PromptGeneratorBase.py\nfrom typing import Dict, List, Generator, Any, Union, Optional, Literal\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.prompts.IPrompt import IPrompt\nfrom swarmauri.core.prompts.ITemplate import ITemplate\n\n\nclass PromptGeneratorBase(IPrompt, ITemplate, ComponentBase):\n    \"\"\"\n    A class that generates prompts based on a template and a list of variable sets.\n    It implements the IPrompt and ITemplate interfaces.\n    \"\"\"\n\n    template: str = \"\"\n    variables: Union[List[Dict[str, Any]], Dict[str, Any]] = {}\n    resource: Optional[str] =  Field(default=ResourceTypes.PROMPT.value, frozen=True)\n    type: Literal['PromptGeneratorBase'] = 'PromptGeneratorBase'\n\n    def set_template(self, template: str) -> None:\n        self.template = template\n\n    def set_variables(self, variables: List[Dict[str, Any]]) -> None:\n        self.variables = variables\n\n    def generate_prompt(self, variables: Dict[str, Any]) -> str:\n        \"\"\"\n        Generates a prompt using the provided variables if any\n        else uses the next variables set in the list.\n        \"\"\"\n        variables = variables if variables else self.variables.pop(0) if self.variables else {}\n        return self.template.format(**variables)\n\n    def __call__(self) -> Generator[str, None, None]:\n        \"\"\"\n        Returns a generator that yields prompts constructed from the template and \n        each set of variables in the variables list.\n        \"\"\"\n        for variables_set in self.variables:\n            yield self.generate_prompt(variables_set)\n        self.variables = []\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/base/PromptMatrixBase.py",
        "content": "```swarmauri/standard/prompts/base/PromptMatrixBase.py\nfrom typing import List, Tuple, Optional, Any, Literal\nfrom pydantic import Field, ConfigDict\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.prompts.IPromptMatrix import IPromptMatrix\n\nclass PromptMatrixBase(IPromptMatrix, ComponentBase):\n    matrix: List[List[str]] = []\n    resource: Optional[str] =  Field(default=ResourceTypes.PROMPT.value)\n    type: Literal['PromptMatrixBase'] = 'PromptMatrixBase'    \n\n    @property\n    def shape(self) -> Tuple[int, int]:\n        \"\"\"Get the shape (number of agents, sequence length) of the prompt matrix.\"\"\"\n        if self.matrix:\n            return len(self.matrix), len(self.matrix[0])\n        return 0, 0\n\n    def add_prompt_sequence(self, sequence: List[Optional[str]]) -> None:\n        if not self.matrix or (self.matrix and len(sequence) == len(self.matrix[0])):\n            self.matrix.append(sequence)\n        else:\n            raise ValueError(\"Sequence length does not match the prompt matrix dimensions.\")\n\n    def remove_prompt_sequence(self, index: int) -> None:\n        if 0 <= index < len(self.matrix):\n            self.matrix.pop(index)\n        else:\n            raise IndexError(\"Index out of range.\")\n\n    def get_prompt_sequence(self, index: int) -> List[Optional[str]]:\n        if 0 <= index < len(self._matrix):\n            return self.matrix[index]\n        else:\n            raise IndexError(\"Index out of range.\")\n\n    def show(self) -> List[List[Optional[str]]]:\n        return self.matrix\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/base/PromptTemplateBase.py",
        "content": "```swarmauri/standard/prompts/base/PromptTemplateBase.py\nfrom typing import Dict, List, Union, Optional, Literal\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.prompts.IPrompt import IPrompt\nfrom swarmauri.core.prompts.ITemplate import ITemplate\n\nclass PromptTemplateBase(IPrompt, ITemplate, ComponentBase):\n    \"\"\"\n    A class for generating prompts based on a template and variables.\n    Implements the IPrompt for generating prompts and ITemplate for template manipulation.\n    \"\"\"\n\n    template: str = \"\"\n    variables: Union[List[Dict[str, str]], Dict[str,str]] = {}\n    resource: Optional[str] =  Field(default=ResourceTypes.PROMPT.value, frozen=True)\n    type: Literal['PromptTemplateBase'] = 'PromptTemplateBase'\n\n    def set_template(self, template: str) -> None:\n        \"\"\"\n        Sets or updates the current template string.\n        \"\"\"\n        self.template = template\n\n    def set_variables(self, variables: Dict[str, str]) -> None:\n        \"\"\"\n        Sets or updates the variables to be substituted into the template.\n        \"\"\"\n        self.variables = variables\n\n    def generate_prompt(self, variables: Dict[str, str] = None) -> str:\n        variables = variables or self.variables\n        return self.template.format(**variables)\n\n    def __call__(self, variables: Optional[Dict[str, str]] = None) -> str:\n        \"\"\"\n        Generates a prompt using the current template and provided keyword arguments for substitution.\n        \"\"\"\n        variables = variables if variables else self.variables\n        return self.generate_prompt(variables)\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/__init__.py",
        "content": "```swarmauri/standard/prompts/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/Prompt.py",
        "content": "```swarmauri/standard/prompts/concrete/Prompt.py\nfrom typing import Literal\nfrom swarmauri.standard.prompts.base.PromptBase import PromptBase\n\nclass Prompt(PromptBase):\n    type: Literal['Prompt'] = 'Prompt'\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/PromptGenerator.py",
        "content": "```swarmauri/standard/prompts/concrete/PromptGenerator.py\nfrom typing import Literal\nfrom swarmauri.standard.prompts.base.PromptGeneratorBase import PromptGeneratorBase\n\nclass PromptGenerator(PromptGeneratorBase):\n    type: Literal['PromptGenerator'] = 'PromptGenerator'\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/PromptMatrix.py",
        "content": "```swarmauri/standard/prompts/concrete/PromptMatrix.py\nfrom typing import Literal\nfrom swarmauri.standard.prompts.base.PromptMatrixBase import PromptMatrixBase\n\nclass PromptMatrix(PromptMatrixBase):\n    type: Literal['PromptMatrix'] = 'PromptMatrix'\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/PromptTemplate.py",
        "content": "```swarmauri/standard/prompts/concrete/PromptTemplate.py\nfrom typing import Literal\nfrom swarmauri.standard.prompts.base.PromptTemplateBase import PromptTemplateBase\n\nclass PromptTemplate(PromptTemplateBase):\n    type: Literal['PromptTemplate'] = 'PromptTemplate'\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/__init__.py",
        "content": "```swarmauri/standard/swarms/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/base/__init__.py",
        "content": "```swarmauri/standard/swarms/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/base/SwarmComponentBase.py",
        "content": "```swarmauri/standard/swarms/base/SwarmComponentBase.py\nfrom swarmauri.core.swarms.ISwarmComponent import ISwarmComponent\n\nclass SwarmComponentBase(ISwarmComponent):\n    \"\"\"\n    Interface for defining basics of any component within the swarm system.\n    \"\"\"\n    def __init__(self, key: str, name: str, superclass: str, module: str, class_name: str, args=None, kwargs=None):\n        self.key = key\n        self.name = name\n        self.superclass = superclass\n        self.module = module\n        self.class_name = class_name\n        self.args = args or []\n        self.kwargs = kwargs or {}\n    \n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/concrete/__init__.py",
        "content": "```swarmauri/standard/swarms/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/concrete/SimpleSwarmFactory.py",
        "content": "```swarmauri/standard/swarms/concrete/SimpleSwarmFactory.py\nimport json\nimport pickle\nfrom typing import List\nfrom swarmauri.core.chains.ISwarmFactory import (\n    ISwarmFactory , \n    CallableChainItem, \n    AgentDefinition, \n    FunctionDefinition\n)\nclass SimpleSwarmFactory(ISwarmFactory):\n    def __init__(self):\n        self.swarms = []\n        self.callable_chains = []\n\n    def create_swarm(self, agents=[]):\n        swarm = {\"agents\": agents}\n        self.swarms.append(swarm)\n        return swarm\n\n    def create_agent(self, agent_definition: AgentDefinition):\n        # For simplicity, agents are stored in a list\n        # Real-world usage might involve more sophisticated management and instantiation based on type and configuration\n        agent = {\"definition\": agent_definition._asdict()}\n        self.agents.append(agent)\n        return agent\n\n    def create_callable_chain(self, chain_definition: List[CallableChainItem]):\n        chain = {\"definition\": [item._asdict() for item in chain_definition]}\n        self.callable_chains.append(chain)\n        return chain\n\n    def register_function(self, function_definition: FunctionDefinition):\n        if function_definition.identifier in self.functions:\n            raise ValueError(f\"Function {function_definition.identifier} is already registered.\")\n        \n        self.functions[function_definition.identifier] = function_definition\n    \n    def export_configuration(self, format_type: str = 'json'):\n        # Now exporting both swarms and callable chains\n        config = {\"swarms\": self.swarms, \"callable_chains\": self.callable_chains}\n        if format_type == \"json\":\n            return json.dumps(config)\n        elif format_type == \"pickle\":\n            return pickle.dumps(config)\n\n    def load_configuration(self, config_data, format_type: str = 'json'):\n        # Loading both swarms and callable chains\n        config = json.loads(config_data) if format_type == \"json\" else pickle.loads(config_data)\n        self.swarms = config.get(\"swarms\", [])\n        self.callable_chains = config.get(\"callable_chains\", [])\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/__init__.py",
        "content": "```swarmauri/standard/toolkits/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/base/__init__.py",
        "content": "```swarmauri/standard/toolkits/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/base/ToolkitBase.py",
        "content": "```swarmauri/standard/toolkits/base/ToolkitBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, List, Literal\nfrom pydantic import Field, ConfigDict\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.toolkits.IToolkit import IToolkit\n\n\n\nclass ToolkitBase(IToolkit, ComponentBase):\n    \"\"\"\n    A class representing a toolkit used by Swarm Agents.\n    Tools are maintained in a dictionary keyed by the tool's name.\n    \"\"\"\n\n    tools: Dict[str, SubclassUnion[ToolBase]] = {}\n    resource: Optional[str] =  Field(default=ResourceTypes.TOOLKIT.value, frozen=True)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['ToolkitBase'] = 'ToolkitBase'\n\n    def get_tools(self, \n                   include: Optional[List[str]] = None, \n                   exclude: Optional[List[str]] = None,\n                   by_alias: bool = False, \n                   exclude_unset: bool = False,\n                   exclude_defaults: bool = False, \n                   exclude_none: bool = False\n                   ) -> Dict[str, SubclassUnion[ToolBase]]:\n            \"\"\"\n            List all tools in the toolkit with options to include or exclude specific fields.\n    \n            Parameters:\n                include (List[str], optional): Fields to include in the returned dictionary.\n                exclude (List[str], optional): Fields to exclude from the returned dictionary.\n    \n            Returns:\n                Dict[str, SubclassUnion[ToolBase]]: A dictionary of tools with specified fields included or excluded.\n            \"\"\"\n            return [tool.model_dump(include=include, exclude=exclude, by_alias=by_alias,\n                                   exclude_unset=exclude_unset, exclude_defaults=exclude_defaults, \n                                    exclude_none=exclude_none) for name, tool in self.tools.items()]\n\n    def add_tools(self, tools: Dict[str, SubclassUnion[ToolBase]]) -> None:\n        \"\"\"\n        Add multiple tools to the toolkit.\n\n        Parameters:\n            tools (Dict[str, Tool]): A dictionary of tool objects keyed by their names.\n        \"\"\"\n        self.tools.update(tools)\n\n    def add_tool(self, tool: SubclassUnion[ToolBase])  -> None:\n        \"\"\"\n        Add a single tool to the toolkit.\n\n        Parameters:\n            tool (Tool): The tool instance to be added to the toolkit.\n        \"\"\"\n        self.tools[tool.name] = tool\n\n    def remove_tool(self, tool_name: str) -> None:\n        \"\"\"\n        Remove a tool from the toolkit by name.\n\n        Parameters:\n            tool_name (str): The name of the tool to be removed from the toolkit.\n        \"\"\"\n        if tool_name in self.tools:\n            del self.tools[tool_name]\n        else:\n            raise ValueError(f\"Tool '{tool_name}' not found in the toolkit.\")\n\n    def get_tool_by_name(self, tool_name: str) -> SubclassUnion[ToolBase]:\n        \"\"\"\n        Get a tool from the toolkit by name.\n\n        Parameters:\n            tool_name (str): The name of the tool to retrieve.\n\n        Returns:\n            Tool: The tool instance with the specified name.\n        \"\"\"\n        if tool_name in self.tools:\n            return self.tools[tool_name]\n        else:\n            raise ValueError(f\"Tool '{tool_name}' not found in the toolkit.\")\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns the number of tools in the toolkit.\n\n        Returns:\n            int: The number of tools in the toolkit.\n        \"\"\"\n        return len(self.tools)\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/concrete/__init__.py",
        "content": "```swarmauri/standard/toolkits/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/concrete/Toolkit.py",
        "content": "```swarmauri/standard/toolkits/concrete/Toolkit.py\nfrom typing import Literal\nfrom swarmauri.standard.toolkits.base.ToolkitBase import ToolkitBase\n\nclass Toolkit(ToolkitBase):\n    type: Literal['Toolkit'] = 'Toolkit'\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/__init__.py",
        "content": "```swarmauri/standard/tools/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/base/__init__.py",
        "content": "```swarmauri/standard/tools/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/base/ParameterBase.py",
        "content": "```swarmauri/standard/tools/base/ParameterBase.py\nfrom typing import Optional, List, Any\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.tools.IParameter import IParameter\n\n\nclass ParameterBase(IParameter, ComponentBase):\n    name: str\n    description: str\n    required: bool = False\n    enum: Optional[List[str]] = None\n    resource: Optional[str] =  Field(default=ResourceTypes.PARAMETER.value)\n    type: str # THIS DOES NOT USE LITERAL\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/base/ToolBase.py",
        "content": "```swarmauri/standard/tools/base/ToolBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, List, Any, Literal\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\nfrom swarmauri.core.tools.ITool import ITool\n\n\nclass ToolBase(ITool, ComponentBase, ABC):\n    name: str\n    description: Optional[str] = None\n    parameters: List[Parameter] = Field(default_factory=list)\n    resource: Optional[str] =  Field(default=ResourceTypes.TOOL.value)\n    type: Literal['ToolBase'] = 'ToolBase'\n    \n    def call(self, *args, **kwargs):\n        return self.__call__(*args, **kwargs)\n    \n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError(\"Subclasses must implement the __call__ method.\")\n\n\n   # #def __getstate__(self):\n        # return {'type': self.type, 'function': self.function}\n\n\n    #def __iter__(self):\n    #    yield ('type', self.type)\n    #    yield ('function', self.function)\n\n    # @property\n    # def function(self):\n    #     # Dynamically constructing the parameters schema\n    #     properties = {}\n    #     required = []\n\n    #     for param in self.parameters:\n    #         properties[param.name] = {\n    #             \"type\": param.type,\n    #             \"description\": param.description,\n    #         }\n    #         if param.enum:\n    #             properties[param.name]['enum'] = param.enum\n\n    #         if param.required:\n    #             required.append(param.name)\n\n    #     function = {\n    #         \"name\": self.name,\n    #         \"description\": self.description,\n    #         \"parameters\": {\n    #             \"type\": \"object\",\n    #             \"properties\": properties,\n    #         }\n    #     }\n        \n    #     if required:  # Only include 'required' if there are any required parameters\n    #         function['parameters']['required'] = required\n    #     return function\n\n   # def as_dict(self):\n    #    #return asdict(self)\n   #     return {'type': self.type, 'function': self.function}\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/__init__.py",
        "content": "```swarmauri/standard/tools/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/AdditionTool.py",
        "content": "```swarmauri/standard/tools/concrete/AdditionTool.py\nfrom typing import List, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase \nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass AdditionTool(ToolBase):\n    version: str = \"0.0.1\"\n    parameters: List[Parameter] = Field(default_factory=lambda: [\n            Parameter(\n                name=\"x\",\n                type=\"integer\",\n                description=\"The left operand\",\n                required=True\n            ),\n            Parameter(\n                name=\"y\",\n                type=\"integer\",\n                description=\"The right operand\",\n                required=True\n            )\n        ])\n\n    name: str = 'AdditionTool'\n    description: str = \"This tool has two numbers together\"\n    type: Literal['AdditionTool'] = 'AdditionTool'\n\n\n    def __call__(self, x: int, y: int) -> int:\n        \"\"\"\n        Add two numbers x and y and return the sum.\n\n        Parameters:\n        - x (int): The first number.\n        - y (int): The second number.\n\n        Returns:\n        - str: The sum of x and y.\n        \"\"\"\n        return str(x + y)\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/CalculatorTool.py",
        "content": "```swarmauri/standard/tools/concrete/CalculatorTool.py\nfrom typing import List, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase \nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass CalculatorTool(ToolBase):\n    version: str = \"1.0.0\"\n    parameters: List[Parameter] = Field(default_factory=lambda: [\n        Parameter(\n            name=\"operation\",\n            type=\"string\",\n            description=\"The arithmetic operation to perform ('add', 'subtract', 'multiply', 'divide').\",\n            required=True,\n            enum=[\"add\", \"subtract\", \"multiply\", \"divide\"]\n        ),\n        Parameter(\n            name=\"x\",\n            type=\"number\",\n            description=\"The left operand for the operation.\",\n            required=True\n        ),\n        Parameter(\n            name=\"y\",\n            type=\"number\",\n            description=\"The right operand for the operation.\",\n            required=True\n        )\n    ])\n    name: str = 'CalculatorTool'\n    description: str = \"Performs basic arithmetic operations.\"\n    type: Literal['CalculatorTool'] = 'CalculatorTool'\n\n    def __call__(self, operation: str, x: float, y: float) -> str:\n        try:\n            if operation == \"add\":\n                result = x + y\n            elif operation == \"subtract\":\n                result = x - y\n            elif operation == \"multiply\":\n                result = x * y\n            elif operation == \"divide\":\n                if y == 0:\n                    return \"Error: Division by zero.\"\n                result = x / y\n            else:\n                return \"Error: Unknown operation.\"\n            return str(result)\n        except Exception as e:\n            return f\"An error occurred: {str(e)}\"\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/CodeInterpreterTool.py",
        "content": "```swarmauri/standard/tools/concrete/CodeInterpreterTool.py\nimport sys\nimport io\nfrom typing import List, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase \nfrom swarmauri.standard.tools.concrete.Parameter import Parameter \n\n\nclass CodeInterpreterTool(ToolBase):\n    version: str = \"1.0.0\"\n    parameters: List[Parameter] = Field(default_factory=lambda: [\n            Parameter(\n                name=\"user_code\",\n                type=\"string\",\n                description=(\"Executes the provided Python code snippet in a secure sandbox environment. \"\n                             \"This tool is designed to interpret the execution of the python code snippet.\"\n                             \"Returns the output\"),\n                required=True\n            )\n        ])\n    name: str = 'CodeInterpreterTool'\n    description: str = \"Executes provided Python code and captures its output.\"\n    type: Literal['CodeInterpreterTool'] = 'CodeInterpreterTool'\n\n    def __call__(self, user_code: str) -> str:\n        \"\"\"\n        Executes the provided user code and captures its stdout output.\n        \n        Parameters:\n            user_code (str): Python code to be executed.\n        \n        Returns:\n            str: Captured output or error message from the executed code.\n        \"\"\"\n        return self.execute_code(user_code)\n    \n    def execute_code(self, user_code: str) -> str:\n        \"\"\"\n        Executes the provided user code and captures its stdout output.\n\n        Args:\n            user_code (str): Python code to be executed.\n\n        Returns:\n            str: Captured output or error message from the executed code.\n        \"\"\"\n        old_stdout = sys.stdout\n        redirected_output = sys.stdout = io.StringIO()\n\n        try:\n            # Note: Consider security implications of using 'exec'\n            builtins = globals().copy()\n            exec(user_code, builtins)\n            sys.stdout = old_stdout\n            captured_output = redirected_output.getvalue()\n            return str(captured_output)\n        except Exception as e:\n            sys.stdout = old_stdout\n            return f\"An error occurred: {str(e)}\"\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/ImportMemoryModuleTool.py",
        "content": "```swarmauri/standard/tools/concrete/ImportMemoryModuleTool.py\nimport sys\nimport types\nimport importlib\nfrom typing import List, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase \nfrom swarmauri.standard.tools.concrete.Parameter import Parameter \n\nclass ImportMemoryModuleTool(ToolBase):\n    version: str = \"1.0.0\"\n    parameters: List[Parameter] = Field(default_factory=lambda: [\n            Parameter(\n                name=\"name\",\n                type=\"string\",\n                description=\"Name of the new module.\",\n                required=True\n            ),\n            Parameter(\n                name=\"code\",\n                type=\"string\",\n                description=\"Python code snippet to include in the module.\",\n                required=True\n            ),\n            Parameter(\n                name=\"package_path\",\n                type=\"string\",\n                description=\"Dot-separated package path where the new module should be inserted.\",\n                required=True\n            )\n        ])\n    \n    name: str = 'ImportMemoryModuleTool'\n    description: str = \"Dynamically imports a module from memory into a specified package path.\"\n    type: Literal['ImportMemoryModuleTool'] = 'ImportMemoryModuleTool'\n\n    def __call__(self, name: str, code: str, package_path: str) -> str:\n        \"\"\"\n        Dynamically creates a module from a code snippet and inserts it into the specified package path.\n\n        Args:\n            name (str): Name of the new module.\n            code (str): Python code snippet to include in the module.\n            package_path (str): Dot-separated package path where the new module should be inserted.\n        \"\"\"\n        # Implementation adapted from the provided snippet\n        # Ensure the package structure exists\n        current_package = self.ensure_module(package_path)\n        \n        # Create a new module\n        module = types.ModuleType(name)\n        \n        # Execute code in the context of this new module\n        exec(code, module.__dict__)\n        \n        # Insert the new module into the desired location\n        setattr(current_package, name, module)\n        sys.modules[package_path + '.' + name] = module\n        return f\"{name} has been successfully imported into {package_path}\"\n\n    @staticmethod\n    def ensure_module(package_path: str):\n        package_parts = package_path.split('.')\n        module_path = \"\"\n        current_module = None\n\n        for part in package_parts:\n            if module_path:\n                module_path += \".\" + part\n            else:\n                module_path = part\n                \n            if module_path not in sys.modules:\n                try:\n                    # Try importing the module; if it exists, this will add it to sys.modules\n                    imported_module = importlib.import_module(module_path)\n                    sys.modules[module_path] = imported_module\n                except ImportError:\n                    # If the module doesn't exist, create a new placeholder module\n                    new_module = types.ModuleType(part)\n                    if current_module:\n                        setattr(current_module, part, new_module)\n                    sys.modules[module_path] = new_module\n            current_module = sys.modules[module_path]\n\n        return current_module\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/Parameter.py",
        "content": "```swarmauri/standard/tools/concrete/Parameter.py\nfrom swarmauri.standard.tools.base.ParameterBase import ParameterBase\n\nclass Parameter(ParameterBase):\n    pass\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/TestTool.py",
        "content": "```swarmauri/standard/tools/concrete/TestTool.py\nfrom typing import List, Literal\nfrom pydantic import Field\nimport subprocess as sp\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase \nfrom swarmauri.standard.tools.concrete.Parameter import Parameter \n\nclass TestTool(ToolBase):\n    version: str = \"1.0.0\"\n        \n    # Define the parameters required by the tool\n    parameters: List[Parameter] = Field(default_factory=lambda: [\n        Parameter(\n            name=\"program\",\n            type=\"string\",\n            description=\"The program that the user wants to open ('notepad' or 'calc' or 'mspaint')\",\n            required=True,\n            enum=[\"notepad\", \"calc\", \"mspaint\"]\n        )\n    ])\n    name: str = 'TestTool'\n    description: str = \"This opens a program based on the user's request.\"\n    type: Literal['TestTool'] = 'TestTool'\n\n    def __call__(self, program) -> str:\n        # sp.check_output(program)\n        # Here you would implement the actual logic for fetching the weather information.\n        # For demonstration, let's just return the parameters as a string.\n        return f\"Program Opened: {program}\"\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/WeatherTool.py",
        "content": "```swarmauri/standard/tools/concrete/WeatherTool.py\nfrom typing import List, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase \nfrom swarmauri.standard.tools.concrete.Parameter import Parameter \n\nclass WeatherTool(ToolBase):\n    version: str = \"0.1.dev1\"\n    parameters: List[Parameter] = Field(default_factory=lambda: [\n        Parameter(\n            name=\"location\",\n            type=\"string\",\n            description=\"The location for which to fetch weather information\",\n            required=True\n        ),\n        Parameter(\n            name=\"unit\",\n            type=\"string\",\n            description=\"The unit for temperature ('fahrenheit' or 'celsius')\",\n            required=True,\n            enum=[\"fahrenheit\", \"celsius\"]\n        )\n    ])\n    name: str = 'WeatherTool'\n    description: str = \"Fetch current weather info for a location\"\n    type: Literal['WeatherTool'] = 'WeatherTool'\n\n    def __call__(self, location, unit=\"fahrenheit\") -> str:\n        weather_info = (location, unit)\n        # Here you would implement the actual logic for fetching the weather information.\n        # For demonstration, let's just return the parameters as a string.\n        return f\"Weather Info: {weather_info}\\n\"\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/CodeExtractorTool.py",
        "content": "```swarmauri/standard/tools/concrete/CodeExtractorTool.py\nimport ast\nfrom typing import List, Literal\nfrom pydantic import Field\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass CodeExtractorTool(ToolBase):\n    version: str = \"1.0.0\"\n    parameters: List[Parameter] = Field(\n        default_factory=lambda: [\n            Parameter(\n                name=\"file_name\",\n                type=\"string\",\n                description=\"The name of the Python file to extract code from.\",\n                required=True,\n            ),\n            Parameter(\n                name=\"extract_documentation\",\n                type=\"bool\",\n                description=\"Whether to start extracting code from the documentation string.\",\n                required=False,\n                default=True,\n            ),\n            Parameter(\n                name=\"to_be_ignored\",\n                type=\"list\",\n                description=\"A list of function or variable names to be ignored during code extraction.\",\n                required=False,\n                default=[],\n            ),\n        ]\n    )\n    name: str = \"CodeExtractorTool\"\n    description: str = \"Extracts code from a Python file.\"\n    type: Literal[\"CodeExtractorTool\"] = \"CodeExtractorTool\"\n\n    def __call__(\n        self,\n        file_name: str,\n        extract_documentation: bool = True,\n        to_be_ignored: List[str] = [],\n    ) -> str:\n        \"\"\"\n        Extracts code from a Python file.\n\n        Parameters:\n            file_name (str): The name of the Python file to extract code from.\n            extract_documentation (bool): Whether to start extracting code from the documentation string.\n            to_be_ignored (List[str]): A list of function or variable names to be ignored during code extraction.\n\n        Returns:\n            str: Extracted code.\n        \"\"\"\n        return self.extract_code(file_name, extract_documentation, to_be_ignored)\n\n    def extract_code(\n        self,\n        file_name: str,\n        extract_documentation: bool = True,\n        to_be_ignored: List[str] = [],\n    ) -> str:\n        \"\"\"\n        Extracts code from a Python file.\n\n        Args:\n            file_name (str): The name of the Python file to extract code from.\n            extract_documentation (bool): Whether to start extracting code from the documentation string.\n            to_be_ignored (List[str]): A list of function or variable names to be ignored during code extraction.\n\n        Returns:\n            str: Extracted code.\n        \"\"\"\n        response_lines = []\n\n        # Read the current file and collect relevant lines\n        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n            documentation_start = False\n            first = not extract_documentation\n\n            for line in f:\n                stripped_line = line.strip()\n\n                # Check if the line starts or ends the documentation string\n                if first and '\"\"\"' in stripped_line:\n                    documentation_start = not documentation_start\n                    first = False\n                    continue\n\n                if documentation_start and '\"\"\"' in stripped_line:\n                    documentation_start = not documentation_start\n                    continue\n\n                if documentation_start and not extract_documentation:\n                    continue\n\n                # Stop collecting lines when reaching the specified comment\n                if \"#\" in stripped_line and \"non-essentials\" in stripped_line:\n                    break\n\n                # Collect the line\n                response_lines.append(line)\n\n        response = \"\".join(response_lines)\n\n        # Parse the response with AST\n        tree = ast.parse(response)\n\n        # Filter out nodes based on the `to_be_ignored` set\n        class CodeCleaner(ast.NodeTransformer):\n            def visit_FunctionDef(self, node):\n                if any(pattern in node.name for pattern in to_be_ignored):\n                    return None\n                return node\n\n            def visit_Assign(self, node):\n                if any(\n                    isinstance(target, ast.Name)\n                    and any(pattern in target.id for pattern in to_be_ignored)\n                    for target in node.targets\n                ):\n                    return None\n                return node\n\n        # Transform the AST to remove ignored nodes\n        cleaned_tree = CodeCleaner().visit(tree)\n\n        # Convert the cleaned AST back to source code\n        cleaned_code = ast.unparse(cleaned_tree)\n\n        # Return the cleaned code\n        return cleaned_code\n\n```"
    },
    {
        "document_name": "swarmauri/standard/apis/__init__.py",
        "content": "```swarmauri/standard/apis/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/apis/base/__init__.py",
        "content": "```swarmauri/standard/apis/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/apis/concrete/__init__.py",
        "content": "```swarmauri/standard/apis/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/__init__.py",
        "content": "```swarmauri/standard/vector_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/__init__.py",
        "content": "```swarmauri/standard/vector_stores/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/VectorStoreBase.py",
        "content": "```swarmauri/standard/vector_stores/base/VectorStoreBase.py\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Literal\nfrom pydantic import Field, PrivateAttr\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.core.vector_stores.IVectorStore import IVectorStore\n\nclass VectorStoreBase(IVectorStore, ComponentBase):\n    \"\"\"\n    Abstract base class for document stores, implementing the IVectorStore interface.\n\n    This class provides a standard API for adding, updating, getting, and deleting documents in a vector store.\n    The specifics of storing (e.g., in a database, in-memory, or file system) are to be implemented by concrete subclasses.\n    \"\"\"\n    documents: List[Document] = []\n    _embedder = PrivateAttr()\n    _distance = PrivateAttr()\n    resource: Optional[str] =  Field(default=ResourceTypes.VECTOR_STORE.value)\n    type: Literal['VectorStoreBase'] = 'VectorStoreBase'\n    \n    @property\n    def embedder(self):\n        return self._embedder\n\n    @abstractmethod\n    def add_document(self, document: Document) -> None:\n        \"\"\"\n        Add a single document to the document store.\n\n        Parameters:\n        - document (IDocument): The document to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_documents(self, documents: List[Document]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n\n        Parameters:\n        - documents (List[IDocument]): A list of documents to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document(self, id: str) -> Optional[Document]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to retrieve.\n\n        Returns:\n        - Optional[IDocument]: The requested document if found; otherwise, None.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_documents(self) -> List[Document]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n\n        Returns:\n        - List[IDocument]: A list of all documents in the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_document(self, id: str, updated_document: Document) -> None:\n        \"\"\"\n        Update a document in the document store.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to update.\n        - updated_document (IDocument): The updated document instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_document(self, id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to delete.\n        \"\"\"\n        pass\n\n    def clear_documents(self) -> None:\n        \"\"\"\n        Deletes all documents from the vector store\n\n        \"\"\"\n        self.documents = []\n    \n    def document_count(self):\n        return len(self.documents)\n    \n    def document_dumps(self) -> str:\n        \"\"\"\n        Placeholder\n        \"\"\"\n        return json.dumps([each.to_dict() for each in self.documents])\n\n    def document_dump(self, file_path: str) -> None:\n        \"\"\"\n        Placeholder\n        \"\"\"\n        with open(file_path, 'w', encoding='utf-8') as f:\n            json.dump([each.to_dict() for each in self.documents], \n                f,\n                ensure_ascii=False, \n                indent=4)  \n\n    def document_loads(self, json_data: str) -> None:\n        \"\"\"\n        Placeholder\n        \"\"\"\n        self.documents = [globals()[each['type']].from_dict(each) for each in json.loads(json_data)]\n\n    def document_load(self, file_path: str) -> None:\n        \"\"\"\n        Placeholder\n        \"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            self.documents = [globals()[each['type']].from_dict(each) for each in json.load(file_path)]\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/VectorStoreRetrieveMixin.py",
        "content": "```swarmauri/standard/vector_stores/base/VectorStoreRetrieveMixin.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom pydantic import BaseModel\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.core.vector_stores.IVectorStoreRetrieve import IVectorStoreRetrieve\n\n\nclass VectorStoreRetrieveMixin(IVectorStoreRetrieve, BaseModel):\n    \n    @abstractmethod\n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        \n        Args:\n            query (str): The query string used for document retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/VectorStoreSaveLoadMixin.py",
        "content": "```swarmauri/standard/vector_stores/base/VectorStoreSaveLoadMixin.py\nfrom typing import List\nimport os\nfrom pydantic import BaseModel\nimport json\nimport glob\nimport importlib \nfrom swarmauri.core.vector_stores.IVectorStoreSaveLoad import IVectorStoreSaveLoad\nfrom swarmauri.standard.documents.concrete.Document import Document\n\nclass VectorStoreSaveLoadMixin(IVectorStoreSaveLoad, BaseModel):\n    \"\"\"\n    Base class for vector stores with built-in support for saving and loading\n    the vectorizer's model and the documents.\n    \"\"\"\n    \n    \n    def save_store(self, directory_path: str) -> None:\n        \"\"\"\n        Saves both the vectorizer's model and the documents.\n        \"\"\"\n        # Ensure the directory exists\n        if not os.path.exists(directory_path):\n            os.makedirs(directory_path)\n            \n        # Save the vectorizer model\n        model_path = os.path.join(directory_path, \"embedding_model\")\n        self._vectorizer.save_model(model_path)\n        \n        # Save documents\n        documents_path = os.path.join(directory_path, \"documents.json\")\n        with open(documents_path, 'w', encoding='utf-8') as f:\n            json.dump([each.to_dict() for each in self.documents], \n                f,\n                ensure_ascii=False, \n                indent=4)\n\n    \n    def load_store(self, directory_path: str) -> None:\n        \"\"\"\n        Loads both the vectorizer's model and the documents.\n        \"\"\"\n        # Load the vectorizer model\n        model_path = os.path.join(directory_path, \"embedding_model\")\n        self.vectorizer.load_model(model_path)\n        \n        # Load documents\n        documents_path = os.path.join(directory_path, \"documents.json\")\n        with open(documents_path, 'r', encoding='utf-8') as f:\n            self.documents = [self._load_document(each) for each in json.load(f)]\n\n    def _load_document(self, data):\n        document_type = data.pop(\"type\") \n        if document_type:\n            module = importlib.import_module(f\"swarmauri.standard.documents.concrete.{document_type}\")\n            document_class = getattr(module, document_type)\n            document = document_class.from_dict(data)\n            return document\n        else:\n            raise ValueError(\"Unknown document type\")\n\n    def save_parts(self, directory_path: str, chunk_size: int = 10485760) -> None:\n        \"\"\"\n        Splits the file into parts if it's too large and saves those parts individually.\n        \"\"\"\n        file_number = 1\n        model_path = os.path.join(directory_path, \"embedding_model\")\n        parts_directory = os.path.join(directory_path, \"parts\")\n        \n        if not os.path.exists(parts_directory):\n            os.makedirs(parts_directory)\n\n\n\n        with open(f\"{model_path}/model.safetensors\", 'rb') as f:\n            chunk = f.read(chunk_size)\n            while chunk:\n                with open(f\"{parts_directory}/model.safetensors.part{file_number:03}\", 'wb') as chunk_file:\n                    chunk_file.write(chunk)\n                file_number += 1\n                chunk = f.read(chunk_size)\n\n        # Split the documents into parts and save them\n        documents_dir = os.path.join(directory_path, \"documents\")\n\n        self._split_json_file(directory_path, chunk_size=chunk_size)\n\n\n    def _split_json_file(self, directory_path: str, max_records=100, chunk_size: int = 10485760):    \n        # Read the input JSON file\n        combined_documents_file_path = os.path.join(directory_path, \"documents.json\")\n\n        # load the master JSON file\n        with open(combined_documents_file_path , 'r') as file:\n            data = json.load(file)\n\n        # Set and Create documents parts folder if it does not exist\n        documents_dir = os.path.join(directory_path, \"documents\")\n        if not os.path.exists(documents_dir):\n            os.makedirs(documents_dir)\n        current_batch = []\n        file_index = 1\n        current_size = 0\n        \n        for record in data:\n            current_batch.append(record)\n            current_size = len(json.dumps(current_batch).encode('utf-8'))\n            \n            # Check if current batch meets the splitting criteria\n            if len(current_batch) >= max_records or current_size >= chunk_size:\n                # Write current batch to a new file\n                output_file = f'document_part_{file_index}.json'\n                output_file = os.path.join(documents_dir, output_file)\n                with open(output_file, 'w') as outfile:\n                    json.dump(current_batch, outfile)\n                \n                # Prepare for the next batch\n                current_batch = []\n                current_size = 0\n                file_index += 1\n\n        # Check if there's any remaining data to be written\n        if current_batch:\n            output_file = f'document_part_{file_index}.json'\n            output_file = os.path.join(documents_dir, output_file)\n            with open(output_file, 'w') as outfile:\n                json.dump(current_batch, outfile)\n\n    def load_parts(self, directory_path: str, file_pattern: str = '*.part*') -> None:\n        \"\"\"\n        Combines file parts from a directory back into a single file and loads it.\n        \"\"\"\n        model_path = os.path.join(directory_path, \"embedding_model\")\n        parts_directory = os.path.join(directory_path, \"parts\")\n        output_file_path = os.path.join(model_path, \"model.safetensors\")\n\n        parts = sorted(glob.glob(os.path.join(parts_directory, file_pattern)))\n        with open(output_file_path, 'wb') as output_file:\n            for part in parts:\n                with open(part, 'rb') as file_part:\n                    output_file.write(file_part.read())\n\n        # Load the combined_model now        \n        model_path = os.path.join(directory_path, \"embedding_model\")\n        self._vectorizer.load_model(model_path)\n\n        # Load document files\n        self._load_documents(directory_path)\n        \n\n    def _load_documents(self, directory_path: str) -> None:\n        \"\"\"\n        Loads the documents from parts stored in the given directory.\n        \"\"\"\n        part_paths = glob.glob(os.path.join(directory_path, \"documents/*.json\"))\n        for part_path in part_paths:\n            with open(part_path, \"r\") as f:\n                part_documents = json.load(f)\n                for document_data in part_documents:\n                    document_type = document_data.pop(\"type\")\n                    document_module = importlib.import_module(f\"swarmauri.standard.documents.concrete.{document_type}\")\n                    document_class = getattr(document_module, document_type)\n                    document = document_class.from_dict(document_data)\n                    self.documents.append(document)\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/__init__.py",
        "content": "```swarmauri/standard/vector_stores/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/Doc2VecVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/Doc2VecVectorStore.py\nfrom typing import List, Union, Literal\nfrom pydantic import PrivateAttr\n\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.embeddings.concrete.TfidfEmbedding import TfidfEmbedding\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import VectorStoreRetrieveMixin\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import VectorStoreSaveLoadMixin    \n\n\nclass Doc2VecVectorStore(VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase):\n    type: Literal['Doc2VecVectorStore'] = 'Doc2VecVectorStore'\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._embedder = TfidfEmbedding()\n        self._distance = CosineDistance()\n      \n\n    def add_document(self, document: Document) -> None:\n        self.documents.append(document)\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def add_documents(self, documents: List[Document]) -> None:\n        self.documents.extend(documents)\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def get_document(self, id: str) -> Union[Document, None]:\n        for document in self.documents:\n            if document.id == id:\n                return document\n        return None\n\n    def get_all_documents(self) -> List[Document]:\n        return self.documents\n\n    def delete_document(self, id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != id]\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def update_document(self, id: str, updated_document: Document) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == id:\n                self.documents[i] = updated_document\n                break\n\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedding.fit([doc.content for doc in self.documents])\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        documents = [query]\n        documents.extend([doc.content for doc in self.documents])\n        transform_matrix = self._embedder.fit_transform(documents)\n\n        # The inferred vector is the last vector in the transformed_matrix\n        # The rest of the matrix is what we are comparing\n        distances = self._distance.distances(transform_matrix[-1], transform_matrix[:-1])  \n\n        # Get the indices of the top_k most similar (least distant) documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/MlmVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/MlmVectorStore.py\nfrom typing import List, Union, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.embeddings.concrete.MlmEmbedding import MlmEmbedding\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import VectorStoreRetrieveMixin\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import VectorStoreSaveLoadMixin    \n\nclass MlmVectorStore(VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase):\n    type: Literal['MlmVectorStore'] = 'MlmVectorStore'\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._embedder = MlmEmbedding()\n        self._distance = CosineDistance()\n        self.documents: List[Document] = []   \n\n    def add_document(self, document: Document) -> None:\n        self.documents.append(document)\n        documents_text = [_d.content for _d in self.documents if _d.content]\n        embeddings = self._embedder.fit_transform(documents_text)\n\n        embedded_documents = [Document(id=_d.id, \n            content=_d.content, \n            metadata=_d.metadata, \n            embedding=embeddings[_count])\n\n        for _count, _d in enumerate(self.documents) if _d.content]\n\n        self.documents = embedded_documents\n\n    def add_documents(self, documents: List[Document]) -> None:\n        self.documents.extend(documents)\n        documents_text = [_d.content for _d in self.documents if _d.content]\n        embeddings = self._embedder.fit_transform(documents_text)\n\n        embedded_documents = [Document(id=_d.id, \n            content=_d.content, \n            metadata=_d.metadata, \n            embedding=embeddings[_count]) for _count, _d in enumerate(self.documents) \n            if _d.content]\n\n        self.documents = embedded_documents\n\n    def get_document(self, id: str) -> Union[Document, None]:\n        for document in self.documents:\n            if document.id == id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[Document]:\n        return self.documents\n\n    def delete_document(self, id: str) -> None:\n        self.documents = [_d for _d in self.documents if _d.id != id]\n\n    def update_document(self, id: str) -> None:\n        raise NotImplementedError('Update_document not implemented on MLMVectorStore class.')\n        \n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        query_vector = self._embedder.infer_vector(query)\n        document_vectors = [_d.embedding for _d in self.documents if _d.content]\n        distances = self._distance.distances(query_vector, document_vectors)\n        \n        # Get the indices of the top_k most similar documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        \n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/SpatialDocVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/SpatialDocVectorStore.py\nfrom typing import List, Union, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.embeddings.concrete.SpatialDocEmbedding import SpatialDocEmbedding\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import VectorStoreRetrieveMixin\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import VectorStoreSaveLoadMixin    \n\n\nclass SpatialDocVectorStore(VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase):\n    type: Literal['SpatialDocVectorStore'] = 'SpatialDocVectorStore'\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._embedder = SpatialDocEmbedding()  # Assuming this is already implemented\n        self._distance = CosineDistance()\n        self.documents: List[Document] = []\n\n    def add_document(self, document: Document) -> None:\n        self.add_documents([document])  # Reuse the add_documents logic for batch processing\n\n    def add_documents(self, documents: List[Document]) -> None:\n        chunks = [doc.content for doc in documents]\n        # Prepare a list of metadata dictionaries for each document based on the required special tokens\n        metadata_list = [{ \n            'dir': doc.metadata.get('dir', ''),\n            'type': doc.metadata.get('type', ''),\n            'section': doc.metadata.get('section', ''),\n            'path': doc.metadata.get('path', ''),\n            'paragraph': doc.metadata.get('paragraph', ''),\n            'subparagraph': doc.metadata.get('subparagraph', ''),\n            'chapter': doc.metadata.get('chapter', ''), \n            'title': doc.metadata.get('title', ''),\n            'subsection': doc.metadata.get('subsection', ''),\n        } for doc in documents]\n\n        # Use vectorize_document to process all documents with their corresponding metadata\n        embeddings = self._embedder.vectorize_document(chunks, metadata_list=metadata_list)\n        \n        # Create Document instances for each document with the generated embeddings\n        for doc, embedding in zip(documents, embeddings):\n            embedded_doc = Document(\n                id=doc.id, \n                content=doc.content, \n                metadata=doc.metadata, \n                embedding=embedding\n            )\n            self.documents.append(embedded_doc)\n\n    def get_document(self, id: str) -> Union[Document, None]:\n        for document in self.documents:\n            if document.id == id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[Document]:\n        return self.documents\n\n    def delete_document(self, id: str) -> None:\n        self.documents = [_d for _d in self.documents if _d.id != id]\n\n    def update_document(self, id: str) -> None:\n        raise NotImplementedError('Update_document not implemented on SpatialDocVectorStore class.')\n        \n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        query_vector = self._embedder.infer_vector(query)\n        document_vectors = [_d.embedding for _d in self.documents if _d.content]\n        distances = self._distance.distances(query_vector, document_vectors)\n        \n        # Get the indices of the top_k most similar documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        \n        return [self.documents[i] for i in top_k_indices]\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/TfidfVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/TfidfVectorStore.py\nfrom typing import List, Union, Literal\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.embeddings.concrete.TfidfEmbedding import TfidfEmbedding\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import VectorStoreRetrieveMixin\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import VectorStoreSaveLoadMixin    \n\nclass TfidfVectorStore(VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase):\n    type: Literal['TfidfVectorStore'] = 'TfidfVectorStore'\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._embedder = TfidfEmbedding()\n        self._distance = CosineDistance()\n        self.documents = []\n      \n\n    def add_document(self, document: Document) -> None:\n        self.documents.append(document)\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def add_documents(self, documents: List[Document]) -> None:\n        self.documents.extend(documents)\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def get_document(self, id: str) -> Union[Document, None]:\n        for document in self.documents:\n            if document.id == id:\n                return document\n        return None\n\n    def get_all_documents(self) -> List[Document]:\n        return self.documents\n\n    def delete_document(self, id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != id]\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def update_document(self, id: str, updated_document: Document) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == id:\n                self.documents[i] = updated_document\n                break\n\n        # Recalculate TF-IDF matrix for the current set of documents\n        self._embedder.fit([doc.content for doc in self.documents])\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        documents = [query]\n        documents.extend([doc.content for doc in self.documents])\n        transform_matrix = self._embedder.fit_transform(documents)\n        \n        # The inferred vector is the last vector in the transformed_matrix\n        # The rest of the matrix is what we are comparing\n        distances = self._distance.distances(transform_matrix[-1], transform_matrix[:-1])  \n\n        # Get the indices of the top_k most similar (least distant) documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/SqliteVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/SqliteVectorStore.py\nimport json\nimport sqlite3\nfrom typing import List, Optional, Literal, Dict\nimport numpy as np\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import (\n    VectorStoreRetrieveMixin,\n)\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import (\n    VectorStoreSaveLoadMixin,\n)\n\n\nclass SqliteVectorStore(\n    VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase\n):\n    type: Literal[\"SqliteVectorStore\"] = \"SqliteVectorStore\"\n    db_path: str = \"\"\n\n    def __init__(self, db_path: str, **kwargs):\n        super().__init__(**kwargs)\n        self._distance = CosineDistance()\n        self.documents: List[Document] = []\n        self.db_path = db_path\n\n        # Create the SQLite database and table if they don't exist\n        self._create_table()\n\n    def _create_table(self):\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the documents table\n        c.execute(\"\"\"CREATE TABLE IF NOT EXISTS documents\n                     (id TEXT PRIMARY KEY,\n                      content TEXT,\n                      metadata TEXT,\n                      embedding BLOB)\"\"\")\n\n        conn.commit()\n        conn.close()\n\n    def add_document(self, document: Document) -> None:\n        self.documents.append(document)\n        self._insert_document(document)\n\n    def _insert_document(self, document: Document):\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Serialize metadata to JSON\n        metadata_json = json.dumps(document.metadata)\n\n        # Serialize embedding (specifically the value of the Vector)\n        embedding_data = {\n            \"value\": document.embedding.value  # Assuming embedding is of type Vector and has a 'value' attribute\n        }\n        embedding_json = json.dumps(embedding_data)\n\n        # Insert the document into the documents table\n        c.execute(\n            \"INSERT INTO documents (id, content, metadata, embedding) VALUES (?, ?, ?, ?)\",\n            (document.id, document.content, metadata_json, embedding_json),\n        )\n        conn.commit()\n        conn.close()\n\n    def add_documents(self, documents: List[Document]) -> None:\n        self.documents.extend(documents)\n        for document in documents:\n            self._insert_document(document)\n\n    def get_document(self, document_id: str) -> Document:\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        c.execute(\n            \"SELECT id, content, metadata, embedding FROM documents WHERE id = ?\",\n            (document_id,),\n        )\n        row = c.fetchone()\n\n        conn.close()\n\n        if row:\n            document_id, content, metadata_json, embedding_json = row\n            metadata = json.loads(metadata_json)\n            embedding_data = json.loads(embedding_json)\n            embedding = Vector(\n                value=embedding_data[\"value\"]\n            )  # Reconstruct the Vector object\n\n            return Document(\n                id=document_id, content=content, metadata=metadata, embedding=embedding\n            )\n        return None\n\n    def get_all_documents(self) -> List[Document]:\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        c.execute(\"SELECT * FROM documents\")\n        rows = c.fetchall()\n\n        conn.close()\n\n        return [\n            Document(id=row[0], content=row[1], metadata=row[2], embedding=row[3])\n            for row in rows\n        ]\n\n    def delete_document(self, document_id: str) -> None:\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        c.execute(\"DELETE FROM documents WHERE id = ?\", (document_id,))\n\n        conn.commit()\n        conn.close()\n\n    def update_document(self, document: Document) -> None:\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        c.execute(\n            \"\"\"UPDATE documents \n               SET content = ?, metadata = ?, embedding = ? \n               WHERE id = ?\"\"\",\n            (document.content, document.metadata, document.embedding, document.id),\n        )\n\n        conn.commit()\n        conn.close()\n\n    def retrieve(self, query_vector: List[float], top_k: int = 5) -> List[Dict]:\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n        c.execute(\"SELECT id, embedding FROM documents\")\n        rows = c.fetchall()\n        conn.close()\n\n        # Convert query vector to numpy array\n        query_vector = np.array(query_vector, dtype=np.float32)\n\n        results = []\n        for row in rows:\n            doc_id, embedding_json = row\n            embedding = json.loads(embedding_json)\n            vector = np.array(embedding[\"value\"], dtype=np.float32)\n\n            # Compute similarity (e.g., Euclidean distance)\n            distance = np.linalg.norm(query_vector - vector)\n            results.append((doc_id, distance))\n\n        # Sort results by distance\n        results.sort(key=lambda x: x[1])\n\n        # Get top_k results\n        top_results = [doc_id for doc_id, _ in results[:top_k]]\n        return top_results\n\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/__init__.py",
        "content": "```swarmauri/standard/document_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/base/__init__.py",
        "content": "```swarmauri/standard/document_stores/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/base/DocumentStoreBase.py",
        "content": "```swarmauri/standard/document_stores/base/DocumentStoreBase.py\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\n\nclass DocumentStoreBase(IDocumentStore, ABC):\n    \"\"\"\n    Abstract base class for document stores, implementing the IDocumentStore interface.\n\n    This class provides a standard API for adding, updating, getting, and deleting documents in a store.\n    The specifics of storing (e.g., in a database, in-memory, or file system) are to be implemented by concrete subclasses.\n    \"\"\"\n\n    @abstractmethod\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Add a single document to the document store.\n\n        Parameters:\n        - document (IDocument): The document to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n\n        Parameters:\n        - documents (List[IDocument]): A list of documents to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document(self, doc_id: str) -> Optional[IDocument]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to retrieve.\n\n        Returns:\n        - Optional[IDocument]: The requested document if found; otherwise, None.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n\n        Returns:\n        - List[IDocument]: A list of all documents in the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Update a document in the document store.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to update.\n        - updated_document (IDocument): The updated document instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_document(self, doc_id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to delete.\n        \"\"\"\n        pass\n    \n    def document_count(self):\n        return len(self.documents)\n    \n    def dump(self, file_path):\n        with open(file_path, 'w') as f:\n            json.dumps([each.__dict__ for each in self.documents], f, indent=4)\n            \n    def load(self, file_path):\n        with open(file_path, 'r') as f:\n            self.documents = json.loads(f)\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/base/DocumentStoreRetrieveBase.py",
        "content": "```swarmauri/standard/document_stores/base/DocumentStoreRetrieveBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.document_stores.IDocumentRetrieve import IDocumentRetrieve\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.document_stores.base.DocumentStoreBase import DocumentStoreBase\n\nclass DocumentStoreRetrieveBase(DocumentStoreBase, IDocumentRetrieve, ABC):\n\n        \n    @abstractmethod\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        \n        Args:\n            query (str): The query string used for document retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/concrete/__init__.py",
        "content": "```swarmauri/standard/document_stores/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/__init__.py",
        "content": "```swarmauri/standard/chunkers/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/base/__init__.py",
        "content": "```swarmauri/standard/chunkers/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/base/ChunkerBase.py",
        "content": "```swarmauri/standard/chunkers/base/ChunkerBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Union, List, Any, Literal\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\n\nclass ChunkerBase(ComponentBase, ABC):\n    \"\"\"\n    Interface for chunking text into smaller pieces.\n\n    This interface defines abstract methods for chunking texts. Implementing classes\n    should provide concrete implementations for these methods tailored to their specific\n    chunking algorithms.\n    \"\"\"\n    resource: Optional[str] =  Field(default=ResourceTypes.CHUNKER.value)\n    type: Literal['ChunkerBase'] = 'ChunkerBase'\n    \n    @abstractmethod\n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[Any]:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/__init__.py",
        "content": "```swarmauri/standard/chunkers/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/DelimiterBasedChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/DelimiterBasedChunker.py\nfrom typing import List, Union, Any, Literal\nimport re\nfrom swarmauri.standard.chunkers.base.ChunkerBase import ChunkerBase\n\nclass DelimiterBasedChunker(ChunkerBase):\n    \"\"\"\n    A concrete implementation of IChunker that splits text into chunks based on specified delimiters.\n    \"\"\"\n    delimiters: List[str] = ['.', '!', '?']\n    type: Literal['DelimiterBasedChunker'] = 'DelimiterBasedChunker'\n    \n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[str]:\n        \"\"\"\n        Chunks the given text based on the delimiters specified during initialization.\n\n        Parameters:\n        - text (Union[str, Any]): The input text to be chunked.\n\n        Returns:\n        - List[str]: A list of text chunks split based on the specified delimiters.\n        \"\"\"\n        delimiter_pattern = f\"({'|'.join(map(re.escape, self.delimiters))})\"\n        \n        # Split the text based on the delimiter pattern, including the delimiters in the result\n        chunks = re.split(delimiter_pattern, text)\n        \n        # Combine delimiters with the preceding text chunk since re.split() separates them\n        combined_chunks = []\n        for i in range(0, len(chunks), 2):  # Step by 2 to process text chunk with its following delimiter\n            combined_chunks.append(chunks[i] + (chunks[i + 1] if i + 1 < len(chunks) else ''))\n\n        # Remove whitespace\n        combined_chunks = [chunk.strip() for chunk in combined_chunks]\n        return combined_chunks\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/FixedLengthChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/FixedLengthChunker.py\nfrom typing import List, Union, Any, Literal\nfrom swarmauri.standard.chunkers.base.ChunkerBase import ChunkerBase\n\nclass FixedLengthChunker(ChunkerBase):\n    \"\"\"\n    Concrete implementation of ChunkerBase that divides text into fixed-length chunks.\n    \n    This chunker breaks the input text into chunks of a specified size, making sure \n    that each chunk has exactly the number of characters specified by the chunk size, \n    except for possibly the last chunk.\n    \"\"\"\n    chunk_size: int = 256\n    type: Literal['FixedLengthChunker'] = 'FixedLengthChunker'\n    \n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[str]:\n        \"\"\"\n        Splits the input text into fixed-length chunks.\n\n        Parameters:\n        - text (Union[str, Any]): The input text to be chunked.\n        \n        Returns:\n        - List[str]: A list of text chunks, each of a specified fixed length.\n        \"\"\"\n        # Check if the input is a string, if not, attempt to convert to a string.\n        if not isinstance(text, str):\n            text = str(text)\n        \n        # Using list comprehension to split text into chunks of fixed size\n        chunks = [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]\n        \n        return chunks\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/MdSnippetChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/MdSnippetChunker.py\nfrom typing import List, Union, Any, Optional, Literal\nimport re\nfrom swarmauri.standard.chunkers.base.ChunkerBase import ChunkerBase\n\nclass MdSnippetChunker(ChunkerBase):\n    language: Optional[str] = None\n    type: Literal['MdSnippetChunker'] = 'MdSnippetChunker'\n    \n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[tuple]:\n        \"\"\"\n        Extracts paired comments and code blocks from Markdown content based on the \n        specified programming language.\n        \"\"\"\n        if self.language:\n            # If language is specified, directly extract the corresponding blocks\n            pattern = fr'```{self.language}\\s*(.*?)```'\n            scripts = re.findall(pattern, text, re.DOTALL)\n            comments_temp = re.split(pattern, text, flags=re.DOTALL)\n        else:\n            # Extract blocks and languages dynamically if no specific language is provided\n            scripts = []\n            languages = []\n            for match in re.finditer(r'```(\\w+)?\\s*(.*?)```', text, re.DOTALL):\n                if match.group(1) is not None:  # Checks if a language identifier is present\n                    languages.append(match.group(1))\n                    scripts.append(match.group(2))\n                else:\n                    languages.append('')  # Default to an empty string if no language is found\n                    scripts.append(match.group(2))\n            comments_temp = re.split(r'```.*?```', text, flags=re.DOTALL)\n\n        comments = [comment.strip() for comment in comments_temp]\n\n        if text.strip().startswith('```'):\n            comments = [''] + comments\n        if text.strip().endswith('```'):\n            comments.append('')\n\n        if self.language:\n            structured_output = [(comments[i], self.language, scripts[i].strip()) for i in range(len(scripts))]\n        else:\n            structured_output = [(comments[i], languages[i], scripts[i].strip()) for i in range(len(scripts))]\n\n        return structured_output\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/SentenceChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/SentenceChunker.py\nfrom typing import Literal\nimport re\nfrom swarmauri.standard.chunkers.base.ChunkerBase import ChunkerBase\n\nclass SentenceChunker(ChunkerBase):\n    \"\"\"\n    A simple implementation of the ChunkerBase to chunk text into sentences.\n    \n    This class uses basic punctuation marks (., !, and ?) as indicators for sentence boundaries.\n    \"\"\"\n    type: Literal['SentenceChunker'] = 'SentenceChunker'\n    \n    def chunk_text(self, text, *args, **kwargs):\n        \"\"\"\n        Chunks the given text into sentences using basic punctuation.\n\n        Args:\n            text (str): The input text to be chunked into sentences.\n        \n        Returns:\n            List[str]: A list of sentence chunks.\n        \"\"\"\n        # Split text using a simple regex pattern that looks for periods, exclamation marks, or question marks.\n        # Note: This is a very simple approach and might not work well with abbreviations or other edge cases.\n        sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s'\n        sentences = re.split(sentence_pattern, text)\n        \n        # Filter out any empty strings that may have resulted from the split operation\n        sentences = [sentence.strip() for sentence in sentences if sentence]\n        \n        return sentences\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/SlidingWindowChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/SlidingWindowChunker.py\nfrom typing import List, Literal\nfrom swarmauri.standard.chunkers.base.ChunkerBase import ChunkerBase\n\n\nclass SlidingWindowChunker(ChunkerBase):\n    \"\"\"\n    A concrete implementation of ChunkerBase that uses sliding window technique\n    to break the text into chunks.\n    \"\"\"\n    window_size: int = 256\n    step_size: int = 256\n    overlap: bool = False\n    type: Literal['SlidingWindowChunker'] = 'SlidingWindowChunker'\n         \n    def chunk_text(self, text: str, *args, **kwargs) -> List[str]:\n        \"\"\"\n        Splits the input text into chunks based on the sliding window technique.\n        \n        Parameters:\n        - text (str): The input text to be chunked.\n        \n        Returns:\n        - List[str]: A list of text chunks.\n        \"\"\"\n        step_size = self.step_size if self.overlap else self.window_size  # Non-overlapping if window size equals step size.\n\n\n        words = text.split()  # Tokenization by whitespaces. More sophisticated tokenization might be necessary.\n        chunks = []\n        \n        for i in range(0, len(words) - self.window_size + 1, step_size):\n            chunk = ' '.join(words[i:i+self.window_size])\n            chunks.append(chunk)\n        \n        return chunks\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/__init__.py",
        "content": "```swarmauri/standard/vectors/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/base/__init__.py",
        "content": "```swarmauri/standard/vectors/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/base/VectorBase.py",
        "content": "```swarmauri/standard/vectors/base/VectorBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Literal\nimport json\nimport numpy as np\nfrom pydantic import Field\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\n\nclass VectorBase(IVector, ComponentBase):\n    value: List[float]\n    resource: Optional[str] =  Field(default=ResourceTypes.VECTOR.value, frozen=True)\n    type: Literal['VectorBase'] = 'VectorBase'\n\n    def to_numpy(self) -> np.ndarray:\n        \"\"\"\n        Converts the vector into a numpy array.\n\n        Returns:\n            np.ndarray: The numpy array representation of the vector.\n        \"\"\"\n        return np.array(self.value)\n\n    @property\n    def shape(self):\n        return self.to_numpy().shape\n        \n    def __len__(self):\n        return len(self.value)\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/concrete/__init__.py",
        "content": "```swarmauri/standard/vectors/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/concrete/Vector.py",
        "content": "```swarmauri/standard/vectors/concrete/Vector.py\nfrom typing import Literal\nfrom swarmauri.standard.vectors.base.VectorBase import VectorBase\n\nclass Vector(VectorBase):\n    type: Literal['Vector'] = 'Vector'\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/concrete/VectorProductMixin.py",
        "content": "```swarmauri/standard/vectors/concrete/VectorProductMixin.py\nimport numpy as np\nfrom typing import List\nfrom pydantic import BaseModel\nfrom swarmauri.core.vectors.IVectorProduct import IVectorProduct\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\nclass VectorProductMixin(IVectorProduct, BaseModel):\n    def dot_product(self, vector_a: Vector, vector_b: Vector) -> float:\n        a = np.array(vector_a.value).flatten()\n        b = np.array(vector_b.value).flatten()\n        return np.dot(a, b)\n    \n    def cross_product(self, vector_a: Vector, vector_b: Vector) -> Vector:\n        if len(vector_a.value) != 3 or len(vector_b.value) != 3:\n            raise ValueError(\"Cross product is only defined for 3-dimensional vectors\")\n        a = np.array(vector_a.value)\n        b = np.array(vector_b.value)\n        cross = np.cross(a, b)\n        return Vector(value=cross.tolist())\n    \n    def vector_triple_product(self, vector_a: Vector, vector_b: Vector, vector_c: Vector) -> Vector:\n        a = np.array(vector_a.value)\n        b = np.array(vector_b.value)\n        c = np.array(vector_c.value)\n        result = np.cross(a, np.cross(b, c))\n        return Vector(value=result.tolist())\n    \n    def scalar_triple_product(self, vector_a: Vector, vector_b: Vector, vector_c: Vector) -> float:\n        a = np.array(vector_a.value)\n        b = np.array(vector_b.value)\n        c = np.array(vector_c.value)\n        return np.dot(a, np.cross(b, c))\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/__init__.py",
        "content": "```swarmauri/standard/tracing/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/base/__init__.py",
        "content": "```swarmauri/standard/tracing/base/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/SimpleTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/SimpleTracer.py\nfrom datetime import datetime\nimport uuid\nfrom typing import Dict, Any, Optional\n\nfrom swarmauri.core.tracing.ITracer import ITracer\nfrom swarmauri.standard.tracing.concrete.SimpleTraceContext import SimpleTraceContext\n\nclass SimpleTracer(ITracer):\n    _instance = None  # Singleton instance placeholder\n\n    @classmethod\n    def instance(cls):\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n\n    def __init__(self):\n        if SimpleTracer._instance is not None:\n            raise RuntimeError(\"SimpleTracer is a singleton. Use SimpleTracer.instance().\")\n        self.trace_stack = []\n\n    def start_trace(self, name: str, initial_attributes: Optional[Dict[str, Any]] = None) -> SimpleTraceContext:\n        trace_id = str(uuid.uuid4())\n        trace_context = SimpleTraceContext(trace_id, name, initial_attributes)\n        self.trace_stack.append(trace_context)\n        return trace_context\n\n    def end_trace(self):\n        if self.trace_stack:\n            completed_trace = self.trace_stack.pop()\n            completed_trace.close()\n            # Example of simply printing the completed trace; in practice, you might log it or store it elsewhere\n            print(f\"Trace Completed: {completed_trace.name}, Duration: {completed_trace.start_time} to {completed_trace.end_time}, Attributes: {completed_trace.attributes}\")\n\n    def annotate_trace(self, key: str, value: Any):\n        if not self.trace_stack:\n            print(\"No active trace to annotate.\")\n            return\n        current_trace = self.trace_stack[-1]\n        current_trace.add_attribute(key, value)\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/TracedVariable.py",
        "content": "```swarmauri/standard/tracing/concrete/TracedVariable.py\nfrom typing import Any\nfrom swarmauri.standard.tracing.concrete.SimpleTracer import SimpleTracer  # Assuming this is the path to the tracer\n\nclass TracedVariable:\n    \"\"\"\n    Wrapper class to trace multiple changes to a variable within the context manager.\n    \"\"\"\n    def __init__(self, name: str, value: Any, tracer: SimpleTracer):\n        self.name = name\n        self._value = value\n        self._tracer = tracer\n        self._changes = []  # Initialize an empty list to track changes\n\n    @property\n    def value(self) -> Any:\n        return self._value\n\n    @value.setter\n    def value(self, new_value: Any):\n        # Record the change before updating the variable's value\n        change_annotation = {\"from\": self._value, \"to\": new_value}\n        self._changes.append(change_annotation)\n        \n        # Update the trace by appending the latest change to the list under a single key\n        # Note that the key is now constant and does not change with each update\n        self._tracer.annotate_trace(key=f\"{self.name}_changes\", value=self._changes)\n        \n        self._value = new_value\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/ChainTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/ChainTracer.py\nfrom swarmauri.core.tracing.IChainTracer import IChainTracer\nfrom typing import Callable, List, Tuple, Dict, Any   \n        \nclass ChainTracer(IChainTracer):\n    def __init__(self):\n        self.traces = []\n\n    def process_chain(self, chain: List[Tuple[Callable[..., Any], List[Any], Dict[str, Any]]]) -> \"ChainTracer\":\n        \"\"\"\n        Processes each item in the operation chain by executing the specified external function\n        with its args and kwargs. Logs starting, annotating, and ending the trace based on tuple position.\n\n        Args:\n            chain (List[Tuple[Callable[..., Any], List[Any], Dict[str, Any]]]): A list where each tuple contains:\n                - An external function to execute.\n                - A list of positional arguments for the function.\n                - A dictionary of keyword arguments for the function.\n        \"\"\"\n        for i, (func, args, kwargs) in enumerate(chain):\n            # Infer operation type and log\n            \n            if i == 0:\n                operation = \"Start\"\n                self.start_trace(*args, **kwargs)\n            elif i == len(chain) - 1:\n                operation = \"End\"\n                self.end_trace(*args, **kwargs)\n            else:\n                operation = \"Annotate\"\n                self.annotate_trace(*args, **kwargs)\n                \n            # For the actual external function call\n            result = func(*args, **kwargs)\n            print(f\"Function '{func.__name__}' executed with result: {result}\")\n\n            self.traces.append((operation, func, args, kwargs, result))\n\n        return self\n\n    def start_trace(self, *args, **kwargs) -> None:\n        print(f\"Starting trace with args: {args}, kwargs: {kwargs}\")\n        \n    def annotate_trace(self, *args, **kwargs) -> None:\n        print(f\"Annotating trace with args: {args}, kwargs: {kwargs}\")\n\n    def end_trace(self, *args, **kwargs) -> None:\n        print(f\"Ending trace with args: {args}, kwargs: {kwargs}\")\n\n    def show(self) -> None:\n        for entry in self.traces:\n            print(entry)\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/SimpleTraceContext.py",
        "content": "```swarmauri/standard/tracing/concrete/SimpleTraceContext.py\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\n\nfrom swarmauri.core.tracing.ITraceContext import ITraceContext\n\nclass SimpleTraceContext(ITraceContext):\n    def __init__(self, trace_id: str, name: str, initial_attributes: Optional[Dict[str, Any]] = None):\n        self.trace_id = trace_id\n        self.name = name\n        self.attributes = initial_attributes if initial_attributes else {}\n        self.start_time = datetime.now()\n        self.end_time = None\n\n    def get_trace_id(self) -> str:\n        return self.trace_id\n\n    def add_attribute(self, key: str, value: Any):\n        self.attributes[key] = value\n\n    def close(self):\n        self.end_time = datetime.now()\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/VariableTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/VariableTracer.py\nfrom contextlib import contextmanager\n\nfrom swarmauri.standard.tracing.concrete.TracedVariable import TracedVariable\nfrom swarmauri.standard.tracing.concrete.SimpleTracer import SimpleTracer\n\n# Initialize a global instance of SimpleTracer for use across the application\nglobal_tracer = SimpleTracer()\n\n@contextmanager\ndef VariableTracer(name: str, initial_value=None):\n    \"\"\"\n    Context manager for tracing the declaration, modification, and usage of a variable.\n    \"\"\"\n    trace_context = global_tracer.start_trace(name=f\"Variable: {name}\", initial_attributes={\"initial_value\": initial_value})\n    traced_variable = TracedVariable(name, initial_value, global_tracer)\n    \n    try:\n        yield traced_variable\n    finally:\n        # Optionally record any final value or state of the variable before it goes out of scope\n        global_tracer.annotate_trace(key=f\"{name}_final\", value={\"final_value\": traced_variable.value})\n        # End the trace, marking the variable's lifecycle\n        global_tracer.end_trace()\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/CallableTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/CallableTracer.py\nimport functools\nfrom swarmauri.standard.tracing.concrete.SimpleTracer import SimpleTracer  # Import SimpleTracer from the previously defined path\n\n# Initialize the global tracer object\ntracer = SimpleTracer()\n\ndef CallableTracer(func):\n    \"\"\"\n    A decorator to trace function or method calls, capturing inputs, outputs, and the caller.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Trying to automatically identify the caller details; practical implementations \n        # might need to adjust based on specific requirements or environment.\n        caller_info = f\"{func.__module__}.{func.__name__}\"\n        \n        # Start a new trace context for this callable\n        trace_context = tracer.start_trace(name=caller_info, initial_attributes={'args': args, 'kwargs': kwargs})\n        \n        try:\n            # Call the actual function/method\n            result = func(*args, **kwargs)\n            tracer.annotate_trace(key=\"result\", value=result)\n            return result\n        except Exception as e:\n            # Optionally annotate the trace with the exception details\n            tracer.annotate_trace(key=\"exception\", value=str(e))\n            raise  # Re-raise the exception to not interfere with the program's flow\n        finally:\n            # End the trace after the function call is complete\n            tracer.end_trace()\n    return wrapper\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/__init__.py",
        "content": "```swarmauri/standard/tracing/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/__init__.py",
        "content": "```swarmauri/standard/chains/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/__init__.py",
        "content": "```swarmauri/standard/chains/base/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/ChainBase.py",
        "content": "```swarmauri/standard/chains/base/ChainBase.py\nfrom typing import List, Dict, Any, Optional, Literal\nfrom pydantic import Field, ConfigDict\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.chains.IChain import IChain\nfrom swarmauri.stanard.chains.concrete.ChainStep import ChainStep\nfrom swarmauri.core.typing import SubclassUnion\n\nclass ChainBase(IChain, ComponentBase):\n    \"\"\"\n    A base implementation of the IChain interface.\n    \"\"\"\n    steps: List[ChainStep] = []\n    resource: Optional[str] =  Field(default=ResourceTypes.CHAIN.value)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['ChainBase'] = 'ChainBase'\n\n    def add_step(self, step: ChainStep) -> None:\n        self.steps.append(step)\n\n    def remove_step(self, step: ChainStep) -> None:\n        \"\"\"\n        Removes an existing step from the chain. This alters the chain's execution sequence\n        by excluding the specified step from subsequent executions of the chain.\n\n        Parameters:\n            step (ChainStep): The Callable representing the step to remove from the chain.\n        \"\"\"\n\n        raise NotImplementedError('This is not yet implemented')\n\n    def execute(self, *args, **kwargs) -> Any:\n        raise NotImplementedError('This is not yet implemented')\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/ChainContextBase.py",
        "content": "```swarmauri/standard/chains/base/ChainContextBase.py\nfrom typing import Any, Callable, Dict, List, Optional, Literal\nfrom pydantic import Field, ConfigDict\nimport re\nfrom swarmauri.standard.chains.concrete.ChainStep import ChainStep\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.chains.IChainContext import IChainContext\n\nclass ChainContextBase(IChainContext, ComponentBase):\n    steps: List[ChainStep] = []\n    context: Dict = {}\n    resource: Optional[str] =  Field(default=ResourceTypes.CHAIN.value)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['ChainContextBase'] = 'ChainContextBase'\n\n    def update(self, **kwargs):\n        self.context.update(kwargs)\n\n    def get_value(self, key: str) -> Any:\n        return self.context.get(key)\n\n    def _resolve_fstring(self, template: str) -> str:\n        pattern = re.compile(r'{([^}]+)}')\n        def replacer(match):\n            expression = match.group(1)\n            try:\n                return str(eval(expression, {}, self.context))\n            except Exception as e:\n                print(f\"Failed to resolve expression: {expression}. Error: {e}\")\n                return f\"{{{expression}}}\"\n        return pattern.sub(replacer, template)\n\n    def _resolve_placeholders(self, value: Any) -> Any:\n        if isinstance(value, str):\n            return self._resolve_fstring(value)\n        elif isinstance(value, dict):\n            return {k: self._resolve_placeholders(v) for k, v in value.items()}\n        elif isinstance(value, list):\n            return [self._resolve_placeholders(v) for v in value]\n        else:\n            return value\n\n    def _resolve_ref(self, value: Any) -> Any:\n        if isinstance(value, str) and value.startswith('$'):\n            placeholder = value[1:]\n            return placeholder\n        return value\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/ChainStepBase.py",
        "content": "```swarmauri/standard/chains/base/ChainStepBase.py\nfrom typing import Any, Tuple, Dict, Optional, Union, Literal\nfrom pydantic import Field, ConfigDict\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainStepBase(IChainStep, ComponentBase):\n    \"\"\"\n    Represents a single step within an execution chain.\n    \"\"\"\n    key: str\n    method: SubclassUnion[ToolBase]\n    args: Tuple = Field(default_factory=tuple)\n    kwargs: Dict[str, Any] = Field(default_factory=dict)\n    ref: Optional[str] =  Field(default=None)\n    resource: Optional[str] =  Field(default=ResourceTypes.CHAINSTEP.value)\n    type: Literal['ChainStepBase'] = 'ChainStepBase'\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/PromptContextChainBase.py",
        "content": "```swarmauri/standard/chains/base/PromptContextChainBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional, Literal\nfrom pydantic import Field\nfrom collections import defaultdict, deque\nimport re\nimport numpy as np\n\n\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.standard.chains.concrete.ChainStep import ChainStep\nfrom swarmauri.standard.chains.base.ChainContextBase import ChainContextBase\nfrom swarmauri.standard.prompts.concrete.PromptMatrix import PromptMatrix\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\nfrom swarmauri.core.prompts.IPromptMatrix import IPromptMatrix\nfrom swarmauri.core.chains.IChainDependencyResolver import IChainDependencyResolver\n\nclass PromptContextChainBase(IChainDependencyResolver, ChainContextBase, ComponentBase):\n    prompt_matrix: PromptMatrix\n    agents: List[SubclassUnion[AgentBase]] = Field(default_factory=list)\n    context: Dict[str, Any] = Field(default_factory=dict)\n    llm_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    response_matrix: Optional[PromptMatrix] = None\n    current_step_index: int = 0\n    steps: List[Any] = Field(default_factory=list)\n    resource: Optional[str] =  Field(default=ResourceTypes.CHAIN.value)\n    type: Literal['PromptContextChainBase'] = 'PromptContextChainBase'\n    \n    def __init__(self, **data: Any):\n        super().__init__(**data)\n        # Now that the instance is created, we can safely access `prompt_matrix.shape`\n        self.response_matrix = PromptMatrix(matrix=[[None for _ in range(self.prompt_matrix.shape[1])] \n                                                    for _ in range(self.prompt_matrix.shape[0])])\n\n    def execute(self, build_dependencies=True) -> None:\n        \"\"\"\n        Execute the chain of prompts based on the state of the prompt matrix.\n        Iterates through each sequence in the prompt matrix, resolves dependencies, \n        and executes prompts in the resolved order.\n        \"\"\"\n        if build_dependencies:\n            self.steps = self.build_dependencies()\n            self.current_step_index = 0\n\n        while self.current_step_index < len(self.steps):\n            step = self.steps[self.current_step_index]\n            method = step.method\n            args = step.args\n            ref = step.ref\n            result = method(*args)\n            self.context[ref] = result\n            prompt_index = self._extract_step_number(ref)\n            self._update_response_matrix(args[0], prompt_index, result)\n            self.current_step_index += 1  # Move to the next step\n        else:\n            print(\"All steps have been executed.\")\n\n    def execute_next_step(self):\n        \"\"\"\n        Execute the next step in the steps list if available.\n        \"\"\"\n        if self.current_step_index < len(self.steps):\n            step = self.steps[self.current_step_index]\n            method = step.method\n            args = step.args\n            ref = step.ref\n            result = method(*args)\n            self.context[ref] = result\n            prompt_index = self._extract_step_number(ref)\n            self._update_response_matrix(args[0], prompt_index, result)\n            self.current_step_index += 1  # Move to the next step\n        else:\n            print(\"All steps have been executed.\")\n\n    def _execute_prompt(self, agent_index: int, prompt: str, ref: str):\n        \"\"\"\n        Executes a given prompt using the specified agent and updates the response.\n        \"\"\"\n        formatted_prompt = prompt.format(**self.context)  # Using context for f-string formatting\n        \n        agent = self.agents[agent_index]\n        # get the unformatted version\n        unformatted_system_context = agent.system_context\n        # use the formatted version\n        agent.system_context = agent.system_context.content.format(**self.context)\n        response = agent.exec(formatted_prompt, model_kwargs=self.model_kwargs)\n        # reset back to the unformatted version\n        agent.system_context = unformatted_system_context\n\n        self.context[ref] = response\n        prompt_index = self._extract_step_number(ref)\n        self._update_response_matrix(agent_index, prompt_index, response)\n        return response\n\n    def _update_response_matrix(self, agent_index: int, prompt_index: int, response: Any):\n        self.response_matrix.matrix[agent_index][prompt_index] = response\n\n\n    def _extract_agent_number(self, text):\n        # Regular expression to match the pattern and capture the agent number\n        match = re.search(r'\\{Agent_(\\d+)_Step_\\d+_response\\}', text)\n        if match:\n            # Return the captured group, which is the agent number\n            return int(match.group(1))\n        else:\n            # Return None if no match is found\n            return None\n    \n    def _extract_step_number(self, ref):\n        # This regex looks for the pattern '_Step_' followed by one or more digits.\n        match = re.search(r\"_Step_(\\d+)_\", ref)\n        if match:\n            return int(match.group(1))  # Convert the extracted digits to an integer\n        else:\n            return None  # If no match is found, return None\n    \n    def build_dependencies(self) -> List[ChainStep]:\n        \"\"\"\n        Build the chain steps in the correct order by resolving dependencies first.\n        \"\"\"\n        steps = []\n        \n        for i in range(self.prompt_matrix.shape[1]):\n            try:\n                sequence = np.array(self.prompt_matrix.matrix)[:,i].tolist()\n                execution_order = self.resolve_dependencies(sequence=sequence)\n                for j in execution_order:\n                    prompt = sequence[j]\n                    if prompt:\n                        ref = f\"Agent_{j}_Step_{i}_response\"  # Using a unique reference string\n                        step = ChainStep(\n                            key=f\"Agent_{j}_Step_{i}\",\n                            method=self._execute_prompt,\n                            args=[j, prompt, ref],\n                            ref=ref\n                        )\n                        steps.append(step)\n            except Exception as e:\n                print(str(e))\n        return steps\n\n    def resolve_dependencies(self, sequence: List[Optional[str]]) -> List[int]:\n        \"\"\"\n        Resolve dependencies within a specific sequence of the prompt matrix.\n        \n        Args:\n            matrix (List[List[Optional[str]]]): The prompt matrix.\n            sequence_index (int): The index of the sequence to resolve dependencies for.\n\n        Returns:\n            List[int]: The execution order of the agents for the given sequence.\n        \"\"\"\n        \n        return [x for x in range(0, len(sequence), 1)]\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/__init__.py",
        "content": "```swarmauri/standard/chains/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/CallableChain.py",
        "content": "```swarmauri/standard/chains/concrete/CallableChain.py\nfrom typing import Any, Callable, List, Dict, Optional\nfrom swarmauri.core.chains.ICallableChain import ICallableChain, CallableDefinition\n\n\nclass CallableChain(ICallableChain):\n    \n    def __init__(self, callables: Optional[List[CallableDefinition]] = None):\n        \n        self.callables = callables if callables is not None else []\n\n    def __call__(self, *initial_args, **initial_kwargs):\n        result = None\n        for func, args, kwargs in self.callables:\n            if result is not None:\n                # If there was a previous result, use it as the first argument for the next function\n                args = [result] + list(args)\n            result = func(*args, **kwargs)\n        return result\n    \n    def add_callable(self, func: Callable[[Any], Any], args: List[Any] = None, kwargs: Dict[str, Any] = None) -> None:\n        # Add a new callable to the chain\n        self.callables.append((func, args or [], kwargs or {}))\n    \n    def __or__(self, other: \"CallableChain\") -> \"CallableChain\":\n        if not isinstance(other, CallableChain):\n            raise TypeError(\"Operand must be an instance of CallableChain\")\n        \n        new_chain = CallableChain(self.callables + other.callables)\n        return new_chain\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/ChainStep.py",
        "content": "```swarmauri/standard/chains/concrete/ChainStep.py\nfrom typing import Literal\nfrom swarmauri.standard.chains.base.ChainStepBase import ChainStepBase\n\nclass ChainStep(ChainStepBase):\n    \"\"\"\n    Represents a single step within an execution chain.\n    \"\"\"\n    type: Literal['ChainStep'] = 'ChainStep'\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/ContextChain.py",
        "content": "```swarmauri/standard/chains/concrete/ContextChain.py\nfrom typing import Any, Dict, List, Callable, Optional, Tuple, Union, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.chains.concrete.ChainStep import ChainStep\nfrom swarmauri.standard.chains.base.ChainContextBase import ChainContextBase\nfrom swarmauri.core.chains.IChain import IChain\n\nclass ContextChain(IChain, ChainContextBase):\n    \"\"\"\n    Enhanced to support ChainSteps with return parameters, storing return values as instance state variables.\n    Implements the IChain interface including get_schema_info and remove_step methods.\n    \"\"\"\n    type: Literal['ContextChain'] = 'ContextChain'\n\n    def add_step(self, \n        key: str, \n        method: SubclassUnion[ToolBase],\n        args: Tuple = (), \n        kwargs: Dict[str, Any] = {}, \n        ref: Optional[str] = None):\n\n        # Directly store args, kwargs, and optionally a return_key without resolving them\n        step = ChainStep(key=key, method=method, args=args, kwargs=kwargs, ref=ref)\n        self.steps.append(step)\n\n    def remove_step(self, step: ChainStep) -> None:\n        self.steps = [s for s in self._steps if s.key != step.key]\n\n    def execute(self, *args, **kwargs) -> Any:\n        # Execute the chain and manage result storage based on return_key\n        for step in self.steps:\n            resolved_args = [self._resolve_placeholders(arg) for arg in step.args]\n            resolved_kwargs = {k: self._resolve_placeholders(v) for k, v in step.kwargs.items() if k != 'ref'}\n            result = step.method(*resolved_args, **resolved_kwargs)\n            if step.ref:  # step.ref is used here as the return_key analogy\n                resolved_ref = self._resolve_ref(step.ref)\n                self.context[resolved_ref] = result\n                self.update(**{resolved_ref: result})  # Update context with new state value\n        return self.context  # or any specific result you intend to return\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/PromptContextChain.py",
        "content": "```swarmauri/standard/chains/concrete/PromptContextChain.py\nfrom typing import Literal\nfrom swarmauri.standard.chains.base.PromptContextChainBase import PromptContextChainBase\n\nclass PromptContextChain(PromptContextChainBase):\n    type: Literal['PromptContextChain'] = 'PromptContextChain'\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/__init__.py",
        "content": "```swarmauri/standard/distances/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/base/__init__.py",
        "content": "```swarmauri/standard/distances/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/base/DistanceBase.py",
        "content": "```swarmauri/standard/distances/base/DistanceBase.py\nfrom abc import abstractmethod\nfrom numpy.linalg import norm\nfrom typing import List, Optional, Literal\nfrom pydantic import Field\nfrom swarmauri.core.distances.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.vectors.concrete.VectorProductMixin import VectorProductMixin\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\n\nclass DistanceBase(IDistanceSimilarity, VectorProductMixin, ComponentBase):\n    \"\"\"\n    Implements cosine distance calculation as an IDistanceSimiliarity interface.\n    Cosine distance measures the cosine of the angle between two non-zero vectors\n    of an inner product space, capturing the orientation rather than the magnitude \n    of these vectors.\n    \"\"\"\n    resource: Optional[str] =  Field(default=ResourceTypes.DISTANCE.value, frozen=True)\n    type: Literal['DistanceBase'] = 'DistanceBase'\n    @abstractmethod\n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        pass\n    \n    @abstractmethod\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        pass\n       \n\n    @abstractmethod\n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        pass\n        \n    @abstractmethod\n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        pass\n        \n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/__init__.py",
        "content": "```swarmauri/standard/distances/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/CanberraDistance.py",
        "content": "```swarmauri/standard/distances/concrete/CanberraDistance.py\nimport numpy as np\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\n\nclass CanberraDistance(DistanceBase):\n    \"\"\"\n    Concrete implementation of the IDistanceSimiliarity interface using the Canberra distance metric.\n    This class now processes Vector instances instead of raw lists.\n    \"\"\"\n    type: Literal['CanberraDistance'] = 'CanberraDistance'   \n\n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Canberra distance between two Vector instances.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Canberra distance between the vectors.\n        \"\"\"\n        # Extract data from Vector\n        data_a = np.array(vector_a.value)\n        data_b = np.array(vector_b.value)\n\n        # Checking dimensions match\n        if data_a.shape != data_b.shape:\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n\n        # Computing Canberra distance\n        distance = np.sum(np.abs(data_a - data_b) / (np.abs(data_a) + np.abs(data_b)))\n        # Handling the case where both vectors have a zero value for the same dimension\n        distance = np.nan_to_num(distance)\n        return distance\n    \n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Compute similarity using the Canberra distance. Since this distance metric isn't\n        directly interpretable as a similarity, a transformation is applied to map the distance\n        to a similarity score.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector to compare with the first vector.\n\n        Returns:\n            float: A similarity score between vector_a and vector_b.\n        \"\"\"\n        # One way to derive a similarity from distance is through inversion or transformation.\n        # Here we use an exponential decay based on the computed distance. This is a placeholder\n        # that assumes closer vectors (smaller distance) are more similar.\n        distance = self.distance(vector_a, vector_b)\n\n        # Transform the distance into a similarity score\n        similarity = np.exp(-distance)\n\n        return similarity\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/ChebyshevDistance.py",
        "content": "```swarmauri/standard/distances/concrete/ChebyshevDistance.py\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass ChebyshevDistance(DistanceBase):\n    \"\"\"\n    Concrete implementation of the IDistanceSimiliarity interface using the Chebyshev distance metric.\n    Chebyshev distance is the maximum absolute distance between two vectors' elements.\n    \"\"\"\n    type: Literal['ChebyshevDistance'] = 'ChebyshevDistance'   \n\n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Chebyshev distance between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Chebyshev distance between vector_a and vector_b.\n        \"\"\"\n        max_distance = 0\n        for a, b in zip(vector_a.value, vector_b.value):\n            max_distance = max(max_distance, abs(a - b))\n        return max_distance\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the similarity between two vectors based on the Chebyshev distance.\n\n        Args:\n            vector_a (Vector): The first vector.\n            vector_b (Vector): The second vector.\n\n        Returns:\n            float: The similarity score between the two vectors.\n        \"\"\"\n\n        return 1 / (1 + self.distance(vector_a, vector_b))\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/ChiSquaredDistance.py",
        "content": "```swarmauri/standard/distances/concrete/ChiSquaredDistance.py\nfrom typing import List, Literal\n\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass ChiSquaredDistance(DistanceBase):\n    \"\"\"\n    Implementation of the IDistanceSimilarity interface using Chi-squared distance metric.\n    \"\"\"    \n    type: Literal['ChiSquaredDistance'] = 'ChiSquaredDistance'\n\n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Chi-squared distance between two vectors.\n        \"\"\"\n        if len(vector_a.value) != len(vector_b.value):\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n\n        chi_squared_distance = 0\n        for a, b in zip(vector_a.value, vector_b.value):\n            if (a + b) != 0:\n                chi_squared_distance += (a - b) ** 2 / (a + b)\n\n        return 0.5 * chi_squared_distance\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Compute the similarity between two vectors based on the Chi-squared distance.\n        \"\"\"\n        return 1 / (1 + self.distance(vector_a, vector_b))\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/CosineDistance.py",
        "content": "```swarmauri/standard/distances/concrete/CosineDistance.py\nfrom numpy.linalg import norm\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass CosineDistance(DistanceBase):\n    \"\"\"\n    Implements cosine distance calculation as an IDistanceSimiliarity interface.\n    Cosine distance measures the cosine of the angle between two non-zero vectors\n    of an inner product space, capturing the orientation rather than the magnitude \n    of these vectors.\n    \"\"\"\n    type: Literal['CosineDistance'] = 'CosineDistance'   \n       \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\" \n        Computes the cosine distance between two vectors: 1 - cosine similarity.\n    \n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n    \n        Returns:\n            float: The computed cosine distance between vector_a and vector_b.\n                   It ranges from 0 (completely similar) to 2 (completely dissimilar).\n        \"\"\"\n        norm_a = norm(vector_a.value)\n        norm_b = norm(vector_b.value)\n    \n        # Check if either of the vector norms is close to zero\n        if norm_a < 1e-10 or norm_b < 1e-10:\n            return 1.0  # Return maximum distance for cosine which varies between -1 to 1, so 1 indicates complete dissimilarity\n    \n        # Compute the cosine similarity between the vectors\n        cos_sim = self.dot_product(vector_a, vector_b) / (norm_a * norm_b)\n    \n        # Covert cosine similarity to cosine distance\n        cos_distance = 1 - cos_sim\n    \n        return cos_distance\n    \n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the cosine similarity between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The cosine similarity between vector_a and vector_b.\n        \"\"\"\n        return 1 - self.distance(vector_a, vector_b)\n\n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/EuclideanDistance.py",
        "content": "```swarmauri/standard/distances/concrete/EuclideanDistance.py\nfrom math import sqrt\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass EuclideanDistance(DistanceBase):\n    \"\"\"\n    Class to compute the Euclidean distance between two vectors.\n    Implements the IDistanceSimiliarity interface.\n    \"\"\"    \n    type: Literal['EuclideanDistance'] = 'EuclideanDistance'\n    \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Euclidean distance between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Euclidean distance between vector_a and vector_b.\n        \"\"\"\n        if len(vector_a.value) != len(vector_b.value):\n            raise ValueError(\"Vectors do not have the same dimensionality.\")\n        \n        distance = sqrt(sum((a - b) ** 2 for a, b in zip(vector_a.value, vector_b.value)))\n        return distance\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the similarity score as the inverse of the Euclidean distance between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The similarity score between vector_a and vector_b.\n        \"\"\"\n        distance = self.distance(vector_a, vector_b)\n        return 1 / (1 + distance)\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/HaversineDistance.py",
        "content": "```swarmauri/standard/distances/concrete/HaversineDistance.py\nfrom typing import List, Literal\nfrom math import radians, cos, sin, sqrt, atan2\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\n\nclass HaversineDistance(DistanceBase):\n    \"\"\"\n    Concrete implementation of IDistanceSimiliarity interface using the Haversine formula.\n    \n    Haversine formula determines the great-circle distance between two points on a sphere given their \n    longitudes and latitudes. This implementation is particularly useful for geo-spatial data.\n    \"\"\" \n    type: Literal['HaversineDistance'] = 'HaversineDistance'   \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Haversine distance between two geo-spatial points.\n\n        Args:\n            vector_a (Vector): The first point in the format [latitude, longitude].\n            vector_b (Vector): The second point in the same format [latitude, longitude].\n\n        Returns:\n            float: The Haversine distance between vector_a and vector_b in kilometers.\n        \"\"\"\n        # Earth radius in kilometers\n        R = 6371.0\n\n        lat1, lon1 = map(radians, vector_a.value)\n        lat2, lon2 = map(radians, vector_b.value)\n\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n\n        # Haversine formula\n        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n        distance = R * c\n\n        return distance\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        raise NotImplementedError(\"Similarity not implemented for Haversine distance.\")\n        \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        raise NotImplementedError(\"Similarity not implemented for Haversine distance.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/JaccardIndexDistance.py",
        "content": "```swarmauri/standard/distances/concrete/JaccardIndexDistance.py\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\n\nclass JaccardIndexDistance(DistanceBase):\n    \"\"\"\n    A class implementing Jaccard Index as a similarity and distance metric between two vectors.\n    \"\"\"    \n    type: Literal['JaccardIndexDistance'] = 'JaccardIndexDistance'\n    \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Jaccard distance between two vectors.\n\n        The Jaccard distance, which is 1 minus the Jaccard similarity,\n        measures dissimilarity between sample sets. It's defined as\n        1 - (the intersection of the sets divided by the union of the sets).\n\n        Args:\n            vector_a (Vector): The first vector.\n            vector_b (Vector): The second vector.\n\n        Returns:\n            float: The Jaccard distance between vector_a and vector_b.\n        \"\"\"\n        set_a = set(vector_a.value)\n        set_b = set(vector_b.value)\n\n        # Calculate the intersection and union of the two sets.\n        intersection = len(set_a.intersection(set_b))\n        union = len(set_a.union(set_b))\n\n        # In the special case where the union is zero, return 1.0 which implies complete dissimilarity.\n        if union == 0:\n            return 1.0\n\n        # Compute Jaccard similarity and then return the distance as 1 - similarity.\n        jaccard_similarity = intersection / union\n        return 1 - jaccard_similarity\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Jaccard similarity between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector.\n            vector_b (Vector): The second vector.\n\n        Returns:\n            float: Jaccard similarity score between vector_a and vector_b.\n        \"\"\"\n        set_a = set(vector_a.value)\n        set_b = set(vector_b.value)\n\n        # Calculate the intersection and union of the two sets.\n        intersection = len(set_a.intersection(set_b))\n        union = len(set_a.union(set_b))\n\n        # In case the union is zero, which means both vectors have no elements, return 1.0 implying identical sets.\n        if union == 0:\n            return 1.0\n\n        # Compute and return Jaccard similarity.\n        return intersection / union\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/LevenshteinDistance.py",
        "content": "```swarmauri/standard/distances/concrete/LevenshteinDistance.py\nimport numpy as np\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\n\nclass LevenshteinDistance(DistanceBase):\n    \"\"\"\n    Implements the IDistance interface to calculate the Levenshtein distance between two vectors.\n    The Levenshtein distance between two strings is given by the minimum number of operations needed to transform\n    one string into the other, where an operation is an insertion, deletion, or substitution of a single character.\n    \"\"\"\n    type: Literal['LevenshteinDistance'] = 'LevenshteinDistance'   \n    \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Compute the Levenshtein distance between two vectors.\n\n        Note: Since Levenshtein distance is typically calculated between strings,\n        it is assumed that the vectors represent strings where each element of the\n        vector corresponds to the ASCII value of a character in the string.\n\n        Args:\n            vector_a (List[float]): The first vector in the comparison.\n            vector_b (List[float]): The second vector in the comparison.\n\n        Returns:\n           float: The computed Levenshtein distance between vector_a and vector_b.\n        \"\"\"\n        string_a = ''.join([chr(int(round(value))) for value in vector_a.value])\n        string_b = ''.join([chr(int(round(value))) for value in vector_b.value])\n        \n        return self.levenshtein(string_a, string_b)\n    \n    def levenshtein(self, seq1: str, seq2: str) -> float:\n        \"\"\"\n        Calculate the Levenshtein distance between two strings.\n        \n        Args:\n            seq1 (str): The first string.\n            seq2 (str): The second string.\n        \n        Returns:\n            float: The Levenshtein distance between seq1 and seq2.\n        \"\"\"\n        size_x = len(seq1) + 1\n        size_y = len(seq2) + 1\n        matrix = np.zeros((size_x, size_y))\n        \n        for x in range(size_x):\n            matrix[x, 0] = x\n        for y in range(size_y):\n            matrix[0, y] = y\n\n        for x in range(1, size_x):\n            for y in range(1, size_y):\n                if seq1[x-1] == seq2[y-1]:\n                    matrix[x, y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1], matrix[x, y-1] + 1)\n                else:\n                    matrix[x, y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1] + 1, matrix[x, y-1] + 1)\n        \n        return matrix[size_x - 1, size_y - 1]\n    \n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        string_a = ''.join([chr(int(round(value))) for value in vector_a.value])\n        string_b = ''.join([chr(int(round(value))) for value in vector_b.value])\n        return 1 - self.levenshtein(string_a, string_b) / max(len(vector_a), len(vector_b))\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/ManhattanDistance.py",
        "content": "```swarmauri/standard/distances/concrete/ManhattanDistance.py\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass ManhattanDistance(DistanceBase):\n    \"\"\"\n    Concrete implementation of the IDistanceSimiliarity interface using the Manhattan distance.\n    \n    The Manhattan distance between two points is the sum of the absolute differences of their Cartesian coordinates.\n    This is also known as L1 distance.\n    \"\"\"\n    type: Literal['ManhattanDistance'] = 'ManhattanDistance'   \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Manhattan distance between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The Manhattan distance between vector_a and vector_b.\n        \"\"\"\n        if vector_a.shape != vector_b.shape:\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n        \n        return sum(abs(a - b) for a, b in zip(vector_a.value, vector_b.value))\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        The similarity based on Manhattan distance can be inversely related to the distance for some applications,\n        but this method intentionally returns NotImplementedError to signal that Manhattan distance is typically\n        not directly converted to similarity in the conventional sense used in this context.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            NotImplementedError: This is intended as this distance metric doesn't directly offer a similarity measure.\n        \"\"\"\n        raise NotImplementedError(\"ManhattanDistance does not directly provide a similarity measure.\")\n        \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        raise NotImplementedError(\"ManhattanDistance does not directly provide a similarity measure.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/MinkowskiDistance.py",
        "content": "```swarmauri/standard/distances/concrete/MinkowskiDistance.py\nfrom typing import List, Literal\nfrom scipy.spatial.distance import minkowski\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass MinkowskiDistance(DistanceBase):\n    \"\"\"\n    Implementation of the IDistanceSimiliarity interface using the Minkowski distance metric.\n    Minkowski distance is a generalized metric form that includes Euclidean distance,\n    Manhattan distance, and others depending on the order (p) parameter.\n\n    The class provides methods to compute the Minkowski distance between two vectors.\n\n    Parameters:\n    - p (int): The order of the Minkowski distance. p=2 corresponds to the Euclidean distance,\n               while p=1 corresponds to the Manhattan distance. Default is \n    \"\"\"\n    type: Literal['MinkowskiDistance'] = 'MinkowskiDistance'\n    p: int = 2\n\n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the Minkowski distance between two vectors.\n\n        Args:\n            vector_a (Vector): The first vector in the comparison.\n            vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Minkowski distance between vector_a and vector_b.\n        \"\"\"\n        # Check if both vectors have the same dimensionality\n        if vector_a.shape != vector_b.shape:\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n\n        # Extract data from Vector instances\n        data_a = vector_a.value\n        data_b = vector_b.value\n\n        # Calculate and return the Minkowski distance\n        return minkowski(data_a, data_b, p=self.p)\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Compute the similarity between two vectors based on the Minkowski distance.\n        The similarity is inversely related to the distance.\n\n        Args:\n            vector_a (Vector): The first vector to compare for similarity.\n            vector_b (Vector): The second vector to compare with the first vector.\n\n        Returns:\n            float: A similarity score between vector_a and vector_b.\n        \"\"\"\n        dist = self.distance(vector_a, vector_b)\n        return 1 / (1 + dist)  # An example similarity score\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/SorensenDiceDistance.py",
        "content": "```swarmauri/standard/distances/concrete/SorensenDiceDistance.py\nimport numpy as np\nfrom typing import List, Literal\nfrom collections import Counter\n\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass SorensenDiceDistance(DistanceBase):\n    \"\"\"\n    Implementing a concrete Vector Store class for calculating S\u00c3\u00b6rensen-Dice Index Distance.\n    The S\u00c3\u00b6rensen-Dice Index, or Dice's coefficient, is a measure of the similarity between two sets.\n    \"\"\"\n    type: Literal['SorensenDiceDistance'] = 'SorensenDiceDistance'   \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Compute the S\u00c3\u00b6rensen-Dice distance between two vectors.\n        \n        Args:\n            vector_a (List[float]): The first vector in the comparison.\n            vector_b (List[float]): The second vector in the comparison.\n        \n        Returns:\n            float: The computed S\u00c3\u00b6rensen-Dice distance between vector_a and vector_b.\n        \"\"\"\n        # Convert vectors to binary sets\n        set_a = set([i for i, val in enumerate(vector_a) if val])\n        set_b = set([i for i, val in enumerate(vector_b) if val])\n        \n        # Calculate the intersection size\n        intersection_size = len(set_a.intersection(set_b))\n        \n        # Sorensen-Dice Index calculation\n        try:\n            sorensen_dice_index = (2 * intersection_size) / (len(set_a) + len(set_b))\n        except ZeroDivisionError:\n            sorensen_dice_index = 0.0\n        \n        # Distance is inverse of similarity for S\u00c3\u00b6rensen-Dice\n        distance = 1 - sorensen_dice_index\n        \n        return distance\n    \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarity(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        raise NotImplementedError(\"Similarity calculation is not implemented for SorensenDiceDistance.\")\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        raise NotImplementedError(\"Similarity calculation is not implemented for SorensenDiceDistance.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/SquaredEuclideanDistance.py",
        "content": "```swarmauri/standard/distances/concrete/SquaredEuclideanDistance.py\nfrom typing import List, Literal\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\nfrom swarmauri.standard.distances.base.DistanceBase import DistanceBase\n\nclass SquaredEuclideanDistance(DistanceBase):\n    \"\"\"\n    A concrete class for computing the squared Euclidean distance between two vectors.\n    \"\"\"\n    type: Literal['SquaredEuclideanDistance'] = 'SquaredEuclideanDistance'\n    \n    def distance(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Computes the squared Euclidean distance between vectors `vector_a` and `vector_b`.\n\n        Parameters:\n        - vector_a (Vector): The first vector in the comparison.\n        - vector_b (Vector): The second vector in the comparison.\n\n        Returns:\n        - float: The computed squared Euclidean distance between vector_a and vector_b.\n        \"\"\"\n        if vector_a.shape != vector_b.shape:\n            raise ValueError(\"Vectors must be of the same dimensionality.\")\n\n        squared_distance = sum((a - b) ** 2 for a, b in zip(vector_a.value, vector_b.value))\n        return squared_distance\n\n    def similarity(self, vector_a: Vector, vector_b: Vector) -> float:\n        \"\"\"\n        Squared Euclidean distance is not used for calculating similarity.\n        \n        Parameters:\n        - vector_a (Vector): The first vector.\n        - vector_b (Vector): The second vector.\n\n        Raises:\n        - NotImplementedError: Indicates that similarity calculation is not implemented.\n        \"\"\"\n        raise NotImplementedError(\"Similarity calculation is not implemented for Squared Euclidean distance.\")\n        \n        \n    def distances(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: Vector, vectors_b: List[Vector]) -> List[float]:\n        raise NotImplementedError(\"Similarity calculation is not implemented for Squared Euclidean distance.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/__init__.py",
        "content": "```swarmauri/standard/metrics/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/base/__init__.py",
        "content": "```swarmauri/standard/metrics/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/base/MetricAggregateMixin.py",
        "content": "```swarmauri/standard/metrics/base/MetricAggregateMixin.py\nfrom typing import List, Any, Literal\nfrom pydantic import BaseModel\nfrom swarmauri.core.metrics.IMetricAggregate import IMetricAggregate\n\nclass MetricAggregateMixin(IMetricAggregate, BaseModel):\n    \"\"\"\n    An abstract base class that implements the IMetric interface, providing common \n    functionalities and properties for metrics within SwarmAURI.\n    \"\"\"\n    measurements: List[Any] = []\n\n    \n    def add_measurement(self, measurement) -> None:\n        \"\"\"\n        Adds measurement to the internal store of measurements.\n        \"\"\"\n        self.measurements.append(measurement)\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric's state/value, allowing for fresh calculations.\n        \"\"\"\n        self.measurements.clear()\n        self.value = None\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/base/MetricBase.py",
        "content": "```swarmauri/standard/metrics/base/MetricBase.py\nfrom typing import Any, Optional, Literal\nfrom pydantic import BaseModel, ConfigDict, Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.metrics.IMetric import IMetric\n\nclass MetricBase(IMetric, ComponentBase):\n    \"\"\"\n    A base implementation of the IMetric interface that provides the foundation\n    for specific metric implementations.\n    \"\"\"\n    unit: str\n    value: Any = None\n    resource: Optional[str] =  Field(default=ResourceTypes.METRIC.value, frozen=True)\n    type: Literal['MetricBase'] = 'MetricBase'\n\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"\n        Retrieves the current value of the metric.\n\n        Returns:\n            The current value of the metric.\n        \"\"\"\n        return self.value\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/base/MetricCalculateMixin.py",
        "content": "```swarmauri/standard/metrics/base/MetricCalculateMixin.py\nfrom abc import abstractmethod\nfrom typing import Any, Literal\nfrom pydantic import BaseModel\nfrom swarmauri.core.metrics.IMetricCalculate import IMetricCalculate\n\nclass MetricCalculateMixin(IMetricCalculate, BaseModel):\n    \"\"\"\n    A base implementation of the IMetric interface that provides the foundation\n    for specific metric implementations.\n    \"\"\"\n    \n    def update(self, value) -> None:\n        \"\"\"\n        Update the metric value based on new information.\n        This should be used internally by the `calculate` method or other logic.\n        \"\"\"\n        self.value = value\n\n    @abstractmethod\n    def calculate(self, **kwargs) -> Any:\n        \"\"\"\n        Calculate the metric based on the provided data.\n        This method must be implemented by subclasses to define specific calculation logic.\n        \"\"\"\n        raise NotImplementedError('calculate is not implemented yet.')\n    \n    def __call__(self, **kwargs) -> Any:\n        \"\"\"\n        Calculates the metric, updates the value, and returns the current value.\n        \"\"\"\n        self.calculate(**kwargs)\n        return self.value\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/base/MetricThresholdMixin.py",
        "content": "```swarmauri/standard/metrics/base/MetricThresholdMixin.py\nfrom abc import ABC, abstractmethod\nfrom typing import Literal\nfrom pydantic import BaseModel\nfrom swarmauri.core.metrics.IThreshold import IThreshold\n\nclass MetricThresholdMixin(IThreshold, BaseModel):\n    k: int\n    \n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/concrete/__init__.py",
        "content": "```swarmauri/standard/metrics/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/concrete/FirstImpressionMetric.py",
        "content": "```swarmauri/standard/metrics/concrete/FirstImpressionMetric.py\nfrom typing import Any, Literal\nfrom swarmauri.standard.metrics.base.MetricBase import MetricBase\n\nclass FirstImpressionMetric(MetricBase):\n    \"\"\"\n    Metric for capturing the first impression score from a set of scores.\n    \"\"\"\n    type: Literal['FirstImpressionMetric'] = 'FirstImpressionMetric'\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"\n        Retrieves the current value of the metric.\n\n        Returns:\n            The current value of the metric.\n        \"\"\"\n        return self.value\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/concrete/MeanMetric.py",
        "content": "```swarmauri/standard/metrics/concrete/MeanMetric.py\nfrom typing import Literal\nfrom swarmauri.standard.metrics.base.MetricBase import MetricBase\nfrom swarmauri.standard.metrics.base.MetricCalculateMixin import MetricCalculateMixin\nfrom swarmauri.standard.metrics.base.MetricAggregateMixin import MetricAggregateMixin\n\nclass MeanMetric(MetricAggregateMixin, MetricCalculateMixin, MetricBase):\n    \"\"\"\n    A metric that calculates the mean (average) of a list of numerical values.\n\n    Attributes:\n        name (str): The name of the metric.\n        unit (str): The unit of measurement for the mean (e.g., \"degrees\", \"points\").\n        _value (float): The calculated mean of the measurements.\n        _measurements (list): A list of measurements (numerical values) to average.\n    \"\"\"\n    type: Literal['MeanMetric'] = 'MeanMetric'\n\n    def add_measurement(self, measurement: int) -> None:\n        \"\"\"\n        Adds a measurement to the internal list of measurements.\n\n        Args:\n            measurement (float): A numerical value to be added to the list of measurements.\n        \"\"\"\n        # Append the measurement to the internal list\n        self.measurements.append(measurement)\n\n    def calculate(self) -> float:\n        \"\"\"\n        Calculate the mean of all added measurements.\n        \n        Returns:\n            float: The mean of the measurements or None if no measurements have been added.\n        \"\"\"\n        if not self.measurements:\n            return None  # Return None if there are no measurements\n        # Calculate the mean\n        mean = sum(self.measurements) / len(self.measurements)\n        # Update the metric's value\n        self.update(mean)\n        # Return the calculated mean\n        return mean\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/concrete/StaticMetric.py",
        "content": "```swarmauri/standard/metrics/concrete/StaticMetric.py\nfrom typing import Any, Literal\nfrom swarmauri.standard.metrics.base.MetricBase import MetricBase\n\nclass StaticMetric(MetricBase):\n    \"\"\"\n    Metric for capturing the first impression score from a set of scores.\n    \"\"\"\n    type: Literal['StaticMetric'] = 'StaticMetric'\n\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"\n        Retrieves the current value of the metric.\n\n        Returns:\n            The current value of the metric.\n        \"\"\"\n        return self.value\n\n```"
    },
    {
        "document_name": "swarmauri/standard/metrics/concrete/ZeroMetric.py",
        "content": "```swarmauri/standard/metrics/concrete/ZeroMetric.py\nfrom typing import Literal\nfrom swarmauri.standard.metrics.base.MetricBase import MetricBase\n\nclass ZeroMetric(MetricBase):\n    \"\"\"\n    A concrete implementation of MetricBase that statically represents the value 0.\n    This can be used as a placeholder or default metric where dynamic calculation is not required.\n    \"\"\"\n    unit: str = \"unitless\"\n    value: int = 0\n    type: Literal['ZeroMetric'] = 'ZeroMetric'\n\n    def __call__(self):\n        \"\"\"\n        Overrides the value property to always return 0.\n        \"\"\"\n        return self.value\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/__init__.py",
        "content": "```swarmauri/standard/agent_factories/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/concrete/AgentFactory.py",
        "content": "```swarmauri/standard/agent_factories/concrete/AgentFactory.py\nimport json\nfrom datetime import datetime\nfrom typing import Callable, Dict, Any\nfrom swarmauri.core.agents.IAgent import IAgent\nfrom swarmauri.core.agentfactories.IAgentFactory import IAgentFactory\nfrom swarmauri.core.agentfactories.IExportConf import IExportConf\n\nclass AgentFactory(IAgentFactory, IExportConf):\n    def __init__(self):\n        \"\"\" Initializes the AgentFactory with an empty registry and metadata. \"\"\"\n        self._registry: Dict[str, Callable[..., IAgent]] = {}\n        self._metadata = {\n            'id': None,\n            'name': 'DefaultAgentFactory',\n            'type': 'Generic',\n            'date_created': datetime.now(),\n            'last_modified': datetime.now()\n        }\n    \n    # Implementation of IAgentFactory methods\n    def create_agent(self, agent_type: str, **kwargs) -> IAgent:\n        if agent_type not in self._registry:\n            raise ValueError(f\"Agent type '{agent_type}' is not registered.\")\n        \n        constructor = self._registry[agent_type]\n        return constructor(**kwargs)\n\n    def register_agent(self, agent_type: str, constructor: Callable[..., IAgent]) -> None:\n        if agent_type in self._registry:\n            raise ValueError(f\"Agent type '{agent_type}' is already registered.\")\n        self._registry[agent_type] = constructor\n        self._metadata['last_modified'] = datetime.now()\n    \n    # Implementation of IExportConf methods\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Exports the registry metadata as a dictionary.\"\"\"\n        export_data = self._metadata.copy()\n        export_data['registry'] = list(self._registry.keys())\n        return export_data\n\n    def to_json(self) -> str:\n        \"\"\"Exports the registry metadata as a JSON string.\"\"\"\n        return json.dumps(self.to_dict(), default=str, indent=4)\n\n    def export_to_file(self, file_path: str) -> None:\n        \"\"\"Exports the registry metadata to a file.\"\"\"\n        with open(file_path, 'w') as f:\n            f.write(self.to_json())\n    \n    @property\n    def id(self) -> int:\n        return self._metadata['id']\n\n    @id.setter\n    def id(self, value: int) -> None:\n        self._metadata['id'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def name(self) -> str:\n        return self._metadata['name']\n\n    @name.setter\n    def name(self, value: str) -> None:\n        self._metadata['name'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def type(self) -> str:\n        return self._metadata['type']\n\n    @type.setter\n    def type(self, value: str) -> None:\n        self._metadata['type'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def date_created(self) -> datetime:\n        return self._metadata['date_created']\n\n    @property\n    def last_modified(self) -> datetime:\n        return self._metadata['last_modified']\n\n    @last_modified.setter\n    def last_modified(self, value: datetime) -> None:\n        self._metadata['last_modified'] = value\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/concrete/ConfDrivenAgentFactory.py",
        "content": "```swarmauri/standard/agent_factories/concrete/ConfDrivenAgentFactory.py\nimport json\nimport importlib\nfrom datetime import datetime\nfrom typing import Any, Dict, Callable\nfrom swarmauri.core.agents.IAgent import IAgent  # Replace with the correct IAgent path\nfrom swarmauri.core.agentfactories.IAgentFactory import IAgentFactory\nfrom swarmauri.core.agentfactories.IExportConf import IExportConf\n\n\nclass ConfDrivenAgentFactory(IAgentFactory, IExportConf):\n    def __init__(self, config_path: str):\n        self._config_path = config_path\n        self._config = self._load_config(config_path)\n        self._registry = {}\n        self._metadata = {\n            'id': None,\n            'name': 'ConfAgentFactory',\n            'type': 'Configurable',\n            'date_created': datetime.now(),\n            'last_modified': datetime.now()\n        }\n        \n        self._initialize_registry()\n\n    def _load_config(self, config_path: str) -> Dict[str, Any]:\n        with open(config_path, 'r') as file:\n            return json.load(file)\n    \n    def _initialize_registry(self) -> None:\n        for agent_type, agent_info in self._config.get(\"agents\", {}).items():\n            module_name, class_name = agent_info[\"class_path\"].rsplit('.', 1)\n            module = importlib.import_module(module_name)\n            cls = getattr(module, class_name)\n            self.register_agent(agent_type, cls)\n    \n    # Implementation of IAgentFactory methods\n    def create_agent(self, agent_type: str, **kwargs) -> IAgent:\n        if agent_type not in self._registry:\n            raise ValueError(f\"Agent type '{agent_type}' is not registered.\")\n        \n        return self._registry[agent_type](**kwargs)\n\n    def register_agent(self, agent_type: str, constructor: Callable[..., IAgent]) -> None:\n        self._registry[agent_type] = constructor\n        self._metadata['last_modified'] = datetime.now()\n    \n    # Implementation of IExportConf methods\n    def to_dict(self) -> Dict[str, Any]:\n        return self._metadata.copy()\n\n    def to_json(self) -> str:\n        return json.dumps(self.to_dict(), default=str, indent=4)\n\n    def export_to_file(self, file_path: str) -> None:\n        with open(file_path, 'w') as f:\n            f.write(self.to_json())\n\n    # Additional methods to implement required properties and their setters\n    # Implementing getters and setters for metadata properties as needed\n    @property\n    def id(self) -> int:\n        return self._metadata['id']\n\n    @id.setter\n    def id(self, value: int) -> None:\n        self._metadata['id'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def name(self) -> str:\n        return self._metadata['name']\n\n    @name.setter\n    def name(self, value: str) -> None:\n        self._metadata['name'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def type(self) -> str:\n        return self._metadata['type']\n\n    @type.setter\n    def type(self, value: str) -> None:\n        self._metadata['type'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def date_created(self) -> datetime:\n        return self._metadata['date_created']\n\n    @property\n    def last_modified(self) -> datetime:\n        return self._metadata['last_modified']\n\n    @last_modified.setter\n    def last_modified(self, value: datetime) -> None:\n        self._metadata['last_modified'] = value\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/concrete/ReflectiveAgentFactory.py",
        "content": "```swarmauri/standard/agent_factories/concrete/ReflectiveAgentFactory.py\nimport importlib\nfrom datetime import datetime\nimport json\nfrom typing import Callable, Dict, Type, Any\nfrom swarmauri.core.agents.IAgent import IAgent  # Update this import path as needed\nfrom swarmauri.core.agentfactories.IAgentFactory import IAgentFactory\nfrom swarmauri.core.agentfactories.IExportConf import IExportConf\n\nclass ReflectiveAgentFactory(IAgentFactory, IExportConf):\n    def __init__(self):\n        self._registry: Dict[str, Type[IAgent]] = {}\n        self._metadata = {\n            'id': None,\n            'name': 'ReflectiveAgentFactory',\n            'type': 'Reflective',\n            'date_created': datetime.now(),\n            'last_modified': datetime.now()\n        }\n\n    def create_agent(self, agent_type: str, **kwargs) -> IAgent:\n        if agent_type not in self._registry:\n            raise ValueError(f\"Agent type '{agent_type}' is not registered.\")\n        \n        agent_class = self._registry[agent_type]\n        return agent_class(**kwargs)\n\n    def register_agent(self, agent_type: str, class_path: str) -> None:\n        module_name, class_name = class_path.rsplit('.', 1)\n        module = importlib.import_module(module_name)\n        cls = getattr(module, class_name)\n        self._registry[agent_type] = cls\n        self._metadata['last_modified'] = datetime.now()\n\n    # Implementations for the IExportConf interface\n    def to_dict(self) -> Dict[str, Any]:\n        return self._metadata.copy()\n\n    def to_json(self) -> str:\n        return json.dumps(self.to_dict(), default=str, indent=4)\n\n    def export_to_file(self, file_path: str) -> None:\n        with open(file_path, 'w') as file:\n            file.write(self.to_json())\n\n    # Property implementations: id, name, type, date_created, and last_modified\n    @property\n    def id(self) -> int:\n        return self._metadata['id']\n\n    @id.setter\n    def id(self, value: int) -> None:\n        self._metadata['id'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def name(self) -> str:\n        return self._metadata['name']\n\n    @name.setter\n    def name(self, value: str) -> None:\n        self._metadata['name'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def type(self) -> str:\n        return self._metadata['type']\n\n    @type.setter\n    def type(self, value: str) -> None:\n        self._metadata['type'] = value\n        self._metadata['last_modified'] = datetime.now()\n\n    @property\n    def date_created(self) -> datetime:\n        return self._metadata['date_created']\n\n    @property\n    def last_modified(self) -> datetime:\n        return self._metadata['last_modified']\n\n    @last_modified.setter\n    def last_modified(self, value: datetime) -> None:\n        self._metadata['last_modified'] = value\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/concrete/JsonAgentFactory.py",
        "content": "```swarmauri/standard/agent_factories/concrete/JsonAgentFactory.py\nimport json\nfrom jsonschema import validate, ValidationError\nfrom typing import Dict, Any, Callable, Type\nfrom swarmauri.core.agents.IAgent import IAgent\nfrom swarmauri.core.agent_factories.IAgentFactory import IAgentFactory\nfrom swarmauri.core.agent_factories.IExportConf import IExportConf\nimport importlib\n\nclass JsonAgentFactory:\n    def __init__(self, config: Dict[str, Any]):\n        self._config = config\n        self._registry: Dict[str, Type[Any]] = {}\n\n        # Load and validate config\n        self._validate_config()\n        self._load_config()\n\n    def _validate_config(self) -> None:\n        \"\"\"Validates the configuration against the JSON schema.\"\"\"\n        schema = {\n              \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n              \"type\": \"object\",\n              \"properties\": {\n                \"agents\": {\n                  \"type\": \"object\",\n                  \"patternProperties\": {\n                    \"^[a-zA-Z][a-zA-Z0-9_-]*$\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"constructor\": {\n                          \"type\": \"object\",\n                          \"required\": [\"module\", \"class\"]\n                        }\n                      },\n                      \"required\": [\"constructor\"]\n                    }\n                  }\n                }\n              },\n              \"required\": [\"agents\"]\n            }\n\n        try:\n            validate(instance=self._config, schema=schema)\n        except ValidationError as e:\n            raise ValueError(f\"Invalid configuration: {e.message}\")\n\n    def _load_config(self):\n        \"\"\"Loads configuration and registers agents accordingly.\"\"\"\n        agents_config = self._config.get(\"agents\", {})\n        for agent_type, agent_info in agents_config.items():\n            module_name = agent_info[\"constructor\"][\"module\"]\n            class_name = agent_info[\"constructor\"][\"class\"]\n\n            module = importlib.import_module(module_name)\n            cls = getattr(module, class_name)\n\n            self.register_agent(agent_type, cls)\n\n    def create_agent(self, agent_type: str, **kwargs) -> Any:\n        if agent_type not in self._registry:\n            raise ValueError(f\"Agent type '{agent_type}' is not registered.\")\n        \n        constructor = self._registry[agent_type]\n        print(f\"Creating instance of {constructor}, with args: {kwargs}\")\n        return constructor(**kwargs)\n\n    def register_agent(self, agent_type: str, constructor: Callable[..., Any]) -> None:\n        if agent_type in self._registry:\n            raise ValueError(f\"Agent type '{agent_type}' is already registered.\")\n        \n        print(f\"Registering agent type '{agent_type}' with constructor: {constructor}\")\n        self._registry[agent_type] = constructor\n\n    def to_dict(self) -> Dict[str, Any]:\n        return self._config\n\n    def to_json(self) -> str:\n        return json.dumps(self._config, default=str, indent=4)\n\n    def export_to_file(self, file_path: str) -> None:\n        with open(file_path, 'w') as file:\n            file.write(self.to_json())\n\n    @property\n    def id(self) -> int:\n        return self._config.get('id', None)  # Assuming config has an 'id'.\n\n    @id.setter\n    def id(self, value: int) -> None:\n        self._config['id'] = value\n\n    @property\n    def name(self) -> str:\n        return self._config.get('name', 'ConfDrivenAgentFactory')\n\n    @name.setter\n    def name(self, value: str) -> None:\n        self._config['name'] = value\n\n    @property\n    def type(self) -> str:\n        return self._config.get('type', 'Configuration-Driven')\n\n    @type.setter\n    def type(self, value: str) -> None:\n        self._config['type'] = value\n\n    @property\n    def date_created(self) -> str:\n        return self._config.get('date_created', None)\n\n    @property\n    def last_modified(self) -> str:\n        return self._config.get('last_modified', None)\n\n    @last_modified.setter\n    def last_modified(self, value: str) -> None:\n        self._config['last_modified'] = value\n        self._config['last_modified'] = value\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/concrete/__init__.py",
        "content": "```swarmauri/standard/agent_factories/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agent_factories/base/__init__.py",
        "content": "```swarmauri/standard/agent_factories/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/__init__.py",
        "content": "```swarmauri/standard/embeddings/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/base/EmbeddingBase.py",
        "content": "```swarmauri/standard/embeddings/base/EmbeddingBase.py\nfrom typing import Optional, Literal\nfrom pydantic import Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.embeddings.IVectorize import IVectorize\nfrom swarmauri.core.embeddings.IFeature import IFeature\nfrom swarmauri.core.embeddings.ISaveModel import ISaveModel\n\nclass EmbeddingBase(IVectorize, IFeature, ISaveModel, ComponentBase):\n    resource: Optional[str] =  Field(default=ResourceTypes.EMBEDDING.value, frozen=True)\n    type: Literal['EmbeddingBase'] = 'EmbeddingBase'\n        \n\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/base/__init__.py",
        "content": "```swarmauri/standard/embeddings/base/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/concrete/Doc2VecEmbedding.py",
        "content": "```swarmauri/standard/embeddings/concrete/Doc2VecEmbedding.py\nfrom typing import List, Union, Any, Literal\nfrom pydantic import PrivateAttr\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom swarmauri.standard.embeddings.base.EmbeddingBase import EmbeddingBase\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\nclass Doc2VecEmbedding(EmbeddingBase):\n    _model = PrivateAttr()\n    type: Literal['Doc2VecEmbedding'] = 'Doc2VecEmbedding'    \n\n    def __init__(self, \n                 vector_size: int = 2000, \n                 window: int = 10,\n                 min_count: int = 1,\n                 workers: int = 5,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self._model = Doc2Vec(vector_size=vector_size, \n                              window=window, \n                              min_count=min_count, \n                              workers=workers)\n        \n\n    def extract_features(self) -> List[Any]:\n        return list(self._model.wv.key_to_index.keys())\n\n    def fit(self, documents: List[str], labels=None) -> None:\n        tagged_data = [TaggedDocument(words=_d.split(), \n            tags=[str(i)]) for i, _d in enumerate(documents)]\n\n        self._model.build_vocab(tagged_data)\n        self._model.train(tagged_data, total_examples=self._model.corpus_count, epochs=self._model.epochs)\n\n    def transform(self, documents: List[str]) -> List[Vector]:\n        vectors = [self._model.infer_vector(doc.split()) for doc in documents]\n        return [Vector(value=vector) for vector in vectors]\n\n    def fit_transform(self, documents: List[str], **kwargs) -> List[Vector]:\n        \"\"\"\n        Fine-tunes the MLM and generates embeddings for the provided documents.\n        \"\"\"\n        self.fit(documents, **kwargs)\n        return self.transform(documents)\n\n    def infer_vector(self, data: str) -> Vector:\n        vector = self._model.infer_vector(data.split())\n        return Vector(value=vector.squeeze().tolist())\n\n    def save_model(self, path: str) -> None:\n        \"\"\"\n        Saves the Doc2Vec model to the specified path.\n        \"\"\"\n        self._model.save(path)\n    \n    def load_model(self, path: str) -> None:\n        \"\"\"\n        Loads a Doc2Vec model from the specified path.\n        \"\"\"\n        self._model = Doc2Vec.load(path)\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/concrete/MlmEmbedding.py",
        "content": "```swarmauri/standard/embeddings/concrete/MlmEmbedding.py\nfrom typing import List, Union, Any, Literal\nimport logging\nfrom pydantic import PrivateAttr\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\n\nfrom swarmauri.standard.embeddings.base.EmbeddingBase import EmbeddingBase\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\nclass MlmEmbedding(EmbeddingBase):\n    \"\"\"\n    EmbeddingBase implementation that fine-tunes a Masked Language Model (MLM).\n    \"\"\"\n\n    embedding_name: str = 'bert-base-uncased'\n    batch_size: int = 32\n    learning_rate: float = 5e-5\n    masking_ratio: float = 0.15\n    randomness_ratio: float = 0.10\n    epochs: int = 0\n    add_new_tokens: bool = False\n    _tokenizer = PrivateAttr()\n    _model = PrivateAttr()\n    _device = PrivateAttr()\n    _mask_token_id = PrivateAttr()        \n    type: Literal['MlmEmbedding'] = 'MlmEmbedding'\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._tokenizer = AutoTokenizer.from_pretrained(self.embedding_name)\n        self._model = AutoModelForMaskedLM.from_pretrained(self.embedding_name)\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._model.to(self._device)\n        self._mask_token_id = self._tokenizer.convert_tokens_to_ids([self._tokenizer.mask_token])[0]\n\n    def extract_features(self) -> List[str]:\n        \"\"\"\n        Extracts the tokens from the vocabulary of the fine-tuned MLM.\n\n        Returns:\n        - List[str]: A list of token strings in the model's vocabulary.\n        \"\"\"\n        # Get the vocabulary size\n        vocab_size = len(self._tokenizer)\n        \n        # Retrieve the token strings for each id in the vocabulary\n        token_strings = [self._tokenizer.convert_ids_to_tokens(i) for i in range(vocab_size)]\n        \n        return token_strings\n\n    def _mask_tokens(self, encodings):\n        input_ids = encodings.input_ids.to(self._device)\n        attention_mask = encodings.attention_mask.to(self._device)\n\n        labels = input_ids.detach().clone()\n\n        probability_matrix = torch.full(labels.shape, self.masking_ratio, device=self._device)\n        special_tokens_mask = [\n            self._tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool, device=self._device), value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        labels[~masked_indices] = -100\n        \n        indices_replaced = torch.bernoulli(torch.full(labels.shape, self.masking_ratio, device=self._device)).bool() & masked_indices\n        input_ids[indices_replaced] = self._mask_token_id\n\n        indices_random = torch.bernoulli(torch.full(labels.shape, self.randomness_ratio, device=self._device)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self._tokenizer), labels.shape, dtype=torch.long, device=self._device)\n        input_ids[indices_random] = random_words[indices_random]\n\n        return input_ids, attention_mask, labels\n\n    def fit(self, documents: List[Union[str, Any]]):\n        # Check if we need to add new tokens\n        if self.add_new_tokens:\n            new_tokens = self.find_new_tokens(documents)\n            if new_tokens:\n                num_added_toks = self._tokenizer.add_tokens(new_tokens)\n                if num_added_toks > 0:\n                    logging.info(f\"Added {num_added_toks} new tokens.\")\n                    self.model.resize_token_embeddings(len(self._tokenizer))\n\n        encodings = self._tokenizer(documents, return_tensors='pt', padding=True, truncation=True, max_length=512)\n        input_ids, attention_mask, labels = self._mask_tokens(encodings)\n        optimizer = AdamW(self._model.parameters(), lr=self.learning_rate)\n        dataset = TensorDataset(input_ids, attention_mask, labels)\n        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        self._model.train()\n        for batch in data_loader:\n            batch = {k: v.to(self._device) for k, v in zip(['input_ids', 'attention_mask', 'labels'], batch)}\n            outputs = self._model(**batch)\n            loss = outputs.loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        self.epochs += 1\n        logging.info(f\"Epoch {self.epochs} complete. Loss {loss.item()}\")\n\n    def find_new_tokens(self, documents):\n        # Identify unique words in documents that are not in the tokenizer's vocabulary\n        unique_words = set()\n        for doc in documents:\n            tokens = set(doc.split())  # Simple whitespace tokenization\n            unique_words.update(tokens)\n        existing_vocab = set(self._tokenizer.get_vocab().keys())\n        new_tokens = list(unique_words - existing_vocab)\n        return new_tokens if new_tokens else None\n\n    def transform(self, documents: List[Union[str, Any]]) -> List[Vector]:\n        \"\"\"\n        Generates embeddings for a list of documents using the fine-tuned MLM.\n        \"\"\"\n        self._model.eval()\n        embedding_list = []\n        \n        for document in documents:\n            inputs = self._tokenizer(document, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n            inputs = {k: v.to(self._device) for k, v in inputs.items()}\n            with torch.no_grad():\n                outputs = self._model(**inputs)\n            # Extract embedding (for simplicity, averaging the last hidden states)\n            if hasattr(outputs, 'last_hidden_state'):\n                embedding = outputs.last_hidden_state.mean(1)\n            else:\n                # Fallback or corrected attribute access\n                embedding = outputs['logits'].mean(1)\n            embedding = embedding.cpu().numpy()\n            embedding_list.append(Vector(value=embedding.squeeze().tolist()))\n\n        return embedding_list\n\n    def fit_transform(self, documents: List[Union[str, Any]], **kwargs) -> List[Vector]:\n        \"\"\"\n        Fine-tunes the MLM and generates embeddings for the provided documents.\n        \"\"\"\n        self.fit(documents, **kwargs)\n        return self.transform(documents)\n\n    def infer_vector(self, data: Union[str, Any], *args, **kwargs) -> Vector:\n        \"\"\"\n        Generates an embedding for the input data.\n\n        Parameters:\n        - data (Union[str, Any]): The input data, expected to be a textual representation.\n                                  Could be a single string or a batch of strings.\n        \"\"\"\n        # Tokenize the input data and ensure the tensors are on the correct device.\n        self._model.eval()\n        inputs = self._tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(self._device) for k, v in inputs.items()}\n\n        # Generate embeddings using the model\n        with torch.no_grad():\n            outputs = self._model(**inputs)\n\n        if hasattr(outputs, 'last_hidden_state'):\n            # Access the last layer and calculate the mean across all tokens (simple pooling)\n            embedding = outputs.last_hidden_state.mean(dim=1)\n        else:\n            embedding = outputs['logits'].mean(1)\n        # Move the embeddings back to CPU for compatibility with downstream tasks if necessary\n        embedding = embedding.cpu().numpy()\n\n        return Vector(value=embedding.squeeze().tolist())\n\n    def save_model(self, path: str) -> None:\n        \"\"\"\n        Saves the model and tokenizer to the specified directory.\n        \"\"\"\n        self._model.save_pretrained(path)\n        self._tokenizer.save_pretrained(path)\n\n    def load_model(self, path: str) -> None:\n        \"\"\"\n        Loads the model and tokenizer from the specified directory.\n        \"\"\"\n        self._model = AutoModelForMaskedLM.from_pretrained(path)\n        self._tokenizer = AutoTokenizer.from_pretrained(path)\n        self._model.to(self._device)  # Ensure the model is loaded to the correct device\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/concrete/NmfEmbedding.py",
        "content": "```swarmauri/standard/embeddings/concrete/NmfEmbedding.py\nimport joblib\nfrom typing import List, Any, Literal\nfrom pydantic import PrivateAttr\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom swarmauri.standard.embeddings.base.EmbeddingBase import EmbeddingBase\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\nclass NmfEmbedding(EmbeddingBase):\n    n_components: int = 10\n    _tfidf_vectorizer = PrivateAttr()\n    _model = PrivateAttr()\n    feature_names: List[Any] = []\n    \n    type: Literal['NmfEmbedding'] = 'NmfEmbedding'\n    def __init__(self,**kwargs):\n\n        super().__init__(**kwargs)\n        # Initialize TF-IDF Vectorizer\n        self._tfidf_vectorizer = TfidfVectorizer()\n        # Initialize NMF with the desired number of components\n        self._model = NMF(n_components=self.n_components)\n\n    def fit(self, data):\n        \"\"\"\n        Fit the NMF model to data.\n\n        Args:\n            data (Union[str, Any]): The text data to fit.\n        \"\"\"\n        # Transform data into TF-IDF matrix\n        tfidf_matrix = self._tfidf_vectorizer.fit_transform(data)\n        # Fit the NMF model\n        self._model.fit(tfidf_matrix)\n        # Store feature names\n        self.feature_names = self._tfidf_vectorizer.get_feature_names_out()\n\n    def transform(self, data):\n        \"\"\"\n        Transform new data into NMF feature space.\n\n        Args:\n            data (Union[str, Any]): Text data to transform.\n\n        Returns:\n            List[IVector]: A list of vectors representing the transformed data.\n        \"\"\"\n        # Transform data into TF-IDF matrix\n        tfidf_matrix = self._tfidf_vectorizer.transform(data)\n        # Transform TF-IDF matrix into NMF space\n        nmf_features = self._model.transform(tfidf_matrix)\n\n        # Wrap NMF features in SimpleVector instances and return\n        return [Vector(value=features.tolist()) for features in nmf_features]\n\n    def fit_transform(self, data):\n        \"\"\"\n        Fit the model to data and then transform it.\n        \n        Args:\n            data (Union[str, Any]): The text data to fit and transform.\n\n        Returns:\n            List[IVector]: A list of vectors representing the fitted and transformed data.\n        \"\"\"\n        self.fit(data)\n        return self.transform(data)\n\n    def infer_vector(self, data):\n        \"\"\"\n        Convenience method for transforming a single data point.\n        \n        Args:\n            data (Union[str, Any]): Single text data to transform.\n\n        Returns:\n            IVector: A vector representing the transformed single data point.\n        \"\"\"\n        return self.transform([data])[0]\n    \n    def extract_features(self):\n        \"\"\"\n        Extract the feature names from the TF-IDF vectorizer.\n        \n        Returns:\n            The feature names.\n        \"\"\"\n        return self.feature_names.tolist()\n\n    def save_model(self, path: str) -> None:\n        \"\"\"\n        Saves the NMF model and TF-IDF vectorizer using joblib.\n        \"\"\"\n        # It might be necessary to save both tfidf_vectorizer and model\n        # Consider using a directory for 'path' or appended identifiers for each model file\n        joblib.dump(self._tfidf_vectorizer, f\"{path}_tfidf.joblib\")\n        joblib.dump(self._model, f\"{path}_nmf.joblib\")\n\n    def load_model(self, path: str) -> None:\n        \"\"\"\n        Loads the NMF model and TF-IDF vectorizer from paths using joblib.\n        \"\"\"\n        self._tfidf_vectorizer = joblib.load(f\"{path}_tfidf.joblib\")\n        self._model = joblib.load(f\"{path}_nmf.joblib\")\n        # Dependending on your implementation, you might need to refresh the feature_names\n        self.feature_names = self._tfidf_vectorizer.get_feature_names_out()\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/concrete/__init__.py",
        "content": "```swarmauri/standard/embeddings/concrete/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/embeddings/concrete/TfidfEmbedding.py",
        "content": "```swarmauri/standard/embeddings/concrete/TfidfEmbedding.py\nfrom typing import List, Union, Any, Literal\nimport joblib\nfrom pydantic import PrivateAttr\nfrom sklearn.feature_extraction.text import TfidfVectorizer as SklearnTfidfVectorizer\n\nfrom swarmauri.standard.embeddings.base.EmbeddingBase import EmbeddingBase\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\n\nclass TfidfEmbedding(EmbeddingBase):\n    _model = PrivateAttr()\n    _fit_matrix = PrivateAttr()\n    type: Literal[\"TfidfEmbedding\"] = \"TfidfEmbedding\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._model = SklearnTfidfVectorizer()\n\n    def extract_features(self):\n        return self._model.get_feature_names_out().tolist()\n\n    def fit(self, documents: List[str]) -> None:\n        self._fit_matrix = self._model.fit_transform(documents)\n\n    def fit_transform(self, documents: List[str]) -> List[Vector]:\n        self._fit_matrix = self._model.fit_transform(documents)\n        # Convert the sparse matrix rows into Vector instances\n        vectors = [\n            Vector(value=vector.toarray().flatten()) for vector in self._fit_matrix\n        ]\n        return vectors\n\n    def transform(self, data: Union[str, Any], documents: List[str]) -> List[Vector]:\n        raise NotImplementedError(\"Transform not implemented on TFIDFVectorizer.\")\n\n    def infer_vector(self, data: str, documents: List[str]) -> Vector:\n        documents.append(data)\n        tmp_tfidf_matrix = self.fit_transform(documents)\n        query_vector = tmp_tfidf_matrix[-1]\n        return query_vector\n\n    def save_model(self, path: str) -> None:\n        \"\"\"\n        Saves the TF-IDF model to the specified path using joblib.\n        \"\"\"\n        joblib.dump(self._model, path)\n\n    def load_model(self, path: str) -> None:\n        \"\"\"\n        Loads a TF-IDF model from the specified path using joblib.\n        \"\"\"\n        self._model = joblib.load(path)\n\n```"
    },
    {
        "document_name": "swarmauri/standard/exceptions/__init__.py",
        "content": "```swarmauri/standard/exceptions/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/exceptions/base/__init__.py",
        "content": "```swarmauri/standard/exceptions/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/exceptions/concrete/IndexErrorWithContext.py",
        "content": "```swarmauri/standard/exceptions/concrete/IndexErrorWithContext.py\nimport inspect\n\nclass IndexErrorWithContext(Exception):\n    def __init__(self, original_exception):\n        self.original_exception = original_exception\n        self.stack_info = inspect.stack()\n        self.handle_error()\n\n    def handle_error(self):\n        # You might want to log this information or handle it differently depending on your application's needs\n        frame = self.stack_info[1]  # Assuming the IndexError occurs one level up from where it's caught\n        error_details = {\n            \"message\": str(self.original_exception),\n            \"function\": frame.function,\n            \"file\": frame.filename,\n            \"line\": frame.lineno,\n            \"code_context\": ''.join(frame.code_context).strip() if frame.code_context else \"No context available\"\n        }\n        print(\"IndexError occurred with detailed context:\")\n        for key, value in error_details.items():\n            print(f\"{key.capitalize()}: {value}\")\n```"
    },
    {
        "document_name": "swarmauri/standard/exceptions/concrete/__init__.py",
        "content": "```swarmauri/standard/exceptions/concrete/__init__.py\nfrom .IndexErrorWithContext import IndexErrorWithContext\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/__init__.py",
        "content": "```swarmauri/standard/llms/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/base/LLMBase.py",
        "content": "```swarmauri/standard/llms/base/LLMBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Union, Optional, List, Literal\nfrom pydantic import BaseModel, ConfigDict, ValidationError, model_validator, Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.llms.IPredict import IPredict\n\nclass LLMBase(IPredict, ComponentBase):\n    allowed_models: List[str] = []\n    resource: Optional[str] =  Field(default=ResourceTypes.LLM.value, frozen=True)\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    type: Literal['LLMBase'] = 'LLMBase'\n\n    @model_validator(mode='after')\n    @classmethod\n    def _validate_name_in_allowed_models(cls, values):\n        name = values.name\n        allowed_models = values.allowed_models\n        if name and name not in allowed_models:\n            raise ValueError(f\"Model name {name} is not allowed. Choose from {allowed_models}\")\n        return values\n        \n    def predict(self, *args, **kwargs):\n        raise NotImplementedError('Predict not implemented in subclass yet.')\n        \n```"
    },
    {
        "document_name": "swarmauri/standard/llms/base/__init__.py",
        "content": "```swarmauri/standard/llms/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/AnthropicModel.py",
        "content": "```swarmauri/standard/llms/concrete/AnthropicModel.py\nimport json\nfrom typing import List, Dict, Literal\nimport anthropic\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass AnthropicModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['claude-3-opus-20240229', \n    'claude-3-sonnet-20240229', \n    'claude-3-haiku-20240307',\n    'claude-2.1',\n    'claude-2.0',\n    'claude-instant-1.2']\n    name: str = \"claude-3-haiku-20240307\"\n    type: Literal['AnthropicModel'] = 'AnthropicModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n       # Get only the properties that we require\n        message_properties = [\"content\", \"role\"]\n\n        # Exclude FunctionMessages\n        formatted_messages = [message.model_dump(include=message_properties) for message in messages if message.role != 'system']\n        return formatted_messages\n\n    def _get_system_context(self, messages: List[SubclassUnion[MessageBase]]) -> str:\n        system_context = None\n        for message in messages:\n            if message.role == 'system':\n                system_context = message.content\n        return system_context\n\n    \n    def predict(self, \n        conversation, \n        temperature=0.7, \n        max_tokens=256):\n\n        # Create client\n        client = anthropic.Anthropic(api_key=self.api_key)\n        \n        # Get system_context from last message with system context in it\n        system_context = self._get_system_context(conversation.history)\n        formatted_messages = self._format_messages(conversation.history)\n\n        if system_context:\n            response = client.messages.create(\n                model=self.name,\n                messages=formatted_messages,\n                system=system_context,\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n        else:\n            response = client.messages.create(\n                model=self.name,\n                messages=formatted_messages,\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n        \n        message_content = response.content[0].text\n        conversation.add_message(AgentMessage(content=message_content))\n        \n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/AnthropicToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/AnthropicToolModel.py\nimport json\nfrom typing import List, Dict, Literal, Any\nimport logging\nimport anthropic\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.AnthropicSchemaConverter import AnthropicSchemaConverter\n\nclass AnthropicToolModel(LLMBase):\n    \"\"\"\n    Provider resources: https://docs.anthropic.com/en/docs/build-with-claude/tool-use\n    \"\"\"\n    api_key: str\n    allowed_models: List[str] = ['claude-3-haiku-20240307',\n    'claude-3-opus-20240229',\n    'claude-3-sonnet-20240229']\n    name: str = \"claude-3-haiku-20240307\"\n    type: Literal['AnthropicToolModel'] = 'AnthropicToolModel'\n    \n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        schema_result = [AnthropicSchemaConverter().convert(tools[tool]) for tool in tools]\n        logging.info(schema_result)\n        return schema_result\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'tool_call_id', 'tool_calls']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n\n    def predict(self, \n        conversation, \n        toolkit=None, \n        tool_choice=None, \n        temperature=0.7, \n        max_tokens=1024):\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        client = anthropic.Anthropic(api_key=self.api_key)\n        if toolkit and not tool_choice:\n            tool_choice = {\"type\":\"auto\"}\n\n        tool_response = client.messages.create(\n            model=self.name,\n            messages=formatted_messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            tools=self._schema_convert_tools(toolkit.tools),\n            tool_choice=tool_choice,\n        )\n\n\n        logging.info(f\"tool_response: {tool_response}\")\n        tool_text_response = None\n        if tool_response.content[0].type =='text':\n            tool_text_response = tool_response.content[0].text\n            logging.info(f\"tool_text_response: {tool_text_response}\")\n\n        for tool_call in tool_response.content:\n            if tool_call.type == 'tool_use':\n                func_name = tool_call.name\n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = tool_call.input\n                func_result = func_call(**func_args)\n\n\n        if tool_text_response:\n            agent_response = f\"{tool_text_response} {func_result}\"\n        else:\n            agent_response = f\"{func_result}\"\n\n        agent_message = AgentMessage(content=agent_response)\n        conversation.add_message(agent_message)\n        logging.info(f\"conversation: {conversation}\")\n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/CohereModel.py",
        "content": "```swarmauri/standard/llms/concrete/CohereModel.py\nimport json\nimport logging\nfrom typing import List, Dict, Literal\nimport cohere\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass CohereModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['command-light',\n    'command', \n    'command-r',\n    'command-r-plus']\n    name: str = \"command-light\"\n    type: Literal['CohereModel'] = 'CohereModel'\n    \n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str,str]]:\n        \"\"\"\n        Cohere utilizes the following roles: CHATBOT, SYSTEM, TOOL, USER\n        \"\"\"\n        message_properties = ['content', 'role']\n\n        messages = [message.model_dump(include=message_properties) for message in messages]\n        for message in messages:\n            message['message'] = message.pop('content')\n            if message.get('role') == 'assistant':\n                message['role'] = 'chatbot'\n            message['role'] = message['role'].upper()\n        logging.info(messages)\n        return messages\n\n\n    def predict(self, \n        conversation, \n        temperature=0.7, \n        max_tokens=256):\n        # Get next message\n        next_message = conversation.history[-1].content\n\n        # Format chat_history\n        messages = self._format_messages(conversation.history[:-1])\n\n\n        client = cohere.Client(api_key=self.api_key)\n        response = client.chat(\n            model=self.name,\n            chat_history=messages,\n            message=next_message,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            prompt_truncation='OFF',\n            connectors=[]\n        )\n        \n        result = json.loads(response.json())\n        message_content = result['text']\n        conversation.add_message(AgentMessage(content=message_content))\n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/CohereToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/CohereToolModel.py\nimport logging\nimport json\nfrom typing import List, Literal\nfrom typing import List, Dict, Any, Literal\nimport cohere\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.CohereSchemaConverter import CohereSchemaConverter\n\nclass CohereToolModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['command-r',\n    'command-r-plus']\n    name: str = \"command-r\"\n    type: Literal['CohereToolModel'] = 'CohereToolModel'\n    \n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [CohereSchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'name', 'tool_call_id', 'tool_calls']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n\n    def predict(self, \n        conversation, \n        toolkit=None, \n        temperature=0.3,\n        max_tokens=1024):\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        client = cohere.Client(api_key=self.api_key)\n        preamble = \"\" # \u00f0\u0178\u0161\u00a7  Placeholder for implementation logic\n\n        logging.info(f\"_schema_convert_tools: {self._schema_convert_tools(toolkit.tools)}\")\n        logging.info(f\"message: {formatted_messages[-1]}\")\n        logging.info(f\"formatted_messages: {formatted_messages}\")\n\n        tool_response = client.chat(\n            model=self.name, \n            message=formatted_messages[-1]['content'], \n            chat_history=formatted_messages[:-1],\n            force_single_step=True,\n            tools=self._schema_convert_tools(toolkit.tools)\n        )\n\n        logging.info(f\"tool_response: {tool_response}\")\n        logging.info(tool_response.text) \n        tool_results = []\n        for tool_call in tool_response.tool_calls:\n            logging.info(f\"tool_call: {tool_call}\")\n            func_name = tool_call.name\n            func_call = toolkit.get_tool_by_name(func_name)\n            func_args = tool_call.parameters\n            func_results = func_call(**func_args)\n            tool_results.append({\"call\": tool_call, \"outputs\": [{'result': func_results}]}) # \u00f0\u0178\u0161\u00a7 Placeholder for variable key-names\n\n        logging.info(f\"tool_results: {tool_results}\")\n        agent_response = client.chat(\n            model=self.name,\n            message=formatted_messages[-1]['content'],\n            chat_history=formatted_messages[:-1],\n            tools=self._schema_convert_tools(toolkit.tools),\n            force_single_step=True,\n            tool_results=tool_results,\n            temperature=temperature\n        )\n\n        logging.info(f\"agent_response: {agent_response}\")\n        conversation.add_message(AgentMessage(content=agent_response.text))\n\n        logging.info(f\"conversation: {conversation}\")\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/GeminiProModel.py",
        "content": "```swarmauri/standard/llms/concrete/GeminiProModel.py\nimport json\nfrom typing import List, Dict, Literal\nimport google.generativeai as genai\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\n\nclass GeminiProModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['gemini-1.5-pro-latest']\n    name: str = \"gemini-1.5-pro-latest\"\n    type: Literal['GeminiProModel'] = 'GeminiProModel'\n    \n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        # Remove system instruction from messages\n        message_properties = ['content', 'role']\n        sanitized_messages = [message.model_dump(include=message_properties) for message in messages \n            if message.role != 'system']\n\n        for message in sanitized_messages:\n            if message['role'] == 'assistant':\n                message['role'] = 'model'\n\n            # update content naming\n            message['parts'] = message.pop('content')\n\n        return sanitized_messages\n\n    def _get_system_context(self, messages: List[SubclassUnion[MessageBase]]) -> str:\n        system_context = None\n        for message in messages:\n            if message.role == 'system':\n                system_context = message.content\n        return system_context\n    \n    def predict(self, \n        conversation, \n        temperature=0.7, \n        max_tokens=256):\n        genai.configure(api_key=self.api_key)\n        generation_config = {\n            \"temperature\": temperature,\n            \"top_p\": 0.95,\n            \"top_k\": 0,\n            \"max_output_tokens\": max_tokens,\n            }\n\n        safety_settings = [\n          {\n            \"category\": \"HARM_CATEGORY_HARASSMENT\",\n            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n          },\n          {\n            \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n          },\n          {\n            \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n          },\n          {\n            \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n          },\n        ]\n\n\n        system_context = self._get_system_context(conversation.history)\n        formatted_messages = self._format_messages(conversation.history)\n\n\n        next_message = formatted_messages.pop()\n\n        client = genai.GenerativeModel(model_name=self.name,\n            safety_settings=safety_settings,\n            generation_config=generation_config,\n            system_instruction=system_context)\n\n        convo = client.start_chat(\n            history=formatted_messages,\n            )\n\n        convo.send_message(next_message['parts'])\n\n        message_content = convo.last.text\n        conversation.add_message(AgentMessage(content=message_content))\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/GroqModel.py",
        "content": "```swarmauri/standard/llms/concrete/GroqModel.py\nimport json\nfrom typing import List, Optional, Dict, Literal\nfrom groq import Groq\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass GroqModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['llama3-8b-8192', \n    'llama3-70b-8192', \n    'mixtral-8x7b-32768', \n    'gemma-7b-it']\n    name: str = \"gemma-7b-it\"\n    type: Literal['GroqModel'] = 'GroqModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'name']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n\n    def predict(self, \n        conversation, \n        temperature: float = 0.7, \n        max_tokens: int = 256, \n        top_p: float = 1.0, \n        enable_json: bool = False, \n        stop: Optional[List[str]] = None) -> str:\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        client = Groq(api_key=self.api_key)\n        stop = stop or []\n        \n        response_format = {\"type\": \"json_object\"} if enable_json else None\n        response = client.chat.completions.create(\n            model=self.name,\n            messages=formatted_messages,\n            temperature=temperature,\n            response_format=response_format,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop\n        )\n        \n        result = json.loads(response.json())\n        message_content = result['choices'][0]['message']['content']\n        conversation.add_message(AgentMessage(content=message_content))\n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/MistralModel.py",
        "content": "```swarmauri/standard/llms/concrete/MistralModel.py\nimport json\nfrom typing import List, Literal, Dict\nfrom mistralai import Mistral \nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass MistralModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['open-mistral-7b', \n    'open-mixtral-8x7b', \n    'open-mixtral-8x22b', \n    'mistral-small-latest',\n    'mistral-medium-latest',\n    'mistral-large-latest',\n    'codestral', \n    'open-mistral-nemo', \n    'codestral-latest', \n    'open-codestral-mamba', \n    ]\n    name: str = \"open-mixtral-8x7b\"\n    type: Literal['MistralModel'] = 'MistralModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n\n    def predict(self, \n        conversation, \n        temperature: int = 0.7, \n        max_tokens: int = 256, \n        top_p: int = 1,\n        enable_json: bool=False, \n        safe_prompt: bool=False):\n        \n        formatted_messages = self._format_messages(conversation.history)\n\n        client =  Mistral(api_key=self.api_key)        \n        if enable_json:\n            response = client.chat.complete(\n                model=self.name,\n                messages=formatted_messages,\n                temperature=temperature,\n                response_format={ \"type\": \"json_object\" },\n                max_tokens=max_tokens,\n                top_p=top_p,\n                safe_prompt=safe_prompt\n            )\n        else:\n            response = client.chat.complete(\n                model=self.name,\n                messages=formatted_messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                top_p=top_p,                \n                safe_prompt=safe_prompt\n            )\n        \n        result = json.loads(response.json())\n        message_content = result['choices'][0]['message']['content']\n        conversation.add_message(AgentMessage(content=message_content))\n\n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/MistralToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/MistralToolModel.py\nimport json\nimport logging\nfrom typing import List, Literal, Dict, Any\nfrom mistralai import Mistral \nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.MistralSchemaConverter import MistralSchemaConverter\n\nclass MistralToolModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['open-mixtral-8x22b', \n    'mistral-small-latest',\n    'mistral-large-latest',\n    ]\n    name: str = \"open-mixtral-8x22b\"\n    type: Literal['MistralToolModel'] = 'MistralToolModel'\n    \n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [MistralSchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'tool_call_id']\n        #message_properties = ['content', 'role', 'tool_call_id', 'tool_calls']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n    \n    def predict(self, \n        conversation, \n        toolkit=None, \n        tool_choice=None, \n        temperature=0.7, \n        max_tokens=1024, \n        safe_prompt: bool = False):\n\n        client =  Mistral(api_key=self.api_key)\n        formatted_messages = self._format_messages(conversation.history)\n\n        if toolkit and not tool_choice:\n            tool_choice = \"auto\"\n            \n        tool_response = client.chat.complete(\n            model=self.name,\n            messages=formatted_messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            tools=self._schema_convert_tools(toolkit.tools),\n            tool_choice=tool_choice,\n            safe_prompt=safe_prompt\n        )\n\n        logging.info(f\"tool_response: {tool_response}\")\n\n        messages = [formatted_messages[-1], tool_response.choices[0].message]\n        tool_calls = tool_response.choices[0].message.tool_calls\n        if tool_calls:\n            for tool_call in tool_calls:\n                logging.info(type(tool_call.function.arguments))\n                logging.info(tool_call.function.arguments)\n                \n                func_name = tool_call.function.name\n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call.function.arguments)\n                func_result = func_call(**func_args)\n\n                messages.append(\n                    {\n                        \"tool_call_id\": tool_call.id,\n                        \"role\": \"tool\",\n                        \"name\": func_name,\n                        \"content\": func_result,\n                    }\n                )\n        logging.info(f\"messages: {messages}\")\n\n        agent_response = client.chat.complete(\n            model=self.name,\n            messages=messages\n        )\n        logging.info(f\"agent_response: {agent_response}\")\n        agent_message = AgentMessage(content=agent_response.choices[0].message.content)\n        conversation.add_message(agent_message)\n        logging.info(f\"conversation: {conversation}\")      \n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/OpenAIImageGenerator.py",
        "content": "```swarmauri/standard/llms/concrete/OpenAIImageGenerator.py\nimport json\nfrom typing import List, Literal\nfrom openai import OpenAI\nfrom swarmauri.core.llms.base.LLMBase import LLMBase\n\nclass OpenAIImageGenerator(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['dall-e']\n    name: str = \"dall-e\"\n    type: Literal['OpenAIImageGenerator'] = 'OpenAIImageGenerator'\n\n    def predict(self, \n        prompt: str, \n        size: str = \"1024x1024\", \n        quality: str = \"standard\", \n        n: int = 1) -> str:\n        \"\"\"\n        Generates an image based on the given prompt and other parameters.\n\n        Parameters:\n        - prompt (str): A description of the image you want to generate.\n        - **kwargs: Additional parameters that the image generation endpoint might use.\n\n        Returns:\n        - str: A URL or identifier for the generated image.\n        \"\"\"\n        try:\n            client =  OpenAI(api_key=self.api_key)\n            response = client.images.generate(\n                model=self.name,\n                prompt=prompt,\n                size=size,\n                quality=quality,\n                n=n\n            )\n            result = response.json()\n            return result\n        \n        except Exception as e:\n            return str(e)\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/OpenAIModel.py",
        "content": "```swarmauri/standard/llms/concrete/OpenAIModel.py\nimport json\nfrom typing import List, Dict, Literal\nfrom openai import OpenAI\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass OpenAIModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['gpt-4o', \n    'gpt-4o-2024-05-13',\n    'gpt-4-turbo', \n    'gpt-4-turbo-2024-04-09',\n    'gpt-4-turbo-preview',\n    'gpt-4-0125-preview',\n    'gpt-4-1106-preview',\n    'gpt-4',\n    'gpt-4-0613',\n    'gpt-4-32k',\n    'gpt-4-32k-0613',\n    'gpt-3.5-turbo-0125',\n    'gpt-3.5-turbo-1106',\n    'gpt-3.5-turbo-0613',\n    'gpt-3.5-turbo-16k-0613',\n    'gpt-3.5-turbo-16k',\n    'gpt-3.5-turbo']\n    name: str = \"gpt-3.5-turbo-16k\"\n    type: Literal['OpenAIModel'] = 'OpenAIModel'\n    \n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'name']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n    \n    def predict(self, \n        conversation, \n        temperature=0.7, \n        max_tokens=256, \n        enable_json=False, \n        stop: List[str] = None):\n        \"\"\"\n        Generate predictions using the OpenAI model.\n\n        Parameters:\n        - messages: Input data/messages for the model.\n        - temperature (float): Sampling temperature.\n        - max_tokens (int): Maximum number of tokens to generate.\n        - enable_json (bool): Format response as JSON.\n        \n        Returns:\n        - The generated message content.\n        \"\"\"\n        formatted_messages = self._format_messages(conversation.history)\n        client = OpenAI(api_key=self.api_key)\n        \n        if enable_json:\n            response = client.chat.completions.create(\n                model=self.name,\n                messages=formatted_messages,\n                temperature=temperature,\n                response_format={ \"type\": \"json_object\" },\n                max_tokens=max_tokens,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=stop\n            )\n        else:\n            response = client.chat.completions.create(\n                model=self.name,\n                messages=formatted_messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=stop\n            )\n        \n        result = json.loads(response.model_dump_json())\n        message_content = result['choices'][0]['message']['content']\n        conversation.add_message(AgentMessage(content=message_content))\n        \n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/OpenAIToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/OpenAIToolModel.py\nimport json\nimport logging\nfrom typing import List, Literal, Dict, Any\nfrom openai import OpenAI\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.OpenAISchemaConverter import OpenAISchemaConverter\n\nclass OpenAIToolModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['gpt-4o', \n    'gpt-4o-2024-05-13',\n    'gpt-4-turbo', \n    'gpt-4-turbo-2024-04-09',\n    'gpt-4-turbo-preview',\n    'gpt-4-0125-preview',\n    'gpt-4-1106-preview',\n    'gpt-4',\n    'gpt-4-0613',\n    'gpt-3.5-turbo',\n    'gpt-3.5-turbo-0125',\n    'gpt-3.5-turbo-1106',\n    'gpt-3.5-turbo-0613']\n    name: str = \"gpt-3.5-turbo-0125\"\n    type: Literal['OpenAIToolModel'] = 'OpenAIToolModel'\n    \n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [OpenAISchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'name', 'tool_call_id', 'tool_calls']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n    \n    def predict(self, \n        conversation, \n        toolkit=None, \n        tool_choice=None, \n        temperature=0.7, \n        max_tokens=1024):\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        client = OpenAI(api_key=self.api_key)\n        if toolkit and not tool_choice:\n            tool_choice = \"auto\"\n\n        tool_response = client.chat.completions.create(\n            model=self.name,\n            messages=formatted_messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            tools=self._schema_convert_tools(toolkit.tools),\n            tool_choice=tool_choice,\n        )\n        logging.info(f\"tool_response: {tool_response}\")\n        messages = [formatted_messages[-1], tool_response.choices[0].message]\n        tool_calls = tool_response.choices[0].message.tool_calls\n        if tool_calls:\n            for tool_call in tool_calls:\n                func_name = tool_call.function.name\n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call.function.arguments)\n                func_result = func_call(**func_args)\n                messages.append(\n                    {\n                        \"tool_call_id\": tool_call.id,\n                        \"role\": \"tool\",\n                        \"name\": func_name,\n                        \"content\": func_result,\n                    }\n                )\n        logging.info(f'messages: {messages}')\n        agent_response = client.chat.completions.create(\n            model=self.name,\n            messages=messages,\n            max_tokens=max_tokens,\n            temperature=temperature\n        )\n        logging.info(f\"agent_response: {agent_response}\")\n        agent_message = AgentMessage(\n            content=agent_response.choices[0].message.content\n        )\n        conversation.add_message(agent_message)\n        logging.info(f\"conversation: {conversation}\")\n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/__init__.py",
        "content": "```swarmauri/standard/llms/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/AI21StudioModel.py",
        "content": "```swarmauri/standard/llms/concrete/AI21StudioModel.py\nimport logging\nimport json\nfrom typing import List, Dict, Literal\nimport ai21\nfrom ai21.models.chat import ChatMessage\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\n\nclass AI21StudioModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n        \"jamba-instruct\",\n        \"jamba-instruct-preview\",\n        \"jamba-next\",\n        \"jamba-large-next\",\n        \"jamba-large-next-2\",\n        \"jamba-1.5\",\n        \"jamba-1.5-large\",\n    ]\n    name: str = \"jamba-instruct\"\n    type: Literal[\"AI21StudioModel\"] = \"AI21StudioModel\"\n\n    def _format_messages(\n        self, messages: List[SubclassUnion[MessageBase]]\n    ) -> List[Dict[str, str]]:\n        # Get only the properties that we require\n        message_properties = [\"content\", \"role\"]\n\n        # Exclude FunctionMessages\n        formatted_messages = [\n            ChatMessage(content=message.content, role=message.role)\n            for message in messages\n        ]\n        return formatted_messages\n\n\n\n    def predict(\n        self,\n        conversation,\n        temperature=0.7,\n        max_tokens=256,\n        top_p=1.0,\n        stop=\"\\n\",\n        n=1,\n        stream=False,\n    ):\n        \"\"\"\n        Args:\n            top_p (0.01 to 1.0)  Limit the pool of next tokens in each step to the top {top_p*100} percentile of possible tokens\n            stop: stop the message when model generates one of these strings.\n            n: how many chat responses to generate\n            stream: whether or not to stream the result, for long answers. if set to true n must be 1.\n        \"\"\"\n\n        # Create client\n        client = ai21.AI21Client(api_key=self.api_key)\n\n        # Get system_context from last message with system context in it\n        formatted_messages = self._format_messages(conversation.history)\n\n        parameters = {\n            \"model\": self.name,\n            \"messages\": formatted_messages,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"top_p\": top_p,\n            \"stop\": stop,\n            \"n\": n,\n            \"stream\": stream,\n        }\n\n        response = client.chat.completions.create(**parameters)\n        logging.info(f\"response: {response}\")\n\n        message_content = response.choices[0].message.content\n        conversation.add_message(AgentMessage(content=message_content))\n\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/DeepSeekModel.py",
        "content": "```swarmauri/standard/llms/concrete/DeepSeekModel.py\nimport json\nfrom typing import List, Dict, Literal\nimport openai \nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass DeepSeekModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['deepseek-chat', \n                                 'deepseek-coder', \n                                ]\n    name: str = \"deepseek-chat\"\n    type: Literal['DeepSeekModel'] = 'DeepSeekModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n       # Get only the properties that we require\n        message_properties = [\"content\", \"role\"]\n\n        # Exclude FunctionMessages\n        formatted_messages = [message.model_dump(include=message_properties) for message in messages]\n        return formatted_messages\n\n    def predict(self, \n        conversation, \n        temperature=0.7, \n        max_tokens=256, \n        frequency_penalty=0, \n        presence_penalty=0, \n        stop='\\n', \n        stream=False, \n        top_p=1.0): \n\n        # Create client\n        client = openai.OpenAI(api_key=self.api_key, base_url=\"https://api.deepseek.com\")\n        \n        # Get system_context from last message with system context in it\n        formatted_messages = self._format_messages(conversation.history)\n\n        response = client.chat.completions.create(\n            model=self.name,\n            messages=formatted_messages,\n            temperature=temperature,\n            max_tokens=max_tokens, \n            frequency_penalty=frequency_penalty,  \n            presence_penalty=presence_penalty, \n            stop=stop, \n            stream=stream, \n            top_p=top_p, \n        )\n        \n        message_content = response.choices[0].message.content\n        conversation.add_message(AgentMessage(content=message_content))\n        \n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/GeminiToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/GeminiToolModel.py\nimport logging\nimport json\nfrom typing import List, Literal, Dict, Any\nimport google.generativeai as genai\nfrom google.generativeai.protos import FunctionDeclaration\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.GeminiSchemaConverter import (\n    GeminiSchemaConverter,\n)\nimport google.generativeai as genai\n\n\nclass GeminiToolModel(LLMBase):\n    \"\"\"\n    3rd Party's Resources: https://ai.google.dev/api/python/google/generativeai/protos/\n    \"\"\"\n\n    api_key: str\n    allowed_models: List[str] = [\"gemini-1.5-pro-latest\"]\n    name: str = \"gemini-1.5-pro-latest\"\n    type: Literal[\"GeminiToolModel\"] = \"GeminiToolModel\"\n\n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        response = [GeminiSchemaConverter().convert(tools[tool]) for tool in tools]\n        logging.info(response)\n        return self._format_tools(response)\n\n    def _format_tools(\n        self, tools: List[SubclassUnion[FunctionMessage]]\n    ) -> List[Dict[str, Any]]:\n        formatted_tool = []\n        for tool in tools:\n            for parameter in tool[\"parameters\"][\"properties\"]:\n                tool[\"parameters\"][\"properties\"][parameter] = genai.protos.Schema(\n                    **tool[\"parameters\"][\"properties\"][parameter]\n                )\n\n            tool[\"parameters\"] = genai.protos.Schema(**tool[\"parameters\"])\n\n            tool = FunctionDeclaration(**tool)\n            formatted_tool.append(tool)\n\n        return formatted_tool\n\n    def _format_messages(\n        self, messages: List[SubclassUnion[MessageBase]]\n    ) -> List[Dict[str, str]]:\n        # Remove system instruction from messages\n        message_properties = [\"content\", \"role\", \"tool_call_id\", \"tool_calls\"]\n        sanitized_messages = [\n            message.model_dump(include=message_properties, exclude_none=True)\n            for message in messages\n            if message.role != \"system\"\n        ]\n\n        for message in sanitized_messages:\n            if message[\"role\"] == \"assistant\":\n                message[\"role\"] = \"model\"\n\n            if message[\"role\"] == \"tool\":\n                message[\"role\"] == \"user\"\n\n            # update content naming\n            message[\"parts\"] = message.pop(\"content\")\n\n        return sanitized_messages\n\n    def predict(self, conversation, toolkit=None, temperature=0.7, max_tokens=256):\n        genai.configure(api_key=self.api_key)\n        generation_config = {\n            \"temperature\": temperature,\n            \"top_p\": 0.95,\n            \"top_k\": 0,\n            \"max_output_tokens\": max_tokens,\n        }\n\n        safety_settings = [\n            {\n                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n            },\n            {\n                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n            },\n            {\n                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n            },\n            {\n                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n            },\n        ]\n\n        tool_config = {\n            \"function_calling_config\": {\"mode\": \"ANY\"},\n        }\n\n        client = genai.GenerativeModel(\n            model_name=self.name,\n            safety_settings=safety_settings,\n            generation_config=generation_config,\n            tool_config=tool_config,\n        )\n\n        formatted_messages = self._format_messages(conversation.history)\n        tools = self._schema_convert_tools(toolkit.tools)\n\n        \n        logging.info(f'formatted_messages: {formatted_messages}')\n        logging.info(f'tools: {tools}')\n\n        tool_response = client.generate_content(\n            formatted_messages,\n            tools=tools,\n        )\n        logging.info(f\"tool_response: {tool_response}\")\n\n        formatted_messages.append(tool_response.candidates[0].content)\n\n        logging.info(\n            f\"tool_response.candidates[0].content: {tool_response.candidates[0].content}\"\n        )\n\n        tool_calls = tool_response.candidates[0].content.parts\n\n        tool_results = {}\n        for tool_call in tool_calls:\n            func_name = tool_call.function_call.name\n            func_args = tool_call.function_call.args\n            logging.info(f\"func_name: {func_name}\")\n            logging.info(f\"func_args: {func_args}\")\n\n            func_call = toolkit.get_tool_by_name(func_name)\n            func_result = func_call(**func_args)\n            logging.info(f\"func_result: {func_result}\")\n            tool_results[func_name] = func_result\n\n        formatted_messages.append(\n            genai.protos.Content(\n                role=\"function\",\n                parts=[\n                    genai.protos.Part(\n                        function_response=genai.protos.FunctionResponse(\n                            name=fn,\n                            response={\n                                \"result\": val,  # Return the API response to Gemini\n                            },\n                        )\n                    )\n                    for fn, val in tool_results.items()\n                ],\n            )\n        )\n\n        logging.info(f\"formatted_messages: {formatted_messages}\")\n\n        agent_response = client.generate_content(formatted_messages)\n\n        logging.info(f\"agent_response: {agent_response}\")\n        conversation.add_message(AgentMessage(content=agent_response.text))\n\n        logging.info(f\"conversation: {conversation}\")\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/GroqToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/GroqToolModel.py\nfrom groq import Groq\nimport json\nfrom typing import List, Literal, Dict, Any\nimport logging\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.GroqSchemaConverter import GroqSchemaConverter\n\nclass GroqToolModel(LLMBase):\n    \"\"\"\n    Provider Documentation: https://console.groq.com/docs/tool-use#models\n    \"\"\"\n    api_key: str\n    allowed_models: List[str] = ['llama3-8b-8192', \n    'llama3-70b-8192', \n    'mixtral-8x7b-32768', \n    'gemma-7b-it']\n    name: str = \"gemma-7b-it\"\n    type: Literal['GroqToolModel'] = 'GroqToolModel'\n    \n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [GroqSchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'name', 'tool_call_id', 'tool_calls']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n\n    def predict(self, \n        conversation, \n        toolkit=None, \n        tool_choice=None, \n        temperature=0.7, \n        max_tokens=1024):\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        client = Groq(api_key=self.api_key)\n        if toolkit and not tool_choice:\n            tool_choice = \"auto\"\n\n        tool_response = client.chat.completions.create(\n            model=self.name,\n            messages=formatted_messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            tools=self._schema_convert_tools(toolkit.tools),\n            tool_choice=tool_choice,\n        )\n        logging.info(tool_response)\n\n        agent_message = AgentMessage(content=tool_response.choices[0].message.content)\n        conversation.add_message(agent_message)\n\n\n        tool_calls = tool_response.choices[0].message.tool_calls\n        if tool_calls:\n            for tool_call in tool_calls:\n                func_name = tool_call.function.name\n                \n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call.function.arguments)\n                func_result = func_call(**func_args)\n                \n                func_message = FunctionMessage(content=func_result, \n                                               name=func_name, \n                                               tool_call_id=tool_call.id)\n                conversation.add_message(func_message)\n            \n        logging.info(conversation.history)\n        formatted_messages = self._format_messages(conversation.history)\n        agent_response = client.chat.completions.create(\n            model=self.name,\n            messages=formatted_messages,\n            max_tokens=max_tokens,\n            temperature=temperature\n        )\n        logging.info(agent_response)\n        agent_message = AgentMessage(content=agent_response.choices[0].message.content)\n        conversation.add_message(agent_message)\n        return conversation\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/PerplexityModel.py",
        "content": "```swarmauri/standard/llms/concrete/PerplexityModel.py\nimport json\nfrom typing import List, Dict, Literal, Optional\nimport requests\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\n\nclass PerplexityModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = ['llama-3-sonar-small-32k-chat',\n        'llama-3-sonar-small-32k-online',\n        'llama-3-sonar-large-32k-chat',\n        'llama-3-sonar-large-32k-online',\n        'llama-3-8b-instruct',\n        'llama-3-70b-instruct']\n    name: str = \"llama-3-70b-instruct\"\n    type: Literal['PerplexityModel'] = 'PerplexityModel'\n    \n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n        message_properties = ['content', 'role', 'name']\n        formatted_messages = [message.model_dump(include=message_properties, exclude_none=True) for message in messages]\n        return formatted_messages\n    \n    def predict(self, \n        conversation, \n        temperature=0.7, \n        max_tokens=256, \n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        return_citations: Optional[bool] = False,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None\n        ):\n\n\n        if top_p and top_k:\n            raise ValueError('Do not set top_p and top_k')\n\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        url = \"https://api.perplexity.ai/chat/completions\"\n\n        payload = {\n            \"model\": self.name,\n            \"messages\": formatted_messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"return_citations\": True,\n            \"top_k\": top_k,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty\n        }\n        headers = {\n            \"accept\": \"application/json\",\n            \"content-type\": \"application/json\",\n            \"authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        response = requests.post(url, json=payload, headers=headers)\n        message_content = response.text\n        conversation.add_message(AgentMessage(content=message_content))\n        return conversation\n\n\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/ShuttleAIModel.py",
        "content": "```swarmauri/standard/llms/concrete/ShuttleAIModel.py\nimport logging\nimport json\nfrom typing import List, Dict, Literal\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase \n\nimport requests \n\nclass ShuttleAIModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n    \"shuttle-2-turbo\", \"shuttle-turbo\", \"gpt-4o-2024-05-13\", \"gpt-4-turbo-2024-04-09\",\n    \"gpt-4-0125-preview\", \"gpt-4-1106-preview\", \"gpt-4-1106-vision-preview\", \"gpt-4-0613\",\n    \"gpt-4-bing\", \"gpt-4-turbo-bing\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-0125\",\n    \"gpt-3.5-turbo-1106\", \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\",\n    \"claude-2.1\", \"claude-2.0\", \"claude-instant-1.2\", \"claude-instant-1.1\",\n    \"claude-instant-1.0\", \"meta-llama-3-70b-instruct\", \"meta-llama-3-8b-instruct\", \"llama-3-sonar-large-32k-online\",\n    \"llama-3-sonar-small-32k-online\", \"llama-3-sonar-large-32k-chat\", \"llama-3-sonar-small-32k-chat\", \"blackbox\",\n    \"blackbox-code\", \"wizardlm-2-8x22b\", \"wizardlm-2-70b\", \"dolphin-2.6-mixtral-8x7b\",\n    \"codestral-latest\", \"mistral-large\", \"mistral-next\", \"mistral-medium\",\n    \"mistral-small\", \"mistral-tiny\", \"mixtral-8x7b-instruct-v0.1\", \"mixtral-8x22b-instruct-v0.1\",\n    \"mistral-7b-instruct-v0.2\", \"mistral-7b-instruct-v0.1\", \"nous-hermes-2-mixtral-8x7b\", \"gemini-1.5-pro-latest\",\n    \"gemini-1.0-pro-latest\", \"gemini-1.0-pro-vision\", \"lzlv-70b\", \"figgs-rp\", \"cinematika-7b\"\n    ]\n    name: str = \"shuttle-2-turbo\"\n    type: Literal['ShuttleAIModel'] = 'ShuttleAIModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n       # Get only the properties that we require\n        message_properties = [\"content\", \"role\"]\n\n        # Exclude FunctionMessages\n        formatted_messages = [\n            message.model_dump(include=message_properties, exclude_none=True)\n            for message in messages\n        ]\n        return formatted_messages\n\n    \n    def predict(self, \n            conversation, \n            temperature=0.7, \n            max_tokens=256, \n            top_p=1, \n            internet=False, \n            citations=False, \n            tone='precise', \n            raw=False, \n            image=None): \n\n            formatted_messages = self._format_messages(conversation.history) \n\n            url = \"https://api.shuttleai.app/v1/chat/completions\" \n            payload = { \n                \"model\": self.name, \n                \"messages\": formatted_messages, \n                \"max_tokens\": max_tokens, \n                \"temperature\": temperature, \n                \"top_p\": top_p\n            } \n\n            if raw:\n                payload['raw'] = True\n\n            if internet:\n                payload['internet'] = True\n\n            # Only include the 'image' field if it's not None\n            if image is not None:\n                payload[\"image\"] = image\n\n            if self.name in ['gpt-4-bing', 'gpt-4-turbo-bing']: \n                payload['tone'] = tone\n                \n                # Include citations only if citations is True\n                if citations:\n                    payload['citations'] = True\n\n            headers = { \n                \"Authorization\": f\"Bearer {self.api_key}\", \n                \"Content-Type\": \"application/json\", \n            }\n\n            # Log payload for debugging\n            logging.info(f\"Payload being sent: {payload}\")\n\n            # Send the request\n            response = requests.post(url, json=payload, headers=headers)\n\n            # Log response for debugging\n            logging.info(f\"Response received: {response.text}\")\n\n            # Parse response JSON safely\n            try:\n                message_content = response.json()['choices'][0]['message']['content']\n            except KeyError as e:\n                logging.info(f\"Error parsing response: {response.text}\")\n                raise e\n\n            conversation.add_message(AgentMessage(content=message_content))  \n            return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/llms/concrete/ShuttleAIToolModel.py",
        "content": "```swarmauri/standard/llms/concrete/ShuttleAIToolModel.py\nimport json\nimport logging\nfrom typing import List, Literal, Dict, Any\nimport requests\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.ShuttleAISchemaConverter import (\n    ShuttleAISchemaConverter,\n)\n\n\nclass ShuttleAIToolModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n        \"shuttle-2-turbo\",\n        \"gpt-4-turbo-2024-04-09\",\n        \"gpt-4-0125-preview\",\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0613\",\n        \"gpt-3.5-turbo-0125\",\n        \"gpt-3.5-turbo-1106\",\n        \"claude-instant-1.1\",\n        \"wizardlm-2-8x22b\",\n        \"mistral-7b-instruct-v0.2\",\n        \"gemini-1.5-pro-latest\",\n        \"gemini-1.0-pro-latest\",\n    ]\n    name: str = \"shuttle-2-turbo\"\n    type: Literal[\"ShuttleAIToolModel\"] = \"ShuttleAIToolModel\"\n\n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [ShuttleAISchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(\n        self, messages: List[SubclassUnion[MessageBase]]\n    ) -> List[Dict[str, str]]:\n        message_properties = [\"content\", \"role\", \"name\", \"tool_call_id\", \"tool_calls\"]\n        formatted_messages = [\n            message.model_dump(include=message_properties, exclude_none=True)\n            for message in messages\n        ]\n        return formatted_messages\n\n    def predict(\n        self,\n        conversation,\n        toolkit=None,\n        tool_choice=\"auto\",\n        temperature=0.7,\n        max_tokens=1024,\n        top_p=1.0,\n        internet=False,\n        raw=False,\n        image=None,\n        citations=False,\n        tone=\"precise\",\n    ):\n        formatted_messages = self._format_messages(conversation.history)\n\n        if toolkit and not tool_choice:\n            tool_choice = \"auto\"\n\n        url = \"https://api.shuttleai.app/v1/chat/completions\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        payload = {\n            \"model\": self.name,\n            \"messages\": formatted_messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"tool_choice\": tool_choice,\n            \"tools\": self._schema_convert_tools(toolkit.tools),\n        }\n\n        if raw:\n            payload[\"raw\"] = True\n\n        if internet:\n            payload[\"internet\"] = True\n\n        if image is not None:\n            payload[\"image\"] = image\n\n        if self.name in [\"gpt-4-bing\", \"gpt-4-turbo-bing\"]:\n            payload[\"tone\"] = tone\n            # Include citations only if citations is True\n            if citations:\n                payload['citations'] = True\n\n\n        logging.info(f\"tool payload: {payload}\")\n        \n        # First we ask agent to give us a response\n        agent_response = requests.request(\"POST\", url, json=payload, headers=headers)\n\n        logging.info(f\"tool agent response {agent_response.json()}\")\n\n        try:\n            messages = [\n                formatted_messages[-1],\n                agent_response.json()[\"choices\"][0][\"message\"][\"content\"],\n            ]\n        except Exception as error:\n            logging.warn(error)\n\n        tool_calls = agent_response.json()[\"choices\"][0][\"message\"].get(\n            \"tool_calls\", None\n        )\n\n\n        # If agent responds with tool call, then we execute the functions\n        if tool_calls:\n            for tool_call in tool_calls:\n                func_name = tool_call[\"function\"][\"name\"]\n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call[\"function\"][\"arguments\"])\n                func_result = func_call(**func_args)\n                payload['messages'].append(\n                    {\n                        \"tool_call_id\": tool_call['id'],\n                        \"role\": \"tool\",\n                        \"name\": func_name,\n                        \"content\": func_result,\n                    }\n                )\n\n        # Remove tools for payload\n        del payload['tools']\n        del payload['tool_choice']\n\n        logging.info(f\"payload['messages']: {payload['messages']}\")\n        logging.info(f\"final payload: {payload}\")\n\n        agent_response = requests.request(\"POST\", url, json=payload, headers=headers)\n        logging.info(f\"final agent response: {agent_response.json()}\")\n        \n        agent_message = AgentMessage(\n            content=agent_response.json()[\"choices\"][0][\"message\"][\"content\"]\n        )\n\n        conversation.add_message(agent_message)\n        logging.info(f\"conversation: {conversation.history}\")\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/__init__.py",
        "content": "```swarmauri/standard/schema_converters/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/base/SchemaConverterBase.py",
        "content": "```swarmauri/standard/schema_converters/base/SchemaConverterBase.py\nfrom abc import abstractmethod\nfrom typing import Optional, Dict, Any, Literal\nfrom pydantic import ConfigDict, Field\nfrom swarmauri.core.ComponentBase import ComponentBase, ResourceTypes\nfrom swarmauri.core.schema_converters.ISchemaConvert import ISchemaConvert\nfrom swarmauri.core.tools.ITool import ITool\n\nclass SchemaConverterBase(ISchemaConvert, ComponentBase):\n    model_config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n    resource: Optional[str] =  Field(default=ResourceTypes.SCHEMA_CONVERTER.value, frozen=True)\n    type: Literal['SchemaConverterBase'] = 'SchemaConverterBase'\n\n    @abstractmethod\n    def convert(self, tool: ITool) -> Dict[str, Any]:\n        raise NotImplementedError(\"Subclasses must implement the convert method.\")\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/base/__init__.py",
        "content": "```swarmauri/standard/schema_converters/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/AnthropicSchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/AnthropicSchemaConverter.py\nfrom typing import  Dict, Any, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import SchemaConverterBase\n\nclass AnthropicSchemaConverter(SchemaConverterBase):\n    type: Literal['AnthropicSchemaConverter'] = 'AnthropicSchemaConverter'\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n        required = []\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"type\": param.type,\n                \"description\": param.description,\n            }\n            if param.required:\n                required.append(param.name)\n\n        return {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": properties,\n                \"required\": required,\n            }\n        }\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/CohereSchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/CohereSchemaConverter.py\nfrom typing import Dict, Any, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import SchemaConverterBase\n\nclass CohereSchemaConverter(SchemaConverterBase):\n    type: Literal['CohereSchemaConverter'] = 'CohereSchemaConverter'\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"description\": param.description,\n                \"required\": param.required\n            }\n            if param.type == 'string':\n                _type = 'str'\n            elif param.type == 'float':\n                _type = 'float'\n            elif param.type == 'integer':\n                _type = 'int'\n            elif param.type == 'boolean':\n                _type = 'bool'\n            else:\n                raise NotImplementedError(f'\u00f0\u0178\u0161\u00a7 Support for missing type pending https://docs.cohere.com/docs/parameter-types-in-tool-use\\n: Missing Type: {param.type}')\n            properties[param.name].update({'type': _type})\n\n        return {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"parameter_definitions\": properties\n        }\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/GroqSchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/GroqSchemaConverter.py\nfrom typing import  Dict, Any, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import SchemaConverterBase\n\nclass GroqSchemaConverter(SchemaConverterBase):\n    type: Literal['GroqSchemaConverter'] = 'GroqSchemaConverter'\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n        required = []\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"type\": param.type,\n                \"description\": param.description,\n            }\n            if param.enum:\n                properties[param.name]['enum'] = param.enum\n\n            if param.required:\n                required.append(param.name)\n\n        function = {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": properties,\n                }\n            }\n        }\n        if required:\n            function['function']['parameters']['required'] = required\n\n        return function\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/MistralSchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/MistralSchemaConverter.py\nfrom typing import Dict, Any, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import SchemaConverterBase\n\nclass MistralSchemaConverter(SchemaConverterBase):\n    type: Literal['MistralSchemaConverter'] = 'MistralSchemaConverter'\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n        required = []\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"type\": param.type,\n                \"description\": param.description,\n            }\n            if param.enum:\n                properties[param.name]['enum'] = param.enum\n\n            if param.required:\n                required.append(param.name)\n\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": properties,\n                    \"required\": required,\n                }\n            }\n        }\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/OpenAISchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/OpenAISchemaConverter.py\nfrom typing import  Dict, Any, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import SchemaConverterBase\n\nclass OpenAISchemaConverter(SchemaConverterBase):\n    type: Literal['OpenAISchemaConverter'] = 'OpenAISchemaConverter'\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n        required = []\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"type\": param.type,\n                \"description\": param.description,\n            }\n            if param.enum:\n                properties[param.name]['enum'] = param.enum\n\n            if param.required:\n                required.append(param.name)\n\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": properties,\n                    \"required\": required,\n                }\n            }\n        }\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/ShuttleAISchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/ShuttleAISchemaConverter.py\nfrom typing import  Dict, Any, Literal\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import SchemaConverterBase\n\nclass ShuttleAISchemaConverter(SchemaConverterBase):\n    type: Literal['ShuttleAISchemaConverter'] = 'ShuttleAISchemaConverter'\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n        required = []\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"type\": param.type,\n                \"description\": param.description,\n            }\n            if param.enum:\n                properties[param.name]['enum'] = param.enum\n\n            if param.required:\n                required.append(param.name)\n\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": properties,\n                    \"required\": required,\n                }\n            }\n        }\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/__init__.py",
        "content": "```swarmauri/standard/schema_converters/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/schema_converters/concrete/GeminiSchemaConverter.py",
        "content": "```swarmauri/standard/schema_converters/concrete/GeminiSchemaConverter.py\nfrom typing import Dict, Any, Literal\nimport google.generativeai as genai\nfrom swarmauri.core.typing import SubclassUnion\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.schema_converters.base.SchemaConverterBase import (\n    SchemaConverterBase,\n)\n\n\nclass GeminiSchemaConverter(SchemaConverterBase):\n    type: Literal[\"GeminiSchemaConverter\"] = \"GeminiSchemaConverter\"\n\n    def convert(self, tool: SubclassUnion[ToolBase]) -> Dict[str, Any]:\n        properties = {}\n        required = []\n\n        for param in tool.parameters:\n            properties[param.name] = {\n                \"type\": self.convert_type(param.type),\n                \"description\": param.description,\n            }\n            if param.required:\n                required.append(param.name)\n\n        schema = {\n            \"type\": genai.protos.Type.OBJECT,\n            \"properties\": properties,\n            \"required\": required,\n        }\n\n        function_declaration = {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"parameters\": schema,\n        }\n\n        return function_declaration\n\n    def convert_type(self, param_type: str) -> str:\n        type_mapping = {\n            \"string\": genai.protos.Type.STRING,\n            \"str\": genai.protos.Type.STRING,\n            \"integer\": genai.protos.Type.INTEGER,\n            \"int\": genai.protos.Type.INTEGER,\n            \"boolean\": genai.protos.Type.BOOLEAN,\n            \"bool\": genai.protos.Type.BOOLEAN,\n            \"array\": genai.protos.Type.ARRAY,\n            \"object\": genai.protos.Type.OBJECT,\n        }\n        return type_mapping.get(param_type, \"string\")\n\n```"
    }
]