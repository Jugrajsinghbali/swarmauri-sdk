[
    {
        "document_name": "swarmauri/community/__init__.py",
        "content": "```swarmauri/community/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/__init__.py",
        "content": "```swarmauri/community/tools/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/base/__init__.py",
        "content": "```swarmauri/community/tools/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/__init__.py",
        "content": "```swarmauri/community/tools/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/GmailSendTool.py",
        "content": "```swarmauri/community/tools/concrete/GmailSendTool.py\nimport base64\nimport json\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom googleapiclient import discovery\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass GmailSendTool(ToolBase):\n    SCOPES = ['https://www.googleapis.com/auth/gmail.send']\n\n    def __init__(self, credentials_path: str, sender_email: str):\n        \"\"\"\n        Initializes the GmailSendTool with a path to the credentials JSON file and the sender email.\n\n        Parameters:\n        credentials_path (str): The path to the Gmail service JSON file.\n        sender_email (str): The email address being used to send emails.\n        \"\"\"\n        \n        parameters = [\n            Parameter(\n                name=\"recipients\",\n                type=\"string\",\n                description=\"The email addresses of the recipients, separated by commas\",\n                required=True\n            ),\n            Parameter(\n                name=\"subject\",\n                type=\"string\",\n                description=\"The subject of the email\",\n                required=True\n            ),\n            Parameter(\n                name=\"htmlMsg\",\n                type=\"string\",\n                description=\"The HTML message to be sent as the email body\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"GmailSendTool\", \n                         description=\"Sends an email using the Gmail API.\",\n                         parameters=parameters)\n        self.credentials_path = credentials_path\n        self.sender_email = sender_email\n        \n\n    def authenticate(self):\n        \"\"\"\n        Authenticates the user and creates a Gmail API service for sending emails.\n        \"\"\"\n        credentials = service_account.Credentials.from_service_account_file(\n                self.credentials_path, scopes=self.SCOPES)\n        \n        delegated_credentials = credentials.with_subject(self.sender_email)\n        self.service = build('gmail', 'v1', credentials=delegated_credentials)\n\n    def create_message(self, to: str, subject: str, message_text: str):\n        \"\"\"\n        Create a MIMEText message for sending an email.\n\n        Parameters:\n        sender (str): The email address of the sender.\n        to (str): The email address of the recipient.\n        subject (str): The subject of the email.\n        message_text (str): The HTML body of the email.\n\n        Returns:\n        The created MIMEText message.\n        \"\"\"\n        message = MIMEMultipart('alternative')\n        message['from'] = self.sender_email\n        message['to'] = to\n        message['subject'] = subject\n        mime_text = MIMEText(message_text, 'html')\n        message.attach(mime_text)\n        raw_message = base64.urlsafe_b64encode(message.as_string().encode('utf-8'))\n        return {'raw': raw_message.decode('utf-8')}\n\n    def __call__(self, recipients, subject, htmlMsg):\n        \"\"\"\n        Sends an email to the specified recipients with the given subject and HTML message.\n        \n        Parameters:\n        sender (str): The email address of the sender.\n        recipients (str): The email address of the recipients, separated by commas.\n        subject (str): The subject of the email.\n        htmlMsg (str): The HTML content of the email body.\n\n        Returns:\n        The result of sending the email or an error message if the operation fails.\n        \"\"\"\n        self.authenticate()\n        try:\n            message = self.create_message(recipients, subject, htmlMsg)\n            sent_message = (self.service.users().messages().send(userId='me', body=message).execute())\n            return f\"Email sent successfully to {recipients}\"\n\n        except Exception as e:\n            return f\"An error occurred in sending the email: {e}\"\n        finally:\n            del self.service\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/ZapierHookTool.py",
        "content": "```swarmauri/community/tools/concrete/ZapierHookTool.py\nimport json\nimport requests\nfrom typing import Dict\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass ZapierHookTool(ToolBase):\n    def __init__(self, auth_token, zap_id):\n        parameters = [\n            Parameter(\n                name=\"payload\",\n                type=\"string\",\n                description=\"A Payload to send when triggering the Zapier webhook\",\n                required=True\n            )\n        ]\n        super().__init__(name=\"ZapierTool\", \n                         description=\"Tool to authenticate with Zapier and execute zaps.\", \n                        parameters=parameters)\n        self._auth_token = auth_token\n        self._zap_id = zap_id\n\n    def authenticate(self):\n        \"\"\"Set up the necessary headers for authentication.\"\"\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {self._auth_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def execute_zap(self, payload: str):\n        \"\"\"Execute a zap with given payload.\n\n        Args:\n            zap_id (str): The unique identifier for the Zap to trigger.\n            payload (dict): The data payload to send to the Zap.\n\n        Returns:\n            dict: The response from Zapier API.\n        \"\"\"\n        self.authenticate()\n        response = requests.post(f'https://hooks.zapier.com/hooks/catch/{self._zap_id}/',\n                                     headers=self.headers, json={\"data\":payload})\n        # Checking the HTTP response status for success or failure\n        if response.status_code == 200:\n            return json.dumps(response.json())\n        else:\n            response.raise_for_status()  # This will raise an error for non-200 responses\n\n    def __call__(self, payload: str):\n        \"\"\"Enable the tool to be called with zap_id and payload directly.\"\"\"\n        return self.execute_zap(payload)\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/DownloadPdfTool.py",
        "content": "```swarmauri/community/tools/concrete/DownloadPdfTool.py\nimport requests\nfrom typing import Dict\nfrom pathlib import Path\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass DownloadPDFTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"The URL of the PDF file to download\",\n                required=True\n            ),\n            Parameter(\n                name=\"destination\",\n                type=\"string\",\n                description=\"The path where the PDF file will be saved\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"DownloadPDFTool\",\n                         description=\"Downloads a PDF from a specified URL and saves it to a specified path.\",\n                         parameters=parameters)\n\n    def __call__(self, url: str, destination: str) -> Dict[str, str]:\n        \"\"\"\n        Download the PDF from the specified URL and saves it to the given destination path.\n\n        Parameters:\n        - url (str): The URL from where to download the PDF.\n        - destination (str): The local file path where the PDF should be saved.\n        \n        Returns:\n        - Dict[str, str]: A dictionary containing the result message and the destination path.\n        \"\"\"\n        try:\n            # Send a GET request to the specified URL\n            response = requests.get(url, stream=True)\n\n            # Raise an HTTPError if the status code is not 200 (OK)\n            response.raise_for_status()\n\n            # Ensure destination directory exists\n            Path(destination).parent.mkdir(parents=True, exist_ok=True)\n\n            # Open a file at the specified destination path and write the content of the response to it\n            with open(Path(destination), 'wb') as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n            \n            return {\n                \"message\": \"PDF downloaded successfully.\",\n                \"destination\": destination\n            }\n\n        except requests.exceptions.RequestException as e:\n            # Handle requests-related errors\n            return {\"error\": f\"Failed to download PDF: {e}\"}\n        except IOError as e:\n            # Handle file I/O errors\n            return {\"error\": f\"Failed to save PDF: {e}\"}\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/EntityRecognitionTool.py",
        "content": "```swarmauri/community/tools/concrete/EntityRecognitionTool.py\nimport json\nfrom transformers import pipeline, logging as hf_logging\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nhf_logging.set_verbosity_error()\n\nclass EntityRecognitionTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\"text\",\"string\",\"The text for entity recognition\",True)\n        ]\n        super().__init__(name=\"EntityRecognitionTool\", \n                         description=\"Extracts named entities from text\", \n                         parameters=parameters)\n        \n\n    def __call__(self, text: str) -> dict:\n        try:\n            self.nlp = pipeline(\"ner\")\n            entities = self.nlp(text)\n            organized_entities = {}\n            for entity in entities:\n                if entity['entity'] not in organized_entities:\n                    organized_entities[entity['entity']] = []\n                organized_entities[entity['entity']].append(entity['word'])\n            return json.dumps(organized_entities)\n        except Exception as e:\n            raise e\n        finally:\n            del self.nlp\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/GmailReadTool.py",
        "content": "```swarmauri/community/tools/concrete/GmailReadTool.py\nimport base64\nimport json\nfrom googleapiclient import discovery\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass GmailReadTool(ToolBase):\n    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n\n    def __init__(self, credentials_path: str, sender_email: str):\n        \"\"\"\n        Initializes the GmailReadTool with a path to the credentials JSON file.\n\n        Parameters:\n        credentials_path (str): The path to the Gmail service JSON file.\n        \"\"\"\n        \n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description='''The query to filter emails. For example, \"is:unread\" or \"from:example@gmail.com\" or \"from:sender@company.com\"''',\n                required=True\n            ),\n            Parameter(\n                name=\"max_results\",\n                type=\"integer\",\n                description='''The number of emails to return. Defaults to 10.'''\n            )\n        ]\n        \n        \n        super().__init__(name=\"GmailReadTool\", \n                         description=\"Read emails from a Gmail account.\", \n                         parameters = parameters)\n        self.credentials_path = credentials_path\n        self.sender_email = sender_email\n        \n\n    def authenticate(self):\n        \"\"\"\n        Authenticates the user and creates a Gmail API service.\n        \"\"\"\n        credentials = service_account.Credentials.from_service_account_file(\n                self.credentials_path, scopes=self.SCOPES)\n        \n        delegated_credentials = credentials.with_subject(self.sender_email)\n        self.service = discovery.build('gmail', 'v1', credentials=delegated_credentials)\n\n\n\n    def __call__(self, query='', max_results=10):\n        \"\"\"\n        Fetches emails from the authenticated Gmail account based on the given query.\n\n        Parameters:\n        query (str): The query to filter emails. For example, \"is:unread\".\n        max_results (int): The maximum number of email messages to fetch.\n\n        Returns:\n        list: A list of email messages.\n        \"\"\"\n        self.authenticate()\n        try:\n            # Call the Gmail API\n            \n            gmail_messages = self.service.users().messages()\n            results = gmail_messages.list(userId='me', q=query, maxResults=max_results).execute()\n            messages = results.get('messages', [])\n            message_data = \"\"\n            for message in messages:\n                \n                msg = gmail_messages.get(userId='me', id=message['threadId'], format=\"full\").execute()\n                headers = msg['payload']['headers']\n                \n                sender = next(header['value'] for header in headers if header['name'] == 'From')\n                subject = next(header['value'] for header in headers if header['name'] == 'Subject')\n                reply_to = next((header['value'] for header in headers if header['name'] == 'Reply-To'), subject)\n                date_time = next(header['value'] for header in headers if header['name'] == 'Date')\n                \n                #part = msg['payload']['parts'][0]\n                #data = part['body']['data']\n                #decoded_data = base64.urlsafe_b64decode(data.encode('ASCII'))\n\n                formatted_msg = f\"\\nsender:{sender} reply-to:{reply_to} subject: {subject} date_time:{date_time}\"\n                \n                message_data += formatted_msg\n                \n            \n            return message_data\n        \n        \n        \n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return []\n        \n        finally:\n            del self.service\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/PaCMAP.py",
        "content": "```swarmauri/community/tools/concrete/PaCMAP.py\nimport numpy as np\nimport pacmap  # Ensure pacmap is installed\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\nclass PaCMAPTool(ToolBase):\n    \"\"\"\n    A tool for applying the PaCMAP method for dimensionality reduction.\n    \"\"\"\n\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"X\",\n                type=\"object\",\n                description=\"X (np.ndarray): The high-dimensional data points to reduce.\",\n                required=True\n            ),\n            Parameter(\n                name=\"n_neighbors\",\n                type=\"integer\",\n                description=\"The size of local neighborhood (in terms of number of neighboring data points) used for manifold approximation.\",\n                required=False\n            ),\n            Parameter(\n                name=\"n_components\",\n                type=\"integer\",\n                description=\"The dimension of the space into which to embed the data.\",\n                required=True\n            ),\n            Parameter(\n                name=\"n_iterations\",\n                type=\"integer\",\n                description=\"The number of iterations used for optimization.\",\n                required=False\n            )\n        ]\n        \n        super().__init__(name=\"PaCMAPTool\", \n                         description=\"Applies PaCMAP for dimensionality reduction.\", \n                         parameters=parameters)\n\n    def __call__(self, **kwargs) -> np.ndarray:\n        \"\"\"\n        Applies the PaCMAP algorithm on the provided dataset.\n\n        Parameters:\n        - kwargs: Additional keyword arguments for the PaCMAP algorithm.\n\n        Returns:\n        - np.ndarray: The reduced dimension data points.\n        \"\"\"\n        # Set default values for any unspecified parameters\n        X = kwargs.get('X')\n        n_neighbors = kwargs.get('n_neighbors', 30)\n        n_components = kwargs.get('n_components', 2)\n        n_iterations = kwargs.get('n_iterations', 500)\n        \n        # Instantiate the PaCMAP instance with specified parameters\n        embedder = pacmap.PaCMAP(n_neighbors=n_neighbors, n_components=n_components, \n                                 n_iters=n_iterations, **kwargs)\n                                 \n        # Fit the model and transform the data\n        X_reduced = embedder.fit_transform(X)\n\n        return X_reduced\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/SentimentAnalysisTool.py",
        "content": "```swarmauri/community/tools/concrete/SentimentAnalysisTool.py\nfrom transformers import pipeline\nfrom transformers import logging as hf_logging\n\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nhf_logging.set_verbosity_error()\n\nclass SentimentAnalysisTool(ToolBase):\n    def __init__(self):\n        super().__init__(\"SentimentAnalysisTool\", \n                         \"Analyzes the sentiment of the given text.\", \n                         parameters=[\n                             Parameter(\"text\", \"string\", \"The text for sentiment analysis\", True)\n                         ])\n        \n\n    def __call__(self, text: str) -> str:\n        try:\n            self.analyzer = pipeline(\"sentiment-analysis\")\n            result = self.analyzer(text)\n            return result[0]['label']\n        except:\n            raise\n        finally:\n            del self.analyzer\n```"
    },
    {
        "document_name": "swarmauri/community/tools/concrete/WebScrapingTool.py",
        "content": "```swarmauri/community/tools/concrete/WebScrapingTool.py\nimport requests\nfrom bs4 import BeautifulSoup\nfrom swarmauri.standard.tools.base.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass WebScrapingTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"URL of the link, website, webpage, etc... to scrape\",\n                required=True\n            ),\n            Parameter(\n                name=\"selector\",\n                type=\"string\",\n                description=\"CSS selector to target specific elements\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"WebScrapingTool\", \n                         description=\"This is a web scraping tool that you can utilize to scrape links, websites, webpages, etc... This tool uses python's requests and BeautifulSoup libraries to parse a URL using a CSS to target specific elements.\", \n                         parameters=parameters)\n\n    def __call__(self, url: str, selector: str) -> str:\n        \"\"\"\n        Fetches content from the specified URL and extracts elements based on the provided CSS selector.\n        \n        Args:\n            url (str): The URL of the webpage to scrape.\n            selector (str): CSS selector to target specific elements in the webpage.\n\n        Returns:\n            str: Extracted text from the selector or an error message.\n        \"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raises HTTPError for bad requests (4xx or 5xx)\n\n            html_content = response.content\n            soup = BeautifulSoup(html_content, 'html.parser')\n\n            elements = soup.select(selector)\n            extracted_text = '\\n'.join([element.text for element in elements])\n            return extracted_text\n        except requests.RequestException as e:\n            return f\"Request Exception: {str(e)}\"\n        except Exception as e:\n            return f\"Unexpected error: {str(e)}\"\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/__init__.py",
        "content": "```swarmauri/community/retrievers/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/base/__init__.py",
        "content": "```swarmauri/community/retrievers/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/concrete/__init__.py",
        "content": "```swarmauri/community/retrievers/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/community/retrievers/concrete/RedisDocumentRetriever.py",
        "content": "```swarmauri/community/retrievers/concrete/RedisDocumentRetriever.py\nfrom typing import List\nfrom redisearch import Client, Query\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.document_stores.concrete.ConcreteDocument import ConcreteDocument\nfrom ....standard.retrievers.base.DocumentRetrieverBase import DocumentRetrieverBase\n\nclass RedisDocumentRetriever(DocumentRetrieverBase):\n    \"\"\"\n    A document retriever that fetches documents from a Redis store.\n    \"\"\"\n    \n    def __init__(self, redis_idx_name, redis_host, redis_port):\n        \"\"\"\n        Initializes a new instance of RedisDocumentRetriever.\n\n        Args:\n            redis_client (Redis): An instance of the Redis client.\n        \"\"\"\n        self._redis_client = None\n        self._redis_idx_name = redis_idx_name\n        self._redis_host = redis_host\n        self._redis_port = redis_port\n\n    @property\n    def redis_client(self):\n        \"\"\"Lazily initialize and return the Redis client using a factory method.\"\"\"\n        if self._redis_client is None:\n            self._redis_client = Client(self.redis_idx_name, host=self.redis_host, port=self.redis_port)\n        return self._redis_client\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the most relevant documents based on the given query.\n        \n        Args:\n            query (str): The query string used for document retrieval.\n            top_k (int, optional): The number of top relevant documents to retrieve. Defaults to 5.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        query_result = self.redis_client.search(Query(query).paging(0, top_k))\n        \n        documents = [\n            ConcreteDocument(\n                doc_id=doc.id,\n                content=doc.text,  # Note: Adjust 'text' based on actual Redis document schema\n                metadata=doc.__dict__  # Including full document fields and values in metadata\n            )\n            for doc in query_result.docs\n        ]\n\n        return documents\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/__init__.py",
        "content": "```swarmauri/community/document_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/base/__init__.py",
        "content": "```swarmauri/community/document_stores/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/concrete/__init__.py",
        "content": "```swarmauri/community/document_stores/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/community/document_stores/concrete/RedisDocumentStore.py",
        "content": "```swarmauri/community/document_stores/concrete/RedisDocumentStore.py\nfrom typing import List, Optional\nfrom ....standard.document_stores.base.DocumentStoreBase import DocumentStoreBase\nfrom ....core.documents.IDocument import IDocument\nimport redis\nimport json\nfrom redis.commands.search.field import TextField, NumericField, TagField\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\n\n\nclass RedisDocumentStore(DocumentStoreBase):\n    def __init__(self, host, password, port, db):\n        \"\"\"Store connection details without initializing the Redis client.\"\"\"\n        self._host = host\n        self._password = password\n        self._port = port\n        self._db = db\n        self._redis_client = None  # Delayed initialization\n\n    @property\n    def redis_client(self):\n        \"\"\"Lazily initialize and return the Redis client using a factory method.\"\"\"\n        if self._redis_client is None:\n            print('here')\n            self._redis_client = redis.Redis(host=self._host, \n                                             password=self._password, \n                                             port=self._port, \n                                             db=self._db)\n            print('there')\n        return self._redis_client\n\n    def add_document(self, document: IDocument) -> None:\n        \n        data = document.as_dict()\n        doc_id = data['id'] \n        del data['id']\n        self.redis_client.json().set(doc_id, '$', json.dumps(data))\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        with self.redis_client.pipeline() as pipe:\n            for document in documents:\n                pipe.set(document.doc_id, document)\n            pipe.execute()\n\n    def get_document(self, doc_id: str) -> Optional[IDocument]:\n        result = self.redis_client.json().get(doc_id)\n        if result:\n            return json.loads(result)\n        return None\n\n    def get_all_documents(self) -> List[IDocument]:\n        keys = self.redis_client.keys('*')\n        documents = []\n        for key in keys:\n            document_data = self.redis_client.get(key)\n            if document_data:\n                documents.append(json.loads(document_data))\n        return documents\n\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        self.add_document(updated_document)\n\n    def delete_document(self, doc_id: str) -> None:\n        self.redis_client.delete(doc_id)\n    \n    def __getstate__(self):\n        \"\"\"Return the object state for serialization, excluding the Redis client.\"\"\"\n        state = self.__dict__.copy()\n        state['_redis_client'] = None  # Exclude Redis client from serialization\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore the object state after serialization, reinitializing the Redis client.\"\"\"\n        self.__dict__.update(state)\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/AnnoyVectorStore.py",
        "content": "```swarmauri/community/vector_stores/AnnoyVectorStore.py\nimport os\nimport json\nimport pickle\nimport tempfile\nfrom typing import List, Union\nfrom annoy import AnnoyIndex\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.base.SaveLoadStoreBase import SaveLoadStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nfrom swarmauri.standard.vectorizers.concrete.Doc2VecVectorizer import Doc2VecVectorizer\n#from swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nclass AnnoyVectorStore(SaveLoadStoreBase, VectorDocumentStoreRetrieveBase):\n    \"\"\"\n    AnnoyVectorStore is a concrete implementation that integrates functionality\n    for saving, loading, storing, and retrieving vector documents, leveraging Annoy as the backend.\n    \"\"\"\n\n    def __init__(self, dimension: int, metric='euclidean', num_trees=10):\n        self.dimension = dimension\n        self.vectorizer = Doc2VecVectorizer()\n        self.metric = metric\n        self.num_trees = num_trees\n        self.index = AnnoyIndex(dimension, metric)\n        self.documents = []  # List of documents\n        self.id_to_index = {}  # Mapping from document ID to index in Annoy\n        SaveLoadStoreBase.__init__(self, self.vectorizer, [])\n\n    def get_state(self) -> dict:\n        \"\"\"\n        Retrieve the internal state of the vector store to be saved.\n        \n        Returns:\n            dict: The internal state of the vector store.\n        \"\"\"\n        return {\n            'documents': [doc.to_dict() for doc in self.documents],\n            'id_to_index': self.id_to_index\n        }\n\n    def set_state(self, state: dict) -> None:\n        \"\"\"\n        Set the internal state of the vector store when loading.\n        \n        Parameters:\n            state (dict): The state to set to the vector store.\n        \"\"\"\n        self.documents = [Document.from_dict(doc_dict) for doc_dict in state.get('documents', [])]\n        self.id_to_index = state['id_to_index']\n        for idx, document in enumerate(self.documents):\n            self.index.add_item(idx, document.content)\n        self.index.build(self.num_trees)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Add a single document to the document store.\n        \n        Parameters:\n            document (IDocument): The document to be added to the store.\n        \"\"\"\n        index = len(self.documents)\n        self.documents.append(document)\n        self.index.add_item(index, document.content)\n        self.id_to_index[document.id] = index\n        try:\n            self.index.build(self.num_trees)\n        except Exception as e:\n            self._rebuild_index()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n        \n        Parameters:\n            documents (List[IDocument]): A list of documents to be added to the store.\n        \"\"\"\n        start_idx = len(self.documents)\n        self.documents.extend(documents)\n        for i, doc in enumerate(documents):\n            idx = start_idx + i\n            self.index.add_item(idx, doc.content)\n            self.id_to_index[doc.id] = idx\n        try:\n            self.index.build(self.num_trees)\n        except Exception as e:\n            self._rebuild_index()\n\n    def get_document(self, id: str) -> Union[IDocument, None]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to retrieve.\n        \n        Returns:\n            Union[IDocument, None]: The requested document if found; otherwise, None.\n        \"\"\"\n        index = self.id_to_index.get(id)\n        if index is not None:\n            return self.documents[index]\n        return None\n\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n        \n        Returns:\n            List[IDocument]: A list of all documents in the store.\n        \"\"\"\n        return self.documents\n\n    def delete_document(self, id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to delete.\n        \"\"\"\n        if id in self.id_to_index:\n            index = self.id_to_index.pop(id)\n            self.documents.pop(index)\n            self._rebuild_index()\n\n    def update_document(self, id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Update a document in the document store.\n        \n        Parameters:\n            id (str): The unique identifier of the document to update.\n            updated_document (IDocument): The updated document instance.\n        \"\"\"\n        if id in self.id_to_index:\n            index = self.id_to_index[id]\n            self.documents[index] = updated_document\n            self._rebuild_index()\n\n    def clear_documents(self) -> None:\n        \"\"\"\n        Deletes all documents from the vector store\n        \"\"\"\n        self.documents = []\n        self.doc_id_to_index = {}\n        self.index = AnnoyIndex(self.dimension, self.metric)\n\n    def document_count(self) -> int:\n        \"\"\"\n        Returns the number of documents in the store.\n        \"\"\"\n        return len(self.documents)\n\n    def retrieve(self, query: List[float], top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        \n        Args:\n            query (List[float]): The content of the document for retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        indices = self.index.get_nns_by_vector(query, top_k, include_distances=False)\n        return [self.documents[idx] for idx in indices]\n\n    def save_store(self, directory_path: str) -> None:\n        \"\"\"\n        Saves the state of the vector store to the specified directory. This includes\n        both the vectorizer's model and the stored documents or vectors.\n\n        Parameters:\n            directory_path (str): The directory path where the store's state will be saved.\n        \"\"\"\n        state = self.get_state()\n        os.makedirs(directory_path, exist_ok=True)\n        state_file = os.path.join(directory_path, 'store_state.json')\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n\n        with open(state_file, 'w') as f:\n            json.dump(state, f, indent=4)\n        self.index.save(index_file)\n\n    def load_store(self, directory_path: str) -> None:\n        \"\"\"\n        Loads the state of the vector store from the specified directory. This includes\n        both the vectorizer's model and the stored documents or vectors.\n\n        Parameters:\n            directory_path (str): The directory path from where the store's state will be loaded.\n        \"\"\"\n        state_file = os.path.join(directory_path, 'store_state.json')\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n\n        with open(state_file, 'r') as f:\n            state = json.load(f)\n        self.set_state(state)\n        self.index.load(index_file)\n\n    def save_parts(self, directory_path: str, chunk_size: int = 10485760) -> None:\n        \"\"\"\n        Save the model in parts to handle large files by splitting them.\n        \"\"\"\n        state = self.get_state()\n        os.makedirs(directory_path, exist_ok=True)\n        temp_state_file = tempfile.NamedTemporaryFile(delete=False)\n\n        try:\n            pickle.dump(state, temp_state_file)\n            temp_state_file.close()\n\n            with open(temp_state_file.name, 'rb') as src:\n                part_num = 0\n                while True:\n                    chunk = src.read(chunk_size)\n                    if not chunk:\n                        break\n                    with open(os.path.join(directory_path, f'state_part_{part_num}.pkl'), 'wb') as dest:\n                        dest.write(chunk)\n                    part_num += 1\n        finally:\n            os.remove(temp_state_file.name)\n\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n        self.index.save(index_file)\n\n        with open(index_file, 'rb') as src:\n            part_num = 0\n            while True:\n                chunk = src.read(chunk_size)\n                if not chunk:\n                    break\n                with open(os.path.join(directory_path, f'index_part_{part_num}.ann'), 'wb') as dest:\n                    dest.write(chunk)\n                part_num += 1\n\n    def load_parts(self, directory_path: str, state_file_pattern: str, index_file_pattern: str) -> None:\n        \"\"\"\n        Load and combine model parts from a directory.\n        \"\"\"\n        temp_state_file = tempfile.NamedTemporaryFile(delete=False)\n        try:\n            with open(temp_state_file.name, 'ab') as dest:\n                part_num = 0\n                while True:\n                    part_file_path = os.path.join(directory_path, state_file_pattern.format(part_num))\n                    if not os.path.isfile(part_file_path):\n                        break\n                    with open(part_file_path, 'rb') as src:\n                        chunk = src.read()\n                        dest.write(chunk)\n                    part_num += 1\n\n            with open(temp_state_file.name, 'rb') as src:\n                state = pickle.load(src)\n            self.set_state(state)\n        finally:\n            os.remove(temp_state_file.name)\n\n        index_file = os.path.join(directory_path, 'annoy_index.ann')\n        self.index.load(index_file)\n\n    def _rebuild_index(self):\n        \"\"\"\n        Rebuild the Annoy index from the current documents.\n        \"\"\"\n        self.index = AnnoyIndex(self.dimension, self.metric)\n        for idx, document in enumerate(self.documents):\n            self.index.add_item(idx, document.content)\n        self.index.build(self.num_trees)\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/ChromadbVectorStore.py",
        "content": "```swarmauri/community/vector_stores/ChromadbVectorStore.py\nimport os\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.vector_stores.base.SaveLoadStoreBase import SaveLoadStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nfrom swarmauri.standard.vectorizers.concrete.Doc2VecVectorizer import Doc2VecVectorizer\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.documents.concrete.Document import Document\nimport chromadb\n\nclass ChromaDBVectorStore(VectorDocumentStoreRetrieveBase, SaveLoadStoreBase):\n    def __init__(self, db_name):\n        self.vectorizer = Doc2VecVectorizer()\n        self.metric = CosineDistance()\n        self.db_name = db_name\n        self.client = chromadb.Client()\n        self.collection = self.client.get_or_create_collection(name=db_name)\n        SaveLoadStoreBase.__init__(self, self.vectorizer, [])\n\n    def add_document(self, document: IDocument) -> None:\n        try:\n            embedding = self.vectorizer.infer_vector(document.content).data\n            self.collection.add(ids=[document.id],\n                    documents=[document.content], \n                    embeddings=[embedding], \n                    metadatas=[document.metadata] )\n        except:\n            texts = [document.content]\n            self.vectorizer.fit_transform(texts)\n            embedding = self.vectorizer.infer_vector(document.content).data\n            self.collection.add(ids=[document.id],\n                                documents=[document.content], \n                                embeddings=[embedding], \n                                metadatas=[document.metadata] )\n            \n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        ids = [doc.id for doc in documents]\n        texts = [doc.content for doc in documents]\n        embeddings = [self.vectorizer.infer_vector(doc.content).data for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        \n        self.collection.add(ids=ids,\n                            documents=texts, \n                            embeddings=embeddings, \n                            metadatas=metadatas)\n\n    def get_document(self, doc_id: str) -> Union[IDocument, None]:\n        try:\n            results = self.collection.get(ids=[doc_id])\n            document = Document(id=results['ids'][0],\n                             content=results['documents'][0],\n                                 metadata=results['metadatas'][0])\n        except Exception as e:\n            print(str(e))\n            document = None\n        return document if document else []\n\n    def get_all_documents(self) -> List[IDocument]:\n        try:\n            results = self.collection.get()\n            print(results)\n            return [Document(id=results['ids'][idx],\n                                 content=results['documents'][idx],\n                                 metadata=results['metadatas'][idx])\n                    for idx, value in enumerate(results['ids'])]\n        except Exception as e:\n            print(str(e))\n            document = None\n        return document if document else []\n            \n\n    def delete_document(self, doc_id: str) -> None:\n        self.collection.delete(ids=[doc_id])\n\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        self.delete_document(doc_id)\n        self.add_document(updated_document)\n\n    def clear_documents(self) -> None:\n        self.client.delete_collection(self.db_name)\n\n    def document_count(self) -> int:\n        try:\n            return len(self.get_all_documents())\n        except StopIteration:\n            return 0\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        embedding = self.vectorizer.infer_vector(query).data\n        results = self.collection.query(query_embeddings=embedding,\n                                        n_results=top_k)\n        print('retrieve reults', results)\n        print(results['ids'][0])\n        documents = []\n        for idx in range(len(results['ids'])):\n            documents.append(Document(id=results['ids'][idx],\n                             content=results['documents'][idx],\n                             metadata=results['metadatas'][idx]))\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/QdrantVectorStore.py",
        "content": "```swarmauri/community/vector_stores/QdrantVectorStore.py\nimport os\nimport json\nimport pickle\nimport tempfile\nfrom typing import List, Union, Literal\nfrom qdrant_client import QdrantClient, models\n\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.embeddings.concrete.Doc2VecEmbedding import Doc2VecEmbedding\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\n\nfrom swarmauri.standard.vector_stores.base.VectorStoreBase import VectorStoreBase\nfrom swarmauri.standard.vector_stores.base.VectorStoreRetrieveMixin import VectorStoreRetrieveMixin\nfrom swarmauri.standard.vector_stores.base.VectorStoreSaveLoadMixin import VectorStoreSaveLoadMixin    \n\n\nclass QdrantVectorStore(VectorStoreSaveLoadMixin, VectorStoreRetrieveMixin, VectorStoreBase):\n    \"\"\"\n    QdrantVectorStore is a concrete implementation that integrates functionality\n    for saving, loading, storing, and retrieving vector documents, leveraging Qdrant as the backend.\n    \"\"\"\n    type: Literal['QdrantVectorStore'] = 'QdrantVectorStore'\n\n    def __init__(self, url: str, api_key: str, collection_name: str, vector_size: int):\n        self.vectorizer = Doc2VecEmbedding(vector_size=vector_size)\n        self.metric = CosineDistance()\n        self.client = QdrantClient(url=url, api_key=api_key)\n        self.collection_name = collection_name\n        exists = self.client.collection_exists(collection_name)\n        \n        if not exists:\n            self.client.create_collection(\n                collection_name=collection_name,\n                vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n            )   \n\n\n    def add_document(self, document: Document) -> None:\n        \"\"\"\n        Add a single document to the document store.\n        \n        Parameters:\n            document (Document): The document to be added to the store.\n        \"\"\"\n        try:\n            embedding = document.embedding or self.vectorizer.fit_transform(document.content).data \n            self.client.upsert(self.collection_name, points=[\n                models.PointStruct(\n                    id=document.id,\n                    vector=embedding,\n                    payload=document.metadata\n                )\n            ])\n            \n        except:\n            embedding = document.embedding or self.vectorizer.fit_transform(document.content).data \n            self.client.upsert(self.collection_name, points=[\n                models.PointStruct(\n                    id=document.id,\n                    vector=embedding,\n                    payload=document.metadata\n                )\n            ])\n            \n        \n\n    def add_documents(self, documents: List[Document]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n        \n        Parameters:\n            documents (List[Document]): A list of documents to be added to the store.\n        \"\"\"\n        self.vectorizer.fit_transform([doc.content for doc in documents])\n        points = [\n            models.PointStruct(\n                id=doc.id,\n                vector=doc.embedding or self.vectorizer.infer_vector(doc.content).data,\n                payload=doc.metadata\n            ) for doc in documents\n        ]\n        self.client.upsert(self.collection_name, points=points)\n\n    def get_document(self, id: str) -> Union[Document, None]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to retrieve.\n        \n        Returns:\n            Union[Document, None]: The requested document if found; otherwise, None.\n        \"\"\"\n        \n        raise NotImplementedError('Get document not implemented, use retrieve().')\n\n    def get_all_documents(self) -> List[Document]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n        \n        Returns:\n            List[Document]: A list of all documents in the store.\n        \"\"\"\n        raise NotImplementedError('Get all documents not implemented, use retrieve().')\n\n    def delete_document(self, id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n        \n        Parameters:\n            id (str): The unique identifier of the document to delete.\n        \"\"\"\n        self.client.delete(self.collection_name, points_selector=[id])\n\n    def update_document(self, id: str, updated_document: Document) -> None:\n        \"\"\"\n        Update a document in the document store.\n        \n        Parameters:\n            id (str): The unique identifier of the document to update.\n            updated_document (Document): The updated document instance.\n        \"\"\"\n        self.client.upsert(self.collection_name, points=[                           \n            models.PointStruct(\n                id=updated_document.id,\n                vector=updated_document.embedding,\n                payload=updated_document.metadata\n            )\n        ])\n\n    def clear_documents(self) -> None:\n        \"\"\"\n        Deletes all documents from the vector store\n        \"\"\"\n        self.documents = []\n        self.client.delete(self.collection_name, points_selector=models.FilterSelector())\n\n    def document_count(self) -> int:\n        \"\"\"\n        Returns the number of documents in the store.\n        \"\"\"\n        raise NotImplementedError('Get document not implemeneted, use retrieve().')\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[Document]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        For the purpose of this example, this method performs a basic search.\n        \n        Args:\n            query (str): The query string used for document retrieval. \n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[Document]: A list of the top_k most relevant documents.\n        \"\"\"\n        # This should be modified to a query to the Qdrant service for relevance search\n        query_vector = self.vectorizer.infer_vector(query).data\n        documents = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_vector,\n            limit=top_k)\n        \n        matching_documents = [\n            doc.payload for doc in documents\n        ]\n        return matching_documents[:top_k]\n\n```"
    },
    {
        "document_name": "swarmauri/community/vector_stores/__init__.py",
        "content": "```swarmauri/community/vector_stores/__init__.py\n\n```"
    }
]