[
    {
        "document_name": "swarmauri/experimental/__init__.py",
        "content": "```swarmauri/experimental/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/RemoteUniversalBase.py",
        "content": "```swarmauri/experimental/RemoteUniversalBase.py\nimport requests\nimport hashlib\nfrom functools import wraps\nfrom uuid import uuid4\nimport inspect\n\ndef remote_local_transport(cls):\n    original_init = cls.__init__\n    def init_wrapper(self, *args, **kwargs):\n        host = kwargs.pop('host', None)\n        resource = kwargs.get('resource', cls.__name__)\n        owner = kwargs.get('owner')\n        name = kwargs.get('name')\n        id = kwargs.get('id')\n        #path = kwargs.get('path')\n        if host:\n            #self.is_remote = True\n            self.host = host\n            self.resource = resource\n            self.owner = owner\n            self.id = id\n            #self.path = path\n            url = f\"{host}/{owner}/{resource}/{id}\"\n            data = {\"class_name\": cls.__name__, \"owner\": owner, \"name\": name, **kwargs}\n            response = requests.post(url, json=data)\n            if not response.ok:\n                raise Exception(f\"Failed to initialize remote {cls.__name__}: {response.text}\")\n        else:\n            original_init(self, owner, name, **kwargs)  # Ensure proper passing of positional arguments\n\n    setattr(cls, '__init__', init_wrapper)\n\n    for attr_name, attr_value in cls.__dict__.items():\n        if callable(attr_value) and not attr_name.startswith(\"_\"):\n            setattr(cls, attr_name, method_wrapper(attr_value))\n        elif isinstance(attr_value, property):\n            prop_get = attr_value.fget and method_wrapper(attr_value.fget)\n            prop_set = attr_value.fset and method_wrapper(attr_value.fset)\n            prop_del = attr_value.fdel and method_wrapper(attr_value.fdel)\n            setattr(cls, attr_name, property(prop_get, prop_set, prop_del, attr_value.__doc__))\n    return cls\n\n\ndef method_wrapper(method):\n    @wraps(method)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        if getattr(self, 'host'):\n            print('[x] Executing remote call...')\n            url = f\"{self.path}\".lower()\n            response = requests.post(url, json={\"args\": args[1:], \"kwargs\": kwargs})\n            if response.ok:\n                return response.json()\n            else:\n                raise Exception(f\"Remote method call failed: {response.text}\")\n        else:\n            return method(*args, **kwargs)\n    return wrapper\n\nclass RemoteLocalMeta(type):\n    def __new__(metacls, name, bases, class_dict):\n        cls = super().__new__(metacls, name, bases, class_dict)\n        if bases:  # This prevents BaseComponent itself from being decorated\n            cls = remote_local_transport(cls)\n        cls.class_hash = cls._calculate_class_hash()\n        return cls\n\n    def _calculate_class_hash(cls):\n        sig_hash = hashlib.sha256()\n        for attr_name, attr_value in cls.__dict__.items():\n            if callable(attr_value) and not attr_name.startswith(\"_\"):\n                sig = inspect.signature(attr_value)\n                sig_hash.update(str(sig).encode())\n        return sig_hash.hexdigest()\n    \n\nclass BaseComponent(metaclass=RemoteLocalMeta):\n    version = \"1.0.0\"  # Semantic versioning initialized here\n    def __init__(self, owner, name, host=None, members=[], resource=None):\n        self.id = uuid4()\n        self.owner = owner\n        self.name = name\n        self.host = host  \n        #self.is_remote = bool(self.host) \n        self.members = members\n        self.resource = resource if resource else self.__class__.__name__\n        self.path = f\"{self.host if self.host else ''}/{self.owner}/{self.resource}/{self.id}\"\n\n    @property\n    def is_remote(self):\n        return bool(self.host)\n\n    @classmethod\n    def public_interfaces(cls):\n        methods = []\n        for attr_name in dir(cls):\n            # Retrieve the attribute\n            attr_value = getattr(cls, attr_name)\n            # Check if it's callable or a property and not a private method\n            if (callable(attr_value) and not attr_name.startswith(\"_\")) or isinstance(attr_value, property):\n                methods.append(attr_name)\n        return methods\n\n    @classmethod\n    def is_method_registered(cls, method_name):\n        \"\"\"\n        Checks if a public method with the given name is registered on the class.\n        Args:\n            method_name (str): The name of the method to check.\n        Returns:\n            bool: True if the method is registered, False otherwise.\n        \"\"\"\n        return method_name in cls.public_interfaces()\n\n    @classmethod\n    def method_with_signature(cls, input_signature):\n        \"\"\"\n        Checks if there is a method with the given signature available in the class.\n        \n        Args:\n            input_signature (str): The string representation of the method signature to check.\n        \n        Returns:\n            bool: True if a method with the input signature exists, False otherwise.\n        \"\"\"\n        for method_name in cls.public_interfaces():\n            method = getattr(cls, method_name)\n            if callable(method):\n                sig = str(inspect.signature(method))\n                if sig == input_signature:\n                    return True\n        return False\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/__init__.py",
        "content": "```swarmauri/experimental/tools/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/CypherQueryTool.py",
        "content": "```swarmauri/experimental/tools/CypherQueryTool.py\nimport json\nfrom neo4j import GraphDatabase\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass CypherQueryTool(ToolBase):\n    def __init__(self, uri: str, user: str, password: str):\n        self.uri = uri\n        self.user = user\n        self.password = password\n        \n        # Define only the 'query' parameter since uri, user, and password are set at initialization\n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"The Cypher query to execute.\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"CypherQueryTool\",\n                         description=\"Executes a Cypher query against a Neo4j database.\",\n                         parameters=parameters)\n\n    def _get_connection(self):\n        return GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n\n    def __call__(self, query) -> str:\n        # Establish connection to the database\n        driver = self._get_connection()\n        session = driver.session()\n\n        # Execute the query\n        result = session.run(query)\n        records = result.data()\n\n        # Close the connection\n        session.close()\n        driver.close()\n\n        # Convert records to JSON string, assuming it's JSON serializable\n        return json.dumps(records)\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/FileDownloaderTool.py",
        "content": "```swarmauri/experimental/tools/FileDownloaderTool.py\nimport requests\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass FileDownloaderTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"The URL of the file to download\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"FileDownloaderTool\",\n                         description=\"Downloads a file from a specified URL into memory.\",\n                         parameters=parameters)\n    \n    def __call__(self, url: str) -> bytes:\n        \"\"\"\n        Downloads a file from the given URL into memory.\n        \n        Parameters:\n        - url (str): The URL of the file to download.\n        \n        Returns:\n        - bytes: The content of the downloaded file.\n        \"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raises an HTTPError if the request resulted in an error\n            return response.content\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Failed to download file from '{url}'. Error: {e}\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/LinkedInArticleTool.py",
        "content": "```swarmauri/experimental/tools/LinkedInArticleTool.py\nimport requests\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass LinkedInArticleTool(ToolBase):\n    \"\"\"\n    A tool to post articles on LinkedIn using the LinkedIn API.\n    \"\"\"\n    def __init__(self, access_token):\n        \"\"\"\n        Initializes the LinkedInArticleTool with the necessary access token.\n        \n        Args:\n            access_token (str): The OAuth access token for authenticating with the LinkedIn API.\n        \"\"\"\n        super().__init__(name=\"LinkedInArticleTool\",\n                         description=\"A tool for posting articles on LinkedIn.\",\n                         parameters=[\n                             Parameter(name=\"title\", type=\"string\", description=\"The title of the article\", required=True),\n                             Parameter(name=\"text\", type=\"string\", description=\"The body text of the article\", required=True),\n                             Parameter(name=\"visibility\", type=\"string\", description=\"The visibility of the article\", required=True, enum=[\"anyone\", \"connectionsOnly\"])\n                         ])\n        self.access_token = access_token\n        \n    def __call__(self, title: str, text: str, visibility: str = \"anyone\") -> str:\n        \"\"\"\n        Posts an article on LinkedIn.\n\n        Args:\n            title (str): The title of the article.\n            text (str): The body text of the article.\n            visibility (str): The visibility of the article, either \"anyone\" or \"connectionsOnly\".\n\n        Returns:\n            str: A message indicating the success or failure of the post operation.\n        \"\"\"\n        # Construct the request URL and payload according to LinkedIn API documentation\n        url = 'https://api.linkedin.com/v2/ugcPosts'\n        headers = {\n            'Authorization': f'Bearer {self.access_token}',\n            'X-Restli-Protocol-Version': '2.0.0',\n            'Content-Type': 'application/json'\n        }\n        \n        payload = {\n            \"author\": \"urn:li:person:YOUR_PERSON_ID_HERE\",\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\n                        \"text\": text\n                    },\n                    \"shareMediaCategory\": \"ARTICLE\",\n                    \"media\": [\n                        {\n                            \"status\": \"READY\",\n                            \"description\": {\n                                \"text\": title\n                            },\n                            \"originalUrl\": \"URL_OF_THE_ARTICLE_OR_IMAGE\",\n                            \"visibility\": {\n                                \"com.linkedin.ugc.MemberNetworkVisibility\": visibility.upper()\n                            }\n                        }\n                    ]\n                }\n            },\n            \"visibility\": {\n                \"com.linkedin.ugc.MemberNetworkVisibility\": visibility.upper()\n            }\n        }\n     \n        # Make the POST request to LinkedIn's API\n        response = requests.post(url, headers=headers, json=payload)\n        \n        if response.status_code == 201:\n            return f\"Article posted successfully: {response.json().get('id')}\"\n        else:\n            return f\"Failed to post the article. Status Code: {response.status_code} - {response.text}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/OutlookSendMailTool.py",
        "content": "```swarmauri/experimental/tools/OutlookSendMailTool.py\nimport requests\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\n\nclass OutlookSendMailTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"recipient\",\n                type=\"string\",\n                description=\"The email address of the recipient\",\n                required=True\n            ),\n            Parameter(\n                name=\"subject\",\n                type=\"string\",\n                description=\"The subject of the email\",\n                required=True\n            ),\n            Parameter(\n                name=\"body\",\n                type=\"string\",\n                description=\"The HTML body of the email\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"OutlookSendMailTool\", \n                         description=\"Sends an email using the Outlook service.\",\n                         parameters=parameters)\n\n        # Add your Microsoft Graph API credentials and endpoint URL here\n        self.tenant_id = \"YOUR_TENANT_ID\"\n        self.client_id = \"YOUR_CLIENT_ID\"\n        self.client_secret = \"YOUR_CLIENT_SECRET\"\n        self.scope = [\"https://graph.microsoft.com/.default\"]\n        self.token_url = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/v2.0/token\"\n        self.graph_endpoint = \"https://graph.microsoft.com/v1.0\"\n\n    def get_access_token(self):\n        data = {\n            \"client_id\": self.client_id,\n            \"scope\": \" \".join(self.scope),\n            \"client_secret\": self.client_secret,\n            \"grant_type\": \"client_credentials\"\n        }\n        response = requests.post(self.token_url, data=data)\n        response.raise_for_status()\n        return response.json().get(\"access_token\")\n\n    def __call__(self, recipient, subject, body):\n        access_token = self.get_access_token()\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        email_data = {\n            \"message\": {\n                \"subject\": subject,\n                \"body\": {\n                    \"contentType\": \"HTML\",\n                    \"content\": body\n                },\n                \"toRecipients\": [\n                    {\n                        \"emailAddress\": {\n                            \"address\": recipient\n                        }\n                    }\n                ]\n            }\n        }\n\n        send_mail_endpoint = f\"{self.graph_endpoint}/users/{self.client_id}/sendMail\"\n        response = requests.post(send_mail_endpoint, json=email_data, headers=headers)\n        if response.status_code == 202:\n            return \"Email sent successfully\"\n        else:\n            return f\"Failed to send email, status code {response.status_code}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/SQLite3QueryTool.py",
        "content": "```swarmauri/experimental/tools/SQLite3QueryTool.py\nimport sqlite3\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass SQLite3QueryTool(ToolBase):\n    def __init__(self, db_name: str):\n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"SQL query to execute\",\n                required=True\n            )\n        ]\n        super().__init__(name=\"SQLQueryTool\", \n                         description=\"Executes an SQL query and returns the results.\", \n                         parameters=parameters)\n        self.db_name = db_name\n\n    def __call__(self, query) -> str:\n        \"\"\"\n        Execute the provided SQL query.\n\n        Parameters:\n        - query (str): The SQL query to execute.\n\n        Returns:\n        - str: The results of the SQL query as a string.\n        \"\"\"\n        try:\n            connection = sqlite3.connect(self.db_name)  # Connect to the specific database file\n            cursor = connection.cursor()\n            \n            cursor.execute(query)\n            rows = cursor.fetchall()\n            result = \"\\n\".join(str(row) for row in rows)\n        except Exception as e:\n            result = f\"Error executing query: {e}\"\n        finally:\n            connection.close()\n        \n        return f\"Query Result:\\n{result}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/TwitterPostTool.py",
        "content": "```swarmauri/experimental/tools/TwitterPostTool.py\nfrom tweepy import Client\n\nfrom swarmauri.standard.tools.concrete.ToolBase import ToolBase\nfrom swarmauri.standard.tools.concrete.Parameter import Parameter\n\nclass TwitterPostTool(ToolBase):\n    def __init__(self, bearer_token):\n        # Initialize parameters necessary for posting a tweet\n        parameters = [\n            Parameter(\n                name=\"status\",\n                type=\"string\",\n                description=\"The status message to post on Twitter\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"TwitterPostTool\", description=\"Post a status update on Twitter\", parameters=parameters)\n        \n        # Initialize Twitter API Client\n        self.client = Client(bearer_token=bearer_token)\n\n    def __call__(self, status: str) -> str:\n        \"\"\"\n        Posts a status on Twitter.\n\n        Args:\n            status (str): The status message to post.\n\n        Returns:\n            str: A confirmation message including the tweet's URL if successful.\n        \"\"\"\n        try:\n            # Using Tweepy to send a tweet\n            response = self.client.create_tweet(text=status)\n            tweet_id = response.data['id']\n            # Constructing URL to the tweet - Adjust the URL to match Twitter API v2 structure if needed\n            tweet_url = f\"https://twitter.com/user/status/{tweet_id}\"\n            return f\"Tweet successful: {tweet_url}\"\n        except Exception as e:\n            return f\"An error occurred: {e}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/__init__.py",
        "content": "```swarmauri/experimental/conversations/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/SemanticConversation.py",
        "content": "```swarmauri/experimental/conversations/SemanticConversation.py\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Dict, Union\nfrom ...core.messages.IMessage import IMessage\nfrom ...core.conversations.IConversation import IConversation\n\nclass SemanticConversation(IConversation, ABC):\n    \"\"\"\n    A concrete implementation of the Conversation class that includes semantic routing.\n    Semantic routing involves analyzing the content of messages to understand their intent\n    or category and then routing them to appropriate handlers based on that analysis.\n\n    This class requires subclasses to implement the _analyze_message method for semantic analysis.\n    \"\"\"\n\n\n    @abstractmethod\n    def register_handler(self, category: str, handler: Callable[[IMessage], None]):\n        \"\"\"\n        Registers a message handler for a specific semantic category.\n\n        Args:\n            category (str): The category of messages this handler should process.\n            handler (Callable[[Message], None]): The function to call for messages of the specified category.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history and routes it to the appropriate handler based on its semantic category.\n\n        Args:\n            message (Message): The message to be added and processed.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _analyze_message(self, message: IMessage) -> Union[str, None]:\n        \"\"\"\n        Analyzes the content of a message to determine its semantic category.\n\n        This method must be implemented by subclasses to provide specific logic for semantic analysis.\n\n        Args:\n            message (Message): The message to analyze.\n\n        Returns:\n            Union[str, None]: The semantic category of the message, if determined; otherwise, None.\n\n        Raises:\n            NotImplementedError: If the method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the _analyze_message method to provide semantic analysis.\")\n\n    # Additional methods as needed for message retrieval, history management, etc., inherited from Conversation\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/ConsensusBuildingConversation.py",
        "content": "```swarmauri/experimental/conversations/ConsensusBuildingConversation.py\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.messages.IMessage import IMessage\n\n\nclass ConsensusBuildingMessage(IMessage):\n    def __init__(self, sender_id: str, content: str, message_type: str):\n        self._sender_id = sender_id\n        self._content = content\n        self._role = 'consensus_message'\n        self._message_type = message_type\n\n    @property\n    def role(self) -> str:\n        return self._role\n\n    @property\n    def content(self) -> str:\n        return self._content\n\n    def as_dict(self) -> dict:\n        return {\n            \"sender_id\": self._sender_id,\n            \"content\": self._content,\n            \"message_type\": self._message_type\n        }\n\n\nclass ConsensusBuildingConversation(IConversation):\n    def __init__(self, topic: str, participants: list):\n        self.topic = topic\n        self.participants = participants  # List of agent IDs\n        self._history = []  # Stores all messages exchanged in the conversation\n        self.proposal_votes = {}  # Tracks votes for each proposal\n\n    @property\n    def history(self) -> list:\n        return self._history\n\n    def add_message(self, message: IMessage):\n        if not isinstance(message, ConsensusBuildingMessage):\n            raise ValueError(\"Only instances of ConsensusBuildingMessage are accepted\")\n        self._history.append(message)\n\n    def get_last(self) -> IMessage:\n        if self._history:\n            return self._history[-1]\n        return None\n\n    def clear_history(self) -> None:\n        self._history.clear()\n\n    def as_dict(self) -> list:\n        return [message.as_dict() for message in self._history]\n\n    def initiate_consensus(self, initiator_id: str, proposal=None):\n        \"\"\"Starts the conversation with an initial proposal, if any.\"\"\"\n        initiate_message = ConsensusBuildingMessage(initiator_id, proposal, \"InitiateConsensusMessage\")\n        self.add_message(initiate_message)\n\n    def add_proposal(self, sender_id: str, proposal: str):\n        \"\"\"Adds a proposal to the conversation.\"\"\"\n        proposal_message = ConsensusBuildingMessage(sender_id, proposal, \"ProposalMessage\")\n        self.add_message(proposal_message)\n\n    def add_comment(self, sender_id: str, comment: str):\n        \"\"\"Adds a comment or feedback regarding a proposal.\"\"\"\n        comment_message = ConsensusBuildingMessage(sender_id, comment, \"CommentMessage\")\n        self.add_message(comment_message)\n\n    def vote(self, sender_id: str, vote: str):\n        \"\"\"Registers a vote for a given proposal.\"\"\"\n        vote_message = ConsensusBuildingMessage(sender_id, vote, \"VoteMessage\")\n        self.add_message(vote_message)\n        # Count the vote\n        self.proposal_votes[vote] = self.proposal_votes.get(vote, 0) + 1\n\n    def check_agreement(self):\n        \"\"\"\n        Checks if there is a consensus on any proposal.\n        A simple majority (>50% of the participants) is required for consensus.\n        \"\"\"\n        consensus_threshold = len(self.participants) / 2  # Define consensus as a simple majority\n\n        for proposal, votes in self.proposal_votes.items():\n            if votes > consensus_threshold:\n                # A consensus has been reached\n                return True, f\"Consensus reached on proposal: {proposal} with {votes} votes.\"\n\n        # If no consensus is reached\n        return False, \"No consensus reached.\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/SharedConversation.py",
        "content": "```swarmauri/experimental/conversations/SharedConversation.py\nimport inspect\nfrom threading import Lock\nfrom typing import Optional, Dict, List, Tuple\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.standard.messages.concrete.HumanMessage import HumanMessage\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n\nclass SharedConversation(ConversationBase):\n    \"\"\"\n    A thread-safe conversation class that supports individual system contexts for each SwarmAgent.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = Lock()  # A lock to ensure thread safety\n        self._agent_system_contexts: Dict[str, SystemMessage] = {}  # Store system contexts for each agent\n        self._history: List[Tuple[str, IMessage]] = []  # Stores tuples of (sender_id, IMessage)\n\n\n    @property\n    def history(self):\n        history = []\n        for each in self._history:\n            history.append((each[0], each[1]))\n        return history\n\n    def add_message(self, message: IMessage, sender_id: str):\n        with self._lock:\n            self._history.append((sender_id, message))\n\n    def reset_messages(self) -> None:\n        self._history = []\n        \n\n    def _get_caller_name(self) -> Optional[str]:\n        for frame_info in inspect.stack():\n            # Check each frame for an instance with a 'name' attribute in its local variables\n            local_variables = frame_info.frame.f_locals\n            for var_name, var_value in local_variables.items():\n                if hasattr(var_value, 'name'):\n                    # Found an instance with a 'name' attribute. Return its value.\n                    return getattr(var_value, 'name')\n        # No suitable caller found\n        return None\n\n    def as_dict(self) -> List[Dict]:\n        caller_name = self._get_caller_name()\n        history = []\n\n        with self._lock:\n            # If Caller is not one of the agents, then give history\n            if caller_name not in self._agent_system_contexts.keys():\n                for sender_id, message in self._history:\n                    history.append((sender_id, message.as_dict()))\n                \n                \n            else:\n                system_context = self.get_system_context(caller_name)\n                #print(caller_name, system_context, type(system_context))\n                if type(system_context) == str:\n                    history.append(SystemMessage(system_context).as_dict())\n                else:\n                    history.append(system_context.as_dict())\n                    \n                for sender_id, message in self._history:\n                    #print(caller_name, sender_id, message, type(message))\n                    if sender_id == caller_name:\n                        if message.__class__.__name__ == 'AgentMessage' or 'FunctionMessage':\n                            # The caller is the sender; treat as AgentMessage\n                            history.append(message.as_dict())\n                            \n                            # Print to see content that is empty.\n                            #if not message.content:\n                                #print('\\n\\t\\t\\t=>', message, message.content)\n                    else:\n                        if message.content:\n                            # The caller is not the sender; treat as HumanMessage\n                            history.append(HumanMessage(message.content).as_dict())\n        return history\n    \n    def get_last(self) -> IMessage:\n        with self._lock:\n            return super().get_last()\n\n\n    def clear_history(self):\n        with self._lock:\n            super().clear_history()\n\n\n        \n\n    def set_system_context(self, agent_id: str, context: SystemMessage):\n        \"\"\"\n        Sets the system context for a specific agent.\n\n        Args:\n            agent_id (str): Unique identifier for the agent.\n            context (SystemMessage): The context message to be set for the agent.\n        \"\"\"\n        with self._lock:\n            self._agent_system_contexts[agent_id] = context\n\n    def get_system_context(self, agent_id: str) -> Optional[SystemMessage]:\n        \"\"\"\n        Retrieves the system context for a specific agent.\n\n        Args:\n            agent_id (str): Unique identifier for the agent.\n\n        Returns:\n            Optional[SystemMessage]: The context message of the agent, or None if not found.\n        \"\"\"\n        return self._agent_system_contexts.get(agent_id, None)\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/__init__.py",
        "content": "```swarmauri/experimental/models/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/SageMaker.py",
        "content": "```swarmauri/experimental/models/SageMaker.py\nimport json\nimport boto3\nfrom ...core.models.IModel import IModel\n\n\nclass AWSSageMakerModel(IModel):\n    def __init__(self, access_key: str, secret_access_key: str, region_name: str, model_name: str):\n        \"\"\"\n        Initialize the AWS SageMaker model with AWS credentials, region, and the model name.\n\n        Parameters:\n        - access_key (str): AWS access key ID.\n        - secret_access_key (str): AWS secret access key.\n        - region_name (str): The region where the SageMaker model is deployed.\n        - model_name (str): The name of the SageMaker model.\n        \"\"\"\n        self.access_key = access_key\n        self.secret_access_key = secret_access_key\n        self.region_name = region_name\n        self.client = boto3.client('sagemaker-runtime',\n                                   aws_access_key_id=access_key,\n                                   aws_secret_access_key=secret_access_key,\n                                   region_name=region_name)\n        super().__init__(model_name)\n\n    def predict(self, payload: str, content_type: str='application/json') -> dict:\n        \"\"\"\n        Generate predictions using the AWS SageMaker model.\n\n        Parameters:\n        - payload (str): Input data in JSON format.\n        - content_type (str): The MIME type of the input data (default: 'application/json').\n        \n        Returns:\n        - dict: The predictions returned by the model.\n        \"\"\"\n        endpoint_name = self.model_name  # Assuming the model name is also the endpoint name\n        response = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                               Body=payload,\n                                               ContentType=content_type)\n        result = json.loads(response['Body'].read().decode())\n        return result\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/HierarchicalAttentionModel.py",
        "content": "```swarmauri/experimental/models/HierarchicalAttentionModel.py\nimport tensorflow as tf\nfrom swarmauri.core.models.IModel import IModel\nfrom typing import Any\n\nclass HierarchicalAttentionModel(IModel):\n    def __init__(self, model_name: str):\n        self._model_name = model_name\n        self._model = None  # This will hold the TensorFlow model with attention\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n\n    @model_name.setter\n    def model_name(self, value: str) -> None:\n        self._model_name = value\n\n    def load_model(self) -> None:\n        \"\"\"\n        Here, we define and compile the TensorFlow model described earlier.\n        \"\"\"\n        # The following code is adapted from the attention model example provided earlier\n        vocab_size = 10000  # Size of the vocabulary\n        embedding_dim = 256  # Dimension of the embedding layer\n        sentence_length = 100  # Max length of a sentence\n        num_sentences = 10  # Number of sentences in a document\n        units = 128  # Dimensionality of the output space of GRU\n        \n        # Word-level attention layer\n        word_input = tf.keras.layers.Input(shape=(sentence_length,), dtype='int32')\n        embedded_word = tf.keras.layers.Embedding(vocab_size, embedding_dim)(word_input)\n        word_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))(embedded_word)\n        word_attention_layer = tf.keras.layers.Attention(use_scale=True, return_attention_scores=True)\n        word_attention_output, word_attention_weights = word_attention_layer([word_gru, word_gru], return_attention_scores=True)\n        word_encoder_with_attention = tf.keras.Model(inputs=word_input, outputs=[word_attention_output, word_attention_weights])\n        \n        # Sentence-level attention layer\n        sentence_input = tf.keras.layers.Input(shape=(num_sentences, sentence_length), dtype='int32')\n        sentence_encoder_with_attention = tf.keras.layers.TimeDistributed(word_encoder_with_attention)(sentence_input)\n        sentence_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))(sentence_encoder_with_attention[0])\n        sentence_attention_layer = tf.keras.layers.Attention(use_scale=True, return_attention_scores=True)\n        sentence_attention_output, sentence_attention_weights = sentence_attention_layer([sentence_gru, sentence_gru], return_attention_scores=True)\n        doc_representation = tf.keras.layers.Dense(units, activation='tanh')(sentence_attention_output)\n        \n        # Classifier\n        classifier = tf.keras.layers.Dense(1, activation='sigmoid')(doc_representation)\n        \n        # The model\n        self._model = tf.keras.Model(inputs=sentence_input, outputs=[classifier, sentence_attention_weights])\n        self._model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    def predict(self, input_data: Any) -> Any:\n        \"\"\"\n        Predict method to use the loaded model for making predictions.\n\n        This example assumes `input_data` is preprocessed appropriately for the model's expected input.\n        \"\"\"\n        if self._model is None:\n            raise ValueError(\"Model is not loaded. Call `load_model` before prediction.\")\n            \n        # Predicting with the model\n        predictions, attention_weights = self._model.predict(input_data)\n        \n        # Additional logic to handle and package the predictions and attention weights could be added here\n        \n        return predictions, attention_weights\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/__init__.py",
        "content": "```swarmauri/experimental/utils/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/get_last_frame.py",
        "content": "```swarmauri/experimental/utils/get_last_frame.py\nimport inspect\n\ndef child_function(arg):\n    # Get the stack frame of the caller\n    caller_frame = inspect.currentframe().f_back\n    # Get the name of the caller function\n    caller_name = caller_frame.f_code.co_name\n    # Inspect the arguments of the caller function\n    args, _, _, values = inspect.getargvalues(caller_frame)\n    # Assuming the caller has only one argument for simplicity\n    arg_name = args[0]\n    arg_value = values[arg_name]\n    print(f\"Caller Name: {caller_name}, Argument Name: {arg_name}, Argument Value: {arg_value}\")\n\ndef caller_function(l):\n    child_function(l)\n\n# Example usage\ncaller_function(\"Hello\")\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/save_schema.py",
        "content": "```swarmauri/experimental/utils/save_schema.py\nimport inspect\nimport random\n\nclass Storage:\n    def __init__(self):\n        self.logs = []\n\n    def log(self, log_data):\n        self.logs.append(log_data)\n\n    def print_logs(self):\n        for log in self.logs:\n            print(log)\n\nclass Loggable:\n    def __init__(self, name, storage):\n        self.name = name\n        self.storage = storage\n\n    def log_call(self, *args, **kwargs):\n        # Inspect the call stack to get the caller's details\n        caller_frame = inspect.stack()[2]\n        caller_name = inspect.currentframe().f_back.f_code.co_name\n        #caller_name = caller_frame.function\n        module = inspect.getmodule(caller_frame[0])\n        module_name = module.__name__ if module else 'N/A'\n\n        # Log all relevant details\n        log_data = {\n            'caller_name': caller_name,\n            'module_name': module_name,\n            'called_name': self.name,\n            'called_function': caller_frame[3], # The function in which log_call was invoked\n            'args': args,\n            'kwargs': kwargs\n        }\n        self.storage.log(log_data)\n\nclass Caller(Loggable):\n    def __init__(self, name, storage, others):\n        super().__init__(name, storage)\n        self.others = others\n\n    def __call__(self, *args, **kwargs):\n        if len(self.storage.logs)<10:\n            self.log_call(*args, **kwargs)\n            # Randomly call another without causing recursive calls\n            if args:  # Ensures it's not the first call without actual target\n                next_caller_name = random.choice([name for name in self.others if name != self.name])\n                self.others[next_caller_name](self.name)\n\n# Initialize storage and callers\nstorage = Storage()\nothers = {}\n\n# Creating callers\nalice = Caller('Alice', storage, others)\nbob = Caller('Bob', storage, others)\ncharlie = Caller('Charlie', storage, others)\ndan = Caller('Dan', storage, others)\n\nothers['Alice'] = alice\nothers['Bob'] = bob\nothers['Charlie'] = charlie\nothers['Dan'] = dan\n\n# Simulate the calls\ndan(1, taco=23)\n\n# Print the logs\nstorage.print_logs()\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/ISerializable.py",
        "content": "```swarmauri/experimental/utils/ISerializable.py\n\n# class Serializable:\n#     def serialize(self):\n#         raise NotImplementedError(\"Serialization method not implemented\")\n    \n#     @classmethod\n#     def deserialize(cls, data):\n#         raise NotImplementedError(\"Deserialization method not implemented\")\n        \n        \n# class ToolAgent(Serializable):\n#     def serialize(self):\n#         # Simplified example, adapt according to actual attributes\n#         return {\"type\": self.__class__.__name__, \"state\": {\"model_name\": self.model.model_name}}\n\n#     @classmethod\n#     def deserialize(cls, data):\n#         # This method should instantiate the object based on the serialized state.\n#         # Example assumes the presence of model_name in the serialized state.\n#         model = OpenAIToolModel(api_key=\"api_key_placeholder\", model_name=data[\"state\"][\"model_name\"])\n#         return cls(model=model, conversation=None, toolkit=None)  # Simplify, omit optional parameters for illustration\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/log_prompt_response.py",
        "content": "```swarmauri/experimental/utils/log_prompt_response.py\nimport sqlite3\nfrom functools import wraps\n\ndef log_prompt_response(db_path):\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extracting the 'message' parameter from args which is assumed to be the first argument\n            message = args[0]  \n            response = await func(*args, **kwargs)\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n            \n            # Create table if it doesn't exist\n            cursor.execute('''CREATE TABLE IF NOT EXISTS prompts_responses\n                            (id INTEGER PRIMARY KEY AUTOINCREMENT, \n                             prompt TEXT, \n                             response TEXT)''')\n            \n            # Insert a new record\n            cursor.execute('''INSERT INTO prompts_responses (prompt, response) \n                            VALUES (?, ?)''', (message, response))\n            conn.commit()\n            conn.close()\n            return response\n        \n        return wrapper\n    return decorator\n```"
    },
    {
        "document_name": "swarmauri/experimental/parsers/__init__.py",
        "content": "```swarmauri/experimental/parsers/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/parsers/PDFToTextParser.py",
        "content": "```swarmauri/experimental/parsers/PDFToTextParser.py\nimport fitz  # PyMuPDF\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.ConcreteDocument import ConcreteDocument\n\nclass PDFtoTextParser(IParser):\n    \"\"\"\n    A parser to extract text from PDF files.\n    \"\"\"\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses a PDF file and extracts its text content as Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The path to the PDF file.\n\n        Returns:\n        - List[IDocument]: A list with a single IDocument instance containing the extracted text.\n        \"\"\"\n        # Ensure data is a valid str path to a PDF file\n        if not isinstance(data, str):\n            raise ValueError(\"PDFtoTextParser expects a file path in str format.\")\n\n        try:\n            # Open the PDF file\n            doc = fitz.open(data)\n            text = \"\"\n\n            # Extract text from each page\n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                text += page.get_text()\n\n            # Create a document with the extracted text\n            document = ConcreteDocument(doc_id=str(hash(data)), content=text, metadata={\"source\": data})\n            return [document]\n        \n        except Exception as e:\n            print(f\"An error occurred while parsing the PDF: {e}\")\n            return []\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/__init__.py",
        "content": "```swarmauri/experimental/vector_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/Word2VecDocumentStore.py",
        "content": "```swarmauri/experimental/vector_stores/Word2VecDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nimport gensim.downloader as api\n\nclass Word2VecDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self):\n        \"\"\"\n        Initializes the Word2VecDocumentStore.\n\n        Parameters:\n        - word2vec_model_path (Optional[str]): File path to a pre-trained Word2Vec model. \n                                               Leave None to use Gensim's pre-trained model.\n        - pre_trained (bool): If True, loads a pre-trained Word2Vec model. If False, an uninitialized model is used that requires further training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)  # Example parameters; adjust as needed\n        self.documents = []\n        self.metric = CosineDistance()\n\n    def add_document(self, document: EmbeddedDocument) -> None:\n        # Check if the document already has an embedding, if not generate one using _average_word_vectors\n        if not hasattr(document, 'embedding') or document.embedding is None:\n            words = document.content.split()  # Simple tokenization, consider using a better tokenizer\n            embedding = self._average_word_vectors(words)\n            document.embedding = embedding\n            print(document.embedding)\n        self.documents.append(document)\n        \n    def add_documents(self, documents: List[EmbeddedDocument]) -> None:\n        self.documents.extend(documents)\n        \n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n        \n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str, updated_document: EmbeddedDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n    def _average_word_vectors(self, words: List[str]) -> np.ndarray:\n        \"\"\"\n        Generate document vector by averaging its word vectors.\n        \"\"\"\n        word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n        print(word_vectors)\n        if word_vectors:\n            return np.mean(word_vectors, axis=0)\n        else:\n            return np.zeros(self.model.vector_size)\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[EmbeddedDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string based on Word2Vec embeddings.\n        \"\"\"\n        query_vector = self._average_word_vectors(query.split())\n        print('query_vector', query_vector)\n        # Compute similarity scores between the query and each document's stored embedding\n        similarities = self.metric.similarities(SimpleVector(query_vector), [SimpleVector(doc.embedding) for doc in self.documents if doc.embedding])\n        print('similarities', similarities)\n        # Retrieve indices of top_k most similar documents\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n        print('top_k_indices', top_k_indices)\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/TriplesDocumentStore.py",
        "content": "```swarmauri/experimental/vector_stores/TriplesDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom rdflib import Graph, URIRef, Literal, BNode\nfrom ampligraph.latent_features import ComplEx\nfrom ampligraph.evaluation import train_test_split_no_unseen\nfrom ampligraph.latent_features import EmbeddingModel\nfrom ampligraph.utils import save_model, restore_model\n\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nfrom swarmauri.standard.vectorizers.concrete.AmpligraphVectorizer import AmpligraphVectorizer\n\n\nclass TriplesDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self, rdf_file_path: str, model_path: Optional[str] = None):\n        \"\"\"\n        Initializes the TriplesDocumentStore.\n        \"\"\"\n        self.graph = Graph()\n        self.rdf_file_path = rdf_file_path\n        self.graph.parse(rdf_file_path, format='turtle')\n        self.documents = []\n        self.vectorizer = AmpligraphVectorizer()\n        self.model_path = model_path\n        if model_path:\n            self.model = restore_model(model_path)\n        else:\n            self.model = None\n        self.metric = CosineDistance()\n        self._load_documents()\n        if not self.model:\n            self._train_model()\n\n    def _train_model(self):\n        \"\"\"\n        Trains a model based on triples in the graph.\n        \"\"\"\n        # Extract triples for embedding model\n        triples = np.array([[str(s), str(p), str(o)] for s, p, o in self.graph])\n        # Split data\n        train, test = train_test_split_no_unseen(triples, test_size=0.1)\n        self.model = ComplEx(batches_count=100, seed=0, epochs=20, k=150, eta=1,\n                             optimizer='adam', optimizer_params={'lr': 1e-3},\n                             loss='pairwise', regularizer='LP', regularizer_params={'p': 3, 'lambda': 1e-5},\n                             verbose=True)\n        self.model.fit(train)\n        if self.model_path:\n            save_model(self.model, self.model_path)\n\n    def _load_documents(self):\n        \"\"\"\n        Load documents into the store from the RDF graph.\n        \"\"\"\n        for subj, pred, obj in self.graph:\n            doc_id = str(hash((subj, pred, obj)))\n            content = f\"{subj} {pred} {obj}\"\n            document = Document(content=content, doc_id=doc_id, metadata={})\n            self.documents.append(document)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Adds a single RDF triple document.\n        \"\"\"\n        subj, pred, obj = document.content.split()  # Splitting content into RDF components\n        self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.append(document)\n        self._train_model()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Adds multiple RDF triple documents.\n        \"\"\"\n        for document in documents:\n            subj, pred, obj = document.content.split()  # Assuming each document's content is \"subj pred obj\"\n            self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.extend(documents)\n        self._train_model()\n\n    # Implementation for get_document, get_all_documents, delete_document, update_document remains same as before\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string.\n        \"\"\"\n        if not self.model:\n            self._train_model()\n        query_vector = self.vectorizer.infer_vector(model=self.model, samples=[query])[0]\n        document_vectors = [self.vectorizer.infer_vector(model=self.model, samples=[doc.content])[0] for doc in self.documents]\n        similarities = self.metric.distances(SimpleVector(data=query_vector), [SimpleVector(vector) for vector in document_vectors])\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/ScannVectorStore.py",
        "content": "```swarmauri/experimental/vector_stores/ScannVectorStore.py\nimport numpy as np\nimport scann\nfrom typing import List, Dict, Union\n\nfrom swarmauri.core.vector_stores.IVectorStore import IVectorStore\nfrom swarmauri.core.vector_stores.ISimiliarityQuery import ISimilarityQuery\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\n\nclass ScannVectorStore(IVectorStore, ISimilarityQuery):\n    \"\"\"\n    A vector store that utilizes ScaNN (Scalable Nearest Neighbors) for efficient similarity searches.\n    \"\"\"\n\n    def __init__(self, dimension: int, num_leaves: int = 100, num_leaves_to_search: int = 10, reordering_num_neighbors: int = 100):\n        \"\"\"\n        Initialize the ScaNN vector store with given parameters.\n\n        Parameters:\n        - dimension (int): The dimensionality of the vectors being stored.\n        - num_leaves (int): The number of leaves for the ScaNN partitioning tree.\n        - num_leaves_to_search (int): The number of leaves to search for query time. Must be <= num_leaves.\n        - reordering_num_neighbors (int): The number of neighbors to re-rank based on the exact distance after searching leaves.\n        \"\"\"\n        self.dimension = dimension\n        self.num_leaves = num_leaves\n        self.num_leaves_to_search = num_leaves_to_search\n        self.reordering_num_neighbors = reordering_num_neighbors\n\n        self.searcher = None  # Placeholder for the ScaNN searcher initialized during building\n        self.dataset_vectors = []\n        self.id_to_metadata = {}\n\n    def _build_scann_searcher(self):\n        \"\"\"Build the ScaNN searcher based on current dataset vectors.\"\"\"\n        self.searcher = scann.ScannBuilder(np.array(self.dataset_vectors, dtype=np.float32), num_neighbors=self.reordering_num_neighbors, distance_measure=\"dot_product\").tree(\n            num_leaves=self.num_leaves, num_leaves_to_search=self.num_leaves_to_search, training_sample_size=25000\n        ).score_ah(\n            dimensions_per_block=2\n        ).reorder(self.reordering_num_neighbors).build()\n\n    def add_vector(self, vector_id: str, vector: Union[np.ndarray, List[float]], metadata: Dict = None) -> None:\n        \"\"\"\n        Adds a vector along with its identifier and optional metadata to the store.\n\n        Args:\n            vector_id (str): Unique identifier for the vector.\n            vector (Union[np.ndarray, List[float]]): The high-dimensional vector to be stored.\n            metadata (Dict, optional): Optional metadata related to the vector.\n        \"\"\"\n        if not isinstance(vector, np.ndarray):\n            vector = np.array(vector, dtype=np.float32)\n        \n        if self.searcher is None:\n            self.dataset_vectors.append(vector)\n        else:\n            raise Exception(\"Cannot add vectors after building the index. Rebuild the index to include new vectors.\")\n\n        if metadata is None:\n            metadata = {}\n        self.id_to_metadata[vector_id] = metadata\n\n    def build_index(self):\n        \"\"\"Builds or rebuilds the ScaNN searcher to reflect the current dataset vectors.\"\"\"\n        self._build_scann_searcher()\n\n    def get_vector(self, vector_id: str) -> Union[IVector, None]:\n        \"\"\"\n        Retrieve a vector by its identifier.\n\n        Args:\n            vector_id (str): The unique identifier for the vector.\n\n        Returns:\n            Union[IVector, None]: The vector associated with the given id, or None if not found.\n        \"\"\"\n        if vector_id in self.id_to_metadata:\n            metadata = self.id_to_metadata[vector_id]\n            return SimpleVector(data=metadata.get('vector'), metadata=metadata)\n        return None\n\n    def delete_vector(self, vector_id: str) -> None:\n        \"\"\"\n        Deletes a vector from the ScannVectorStore and marks the index for rebuilding.\n        Note: For simplicity, this function assumes vectors are uniquely identifiable by their metadata.\n\n        Args:\n            vector_id (str): The unique identifier for the vector to be deleted.\n        \"\"\"\n        if vector_id in self.id_to_metadata:\n            # Identify index of the vector to be deleted\n            vector = self.id_to_metadata[vector_id]['vector']\n            index = self.dataset_vectors.index(vector)\n\n            # Remove vector and its metadata\n            del self.dataset_vectors[index]\n            del self.id_to_metadata[vector_id]\n\n            # Since vector order is important for matching ids, rebuild the searcher to reflect deletion\n            self.searcher = None\n        else:\n            # Handle case where vector_id is not found\n            print(f\"Vector ID {vector_id} not found.\")\n\n    def update_vector(self, vector_id: str, new_vector: Union[np.ndarray, List[float]], new_metadata: Dict = None) -> None:\n        \"\"\"\n        Updates an existing vector in the ScannVectorStore and marks the index for rebuilding.\n\n        Args:\n            vector_id (str): The unique identifier for the vector to be updated.\n            new_vector (Union[np.ndarray, List[float]]): The updated vector.\n            new_metadata (Dict, optional): Optional updated metadata for the vector.\n        \"\"\"\n        # Ensure new_vector is numpy array for consistency\n        if not isinstance(new_vector, np.ndarray):\n            new_vector = np.array(new_vector, dtype=np.float32)\n\n        if vector_id in self.id_to_metadata:\n            # Update operation follows delete then add strategy because vector order matters in ScaNN\n            self.delete_vector(vector_id)\n            self.add_vector(vector_id, new_vector, new_metadata)\n        else:\n            # Handle case where vector_id is not found\n            print(f\"Vector ID {vector_id} not found.\")\n\n\n\n    def search_by_similarity_threshold(self, query_vector: Union[np.ndarray, List[float]], similarity_threshold: float, space_name: str = None) -> List[Dict]:\n        \"\"\"\n        Search vectors exceeding a similarity threshold to a query vector within an optional vector space.\n\n        Args:\n            query_vector (Union[np.ndarray, List[float]]): The high-dimensional query vector.\n            similarity_threshold (float): The similarity threshold for filtering results.\n            space_name (str, optional): The name of the vector space to search within. Not used in this implementation.\n\n        Returns:\n            List[Dict]: A list of dictionaries with vector IDs, similarity scores, and optional metadata that meet the similarity threshold.\n        \"\"\"\n        if not isinstance(query_vector, np.ndarray):\n            query_vector = np.array(query_vector, dtype=np.float32)\n        \n        if self.searcher is None:\n            self._build_scann_searcher()\n        \n        _, indices = self.searcher.search(query_vector, final_num_neighbors=self.reordering_num_neighbors)\n        results = [{\"id\": str(idx), \"metadata\": self.id_to_metadata.get(str(idx), {})} for idx in indices if idx < similarity_threshold]\n        return results\n```"
    },
    {
        "document_name": "swarmauri/experimental/tracing/RemoteTrace.py",
        "content": "```swarmauri/experimental/tracing/RemoteTrace.py\nfrom __future__ import ITraceContext\n\nimport requests\nimport json\nimport uuid\nfrom datetime import datetime\n\nfrom swarmauri.core.tracing.ITracer import ITracer\nfrom swarmauri.core.tracing.ITraceContext import ITraceContext\n\n# Implementing the RemoteTraceContext class\nclass RemoteTraceContext(ITraceContext):\n    def __init__(self, trace_id: str, name: str):\n        self.trace_id = trace_id\n        self.name = name\n        self.start_time = datetime.now()\n        self.attributes = {}\n        self.annotations = {}\n\n    def get_trace_id(self) -> str:\n        return self.trace_id\n\n    def add_attribute(self, key: str, value):\n        self.attributes[key] = value\n        \n    def add_annotation(self, key: str, value):\n        self.annotations[key] = value\n\n# Implementing the RemoteAPITracer class\nclass RemoteAPITracer(ITracer):\n    def __init__(self, api_endpoint: str):\n        self.api_endpoint = api_endpoint\n\n    def start_trace(self, name: str, initial_attributes=None) -> 'RemoteTraceContext':\n        trace_id = str(uuid.uuid4())\n        context = RemoteTraceContext(trace_id, name)\n        if initial_attributes:\n            for key, value in initial_attributes.items():\n                context.add_attribute(key, value)\n        return context\n\n    def end_trace(self, trace_context: 'RemoteTraceContext'):\n        trace_context.end_time = datetime.now()\n        # Pretending to serialize the context information to JSON\n        trace_data = {\n            \"trace_id\": trace_context.get_trace_id(),\n            \"name\": trace_context.name,\n            \"start_time\": str(trace_context.start_time),\n            \"end_time\": str(trace_context.end_time),\n            \"attributes\": trace_context.attributes,\n            \"annotations\": trace_context.annotations\n        }\n        json_data = json.dumps(trace_data)\n        # POST the serialized data to the remote REST API\n        response = requests.post(self.api_endpoint, json=json_data)\n        if not response.ok:\n            raise Exception(f\"Failed to send trace data to {self.api_endpoint}. Status code: {response.status_code}\")\n\n    def annotate_trace(self, trace_context: 'RemoteTraceContext', key: str, value):\n        trace_context.add_annotation(key, value)\n```"
    },
    {
        "document_name": "swarmauri/experimental/tracing/__init__.py",
        "content": "```swarmauri/experimental/tracing/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/TypeAgnosticCallableChain.py",
        "content": "```swarmauri/experimental/chains/TypeAgnosticCallableChain.py\nfrom typing import Any, Callable, List, Dict, Optional, Tuple, Union\n\nCallableDefinition = Tuple[Callable, List[Any], Dict[str, Any], Union[str, Callable, None]]\n\nclass TypeAgnosticCallableChain:\n    def __init__(self, callables: Optional[List[CallableDefinition]] = None):\n        self.callables = callables if callables is not None else []\n\n    @staticmethod\n    def _ignore_previous(_previous_result, *args, **kwargs):\n        return args, kwargs\n\n    @staticmethod\n    def _use_first_arg(previous_result, *args, **kwargs):\n        return [previous_result] + list(args), kwargs\n\n    @staticmethod\n    def _use_all_previous_args_first(previous_result, *args, **kwargs):\n        if not isinstance(previous_result, (list, tuple)):\n            previous_result = [previous_result]\n        return list(previous_result) + list(args), kwargs\n\n    @staticmethod\n    def _use_all_previous_args_only(previous_result, *_args, **_kwargs):\n        if not isinstance(previous_result, (list, tuple)):\n            previous_result = [previous_result]\n        return list(previous_result), {}\n\n    @staticmethod\n    def _add_previous_kwargs_overwrite(previous_result, args, kwargs):\n        if not isinstance(previous_result, dict):\n            raise ValueError(\"Previous result is not a dictionary.\")\n        return args, {**kwargs, **previous_result}\n\n    @staticmethod\n    def _add_previous_kwargs_no_overwrite(previous_result, args, kwargs):\n        if not isinstance(previous_result, dict):\n            raise ValueError(\"Previous result is not a dictionary.\")\n        return args, {**previous_result, **kwargs}\n\n    @staticmethod\n    def _use_all_args_all_kwargs_overwrite(previous_result_args, previous_result_kwargs, *args, **kwargs):\n        combined_args = list(previous_result_args) + list(args) if isinstance(previous_result_args, (list, tuple)) else list(args)\n        combined_kwargs = previous_result_kwargs if isinstance(previous_result_kwargs, dict) else {}\n        combined_kwargs.update(kwargs)\n        return combined_args, combined_kwargs\n\n    @staticmethod\n    def _use_all_args_all_kwargs_no_overwrite(previous_result_args, previous_result_kwargs, *args, **kwargs):\n        combined_args = list(previous_result_args) + list(args) if isinstance(previous_result_args, (list, tuple)) else list(args)\n        combined_kwargs = kwargs if isinstance(kwargs, dict) else {}\n        combined_kwargs = {**combined_kwargs, **(previous_result_kwargs if isinstance(previous_result_kwargs, dict) else {})}\n        return combined_args, combined_kwargs\n\n    def add_callable(self, func: Callable, args: List[Any] = None, kwargs: Dict[str, Any] = None, input_handler: Union[str, Callable, None] = None) -> None:\n        if isinstance(input_handler, str):\n            # Map the string to the corresponding static method\n            input_handler_method = getattr(self, f\"_{input_handler}\", None)\n            if input_handler_method is None:\n                raise ValueError(f\"Unknown input handler name: {input_handler}\")\n            input_handler = input_handler_method\n        elif input_handler is None:\n            input_handler = self._ignore_previous\n        self.callables.append((func, args or [], kwargs or {}, input_handler))\n\n    def __call__(self, *initial_args, **initial_kwargs) -> Any:\n        result = None\n        for func, args, kwargs, input_handler in self.callables:\n            if isinstance(input_handler, str):\n                # Map the string to the corresponding static method\n                input_handler_method = getattr(self, f\"_{input_handler}\", None)\n                if input_handler_method is None:\n                    raise ValueError(f\"Unknown input handler name: {input_handler}\")\n                input_handler = input_handler_method\n            elif input_handler is None:\n                input_handler = self._ignore_previous\n                \n            args, kwargs = input_handler(result, *args, **kwargs) if result is not None else (args, kwargs)\n            result = func(*args, **kwargs)\n        return result\n\n    def __or__(self, other: \"TypeAgnosticCallableChain\") -> \"TypeAgnosticCallableChain\":\n        if not isinstance(other, TypeAgnosticCallableChain):\n            raise TypeError(\"Operand must be an instance of TypeAgnosticCallableChain\")\n        \n        new_chain = TypeAgnosticCallableChain(self.callables + other.callables)\n        return new_chain\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/__init__.py",
        "content": "```swarmauri/experimental/chains/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainFormatter.py",
        "content": "```swarmauri/experimental/chains/IChainFormatter.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass IChainFormatter(ABC):\n    @abstractmethod\n    def format_output(self, step: IChainStep, output: Any) -> str:\n        \"\"\"Format the output of a specific chain step.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainNotification.py",
        "content": "```swarmauri/experimental/chains/IChainNotification.py\nfrom abc import ABC, abstractmethod\n\nclass IChainNotifier(ABC):\n    @abstractmethod\n    def send_notification(self, message: str) -> None:\n        \"\"\"Send a notification message based on chain execution results.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainPersistence.py",
        "content": "```swarmauri/experimental/chains/IChainPersistence.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\nfrom swarmauri.core.chains.IChain import IChain\n\nclass IChainPersistence(ABC):\n    @abstractmethod\n    def save_state(self, chain: IChain, state: Dict[str, Any]) -> None:\n        \"\"\"Save the state of the given chain.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_state(self, chain_id: str) -> Dict[str, Any]:\n        \"\"\"Load the state of a chain by its identifier.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainScheduler.py",
        "content": "```swarmauri/experimental/chains/IChainScheduler.py\nfrom abc import ABC, abstractmethod\nfrom swarmauri.core.chains.IChain import IChain\n\nclass IChainScheduler(ABC):\n    @abstractmethod\n    def schedule_chain(self, chain: IChain, schedule: str) -> None:\n        \"\"\"Schedule the execution of the given chain.\"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/TriplesDocumentStore.py",
        "content": "```swarmauri/experimental/document_stores/TriplesDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom rdflib import Graph, URIRef, Literal, BNode\nfrom ampligraph.latent_features import ComplEx\nfrom ampligraph.evaluation import train_test_split_no_unseen\nfrom ampligraph.latent_features import EmbeddingModel\nfrom ampligraph.utils import save_model, restore_model\n\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nfrom swarmauri.standard.vectorizers.concrete.AmpligraphVectorizer import AmpligraphVectorizer\n\n\nclass TriplesDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self, rdf_file_path: str, model_path: Optional[str] = None):\n        \"\"\"\n        Initializes the TriplesDocumentStore.\n        \"\"\"\n        self.graph = Graph()\n        self.rdf_file_path = rdf_file_path\n        self.graph.parse(rdf_file_path, format='turtle')\n        self.documents = []\n        self.vectorizer = AmpligraphVectorizer()\n        self.model_path = model_path\n        if model_path:\n            self.model = restore_model(model_path)\n        else:\n            self.model = None\n        self.metric = CosineDistance()\n        self._load_documents()\n        if not self.model:\n            self._train_model()\n\n    def _train_model(self):\n        \"\"\"\n        Trains a model based on triples in the graph.\n        \"\"\"\n        # Extract triples for embedding model\n        triples = np.array([[str(s), str(p), str(o)] for s, p, o in self.graph])\n        # Split data\n        train, test = train_test_split_no_unseen(triples, test_size=0.1)\n        self.model = ComplEx(batches_count=100, seed=0, epochs=20, k=150, eta=1,\n                             optimizer='adam', optimizer_params={'lr': 1e-3},\n                             loss='pairwise', regularizer='LP', regularizer_params={'p': 3, 'lambda': 1e-5},\n                             verbose=True)\n        self.model.fit(train)\n        if self.model_path:\n            save_model(self.model, self.model_path)\n\n    def _load_documents(self):\n        \"\"\"\n        Load documents into the store from the RDF graph.\n        \"\"\"\n        for subj, pred, obj in self.graph:\n            doc_id = str(hash((subj, pred, obj)))\n            content = f\"{subj} {pred} {obj}\"\n            document = Document(content=content, doc_id=doc_id, metadata={})\n            self.documents.append(document)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Adds a single RDF triple document.\n        \"\"\"\n        subj, pred, obj = document.content.split()  # Splitting content into RDF components\n        self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.append(document)\n        self._train_model()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Adds multiple RDF triple documents.\n        \"\"\"\n        for document in documents:\n            subj, pred, obj = document.content.split()  # Assuming each document's content is \"subj pred obj\"\n            self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.extend(documents)\n        self._train_model()\n\n    # Implementation for get_document, get_all_documents, delete_document, update_document remains same as before\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string.\n        \"\"\"\n        if not self.model:\n            self._train_model()\n        query_vector = self.vectorizer.infer_vector(model=self.model, samples=[query])[0]\n        document_vectors = [self.vectorizer.infer_vector(model=self.model, samples=[doc.content])[0] for doc in self.documents]\n        similarities = self.metric.distances(SimpleVector(data=query_vector), [SimpleVector(vector) for vector in document_vectors])\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/Word2VecDocumentStore.py",
        "content": "```swarmauri/experimental/document_stores/Word2VecDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nimport gensim.downloader as api\n\nclass Word2VecDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self):\n        \"\"\"\n        Initializes the Word2VecDocumentStore.\n\n        Parameters:\n        - word2vec_model_path (Optional[str]): File path to a pre-trained Word2Vec model. \n                                               Leave None to use Gensim's pre-trained model.\n        - pre_trained (bool): If True, loads a pre-trained Word2Vec model. If False, an uninitialized model is used that requires further training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)  # Example parameters; adjust as needed\n        self.documents = []\n        self.metric = CosineDistance()\n\n    def add_document(self, document: EmbeddedDocument) -> None:\n        # Check if the document already has an embedding, if not generate one using _average_word_vectors\n        if not hasattr(document, 'embedding') or document.embedding is None:\n            words = document.content.split()  # Simple tokenization, consider using a better tokenizer\n            embedding = self._average_word_vectors(words)\n            document.embedding = embedding\n            print(document.embedding)\n        self.documents.append(document)\n        \n    def add_documents(self, documents: List[EmbeddedDocument]) -> None:\n        self.documents.extend(documents)\n        \n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n        \n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str, updated_document: EmbeddedDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n    def _average_word_vectors(self, words: List[str]) -> np.ndarray:\n        \"\"\"\n        Generate document vector by averaging its word vectors.\n        \"\"\"\n        word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n        print(word_vectors)\n        if word_vectors:\n            return np.mean(word_vectors, axis=0)\n        else:\n            return np.zeros(self.model.vector_size)\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[EmbeddedDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string based on Word2Vec embeddings.\n        \"\"\"\n        query_vector = self._average_word_vectors(query.split())\n        print('query_vector', query_vector)\n        # Compute similarity scores between the query and each document's stored embedding\n        similarities = self.metric.similarities(SimpleVector(query_vector), [SimpleVector(doc.embedding) for doc in self.documents if doc.embedding])\n        print('similarities', similarities)\n        # Retrieve indices of top_k most similar documents\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n        print('top_k_indices', top_k_indices)\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/__init__.py",
        "content": "```swarmauri/experimental/document_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SSASimilarity.py",
        "content": "```swarmauri/experimental/distances/SSASimilarity.py\nfrom typing import Set, List, Dict\nfrom ....core.vector_stores.ISimilarity import ISimilarity\nfrom ....core.vectors.IVector import IVector\n\n\nclass SSASimilarity(ISimilarity):\n    \"\"\"\n    Implements the State Similarity in Arity (SSA) similarity measure to\n    compare states (sets of variables) for their similarity.\n    \"\"\"\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Calculate the SSA similarity between two documents by comparing their metadata,\n        assumed to represent states as sets of variables.\n\n        Args:\n        - vector_a (IDocument): The first document.\n        - vector_b (IDocument): The second document to compare with the first document.\n\n        Returns:\n        - float: The SSA similarity measure between vector_a and vector_b, ranging from 0 to 1\n                 where 0 represents no similarity and 1 represents identical states.\n        \"\"\"\n        state_a = set(vector_a.metadata.keys())\n        state_b = set(vector_b.metadata.keys())\n\n        return self.calculate_ssa(state_a, state_b)\n\n    @staticmethod\n    def calculate_ssa(state_a: Set[str], state_b: Set[str]) -> float:\n        \"\"\"\n        Calculate the State Similarity in Arity (SSA) between two states.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n\n        Returns:6\n        - float: The SSA similarity measure, ranging from 0 (no similarity) to 1 (identical states).\n        \"\"\"\n        # Calculate the intersection (shared variables) between the two states\n        shared_variables = state_a.intersection(state_b)\n        \n        # Calculate the union (total unique variables) of the two states\n        total_variables = state_a.union(state_b)\n        \n        # Calculate the SSA measure as the ratio of shared to total variables\n        ssa = len(shared_variables) / len(total_variables) if total_variables else 1\n        \n        return ssa\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SSIVSimilarity.py",
        "content": "```swarmauri/experimental/distances/SSIVSimilarity.py\nfrom typing import List, Dict, Set\nfrom ....core.vector_stores.ISimilarity import ISimilarity\n\nclass SSIVSimilarity(ISimilarity):\n    \"\"\"\n    Concrete class that implements ISimilarity interface using\n    State Similarity of Important Variables (SSIV) as the similarity measure.\n    \"\"\"\n\n    def similarity(self, state_a: Set[str], state_b: Set[str], importance_a: Dict[str, float], importance_b: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate the SSIV between two states represented by sets of variables.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n        - importance_a (Dict[str, float]): A dictionary where keys are variables in state A and values are their importance weights.\n        - importance_b (Dict[str, float]): A dictionary where keys are variables in state B and values are their importance weights.\n\n        Returns:\n        - float: The SSIV similarity measure, ranging from 0 to 1.\n        \"\"\"\n        return self.calculate_ssiv(state_a, state_b, importance_a, importance_b)\n\n    @staticmethod\n    def calculate_ssiv(state_a: Set[str], state_b: Set[str], importance_a: Dict[str, float], importance_b: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate the State Similarity of Important Variables (SSIV) between two states.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n        - importance_a (Dict[str, float]): A dictionary where keys are variables in state A and values are their importance weights.\n        - importance_b (Dict[str, float]): A dictionary where keys are variables in state B and values are their importance weights.\n\n        Returns:\n        - float: The SSIV similarity measure, ranging from 0 to 1.\n        \n        Note: It is assumed that the importance weights are non-negative.\n        \"\"\"\n        shared_variables = state_a.intersection(state_b)\n        \n        # Calculate the summed importance of shared variables\n        shared_importance_sum = sum(importance_a[var] for var in shared_variables) + sum(importance_b[var] for var in shared_variables)\n        \n        # Calculate the total importance of all variables in both states\n        total_importance_sum = sum(importance_a.values()) + sum(importance_b.values())\n        \n        # Calculate and return the SSIV\n        ssiv = (2 * shared_importance_sum) / total_importance_sum if total_importance_sum != 0 else 0\n        return ssiv\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/__init__.py",
        "content": "```swarmauri/experimental/distances/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/apis/CeleryAgentCommands.py",
        "content": "```swarmauri/experimental/apis/CeleryAgentCommands.py\nfrom celery import Celery\nfrom swarmauri.core.agent_apis.IAgentCommands import IAgentCommands\nfrom typing import Callable, Any, Dict\n\nclass CeleryAgentCommands(IAgentCommands):\n    def __init__(self, broker_url: str, backend_url: str):\n        \"\"\"\n        Initializes the Celery application with the specified broker and backend URLs.\n        \"\"\"\n        self.app = Celery('swarmauri_agent_tasks', broker=broker_url, backend=backend_url)\n\n    def register_command(self, command_name: str, function: Callable[..., Any], *args, **kwargs) -> None:\n        \"\"\"\n        Registers a new command as a Celery task.\n        \"\"\"\n        self.app.task(name=command_name, bind=True)(function)\n\n    def execute_command(self, command_name: str, *args, **kwargs) -> Any:\n        \"\"\"\n        Executes a registered command by name asynchronously.\n        \"\"\"\n        result = self.app.send_task(command_name, args=args, kwargs=kwargs)\n        return result.get()\n\n    def get_status(self, task_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Fetches the status of a command execution via its task ID.\n        \"\"\"\n        async_result = self.app.AsyncResult(task_id)\n        return {\"status\": async_result.status, \"result\": async_result.result if async_result.ready() else None}\n\n    def revoke_command(self, task_id: str) -> None:\n        \"\"\"\n        Revokes or terminates a command execution by its task ID.\n        \"\"\"\n        self.app.control.revoke(task_id, terminate=True)\n```"
    },
    {
        "document_name": "swarmauri/experimental/embeddings/SpatialDocEmbedding.py",
        "content": "```swarmauri/experimental/embeddings/SpatialDocEmbedding.py\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom torch import nn\nimport numpy as np\nfrom typing import Literal\nfrom pydantic import PrivateAttr\n\nfrom swarmauri.standard.embeddings.base.EmbeddingBase import EmbeddingBase\nfrom swarmauri.standard.vectors.concrete.Vector import Vector\n\nclass SpatialDocEmbedding(EmbeddingBase):\n    _special_tokens_dict = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _model = PrivateAttr()\n    _device = PrivateAttr()\n    type: Literal['SpatialDocEmbedding'] = 'SpatialDocEmbedding'\n    \n    def __init__(self, special_tokens_dict=None, **kwargs):\n        super().__init__(**kwargs)\n        self._special_tokens_dict = special_tokens_dict or {\n            'additional_special_tokens': [\n                '[DIR]', '[TYPE]', '[SECTION]', '[PATH]',\n                '[PARAGRAPH]', '[SUBPARAGRAPH]', '[CHAPTER]', '[TITLE]', '[SUBSECTION]'\n            ]\n        }\n        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self._tokenizer.add_special_tokens(self._special_tokens_dict)\n        self._model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n        self._model.resize_token_embeddings(len(self._tokenizer))\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._model.to(self._device)\n\n    def add_metadata(self, text, metadata_dict):\n        metadata_components = []\n        for key, value in metadata_dict.items():\n            if f\"[{key.upper()}]\" in self._special_tokens_dict['additional_special_tokens']:\n                token = f\"[{key.upper()}={value}]\"\n                metadata_components.append(token)\n        metadata_str = ' '.join(metadata_components)\n        return metadata_str + ' ' + text if metadata_components else text\n\n    def tokenize_and_encode(self, text):\n        inputs = self._tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        # Move the input tensors to the same device as the model\n        inputs = {key: value.to(self._device) for key, value in inputs.items()}\n        outputs = self._model(**inputs)\n        return outputs.pooler_output\n\n    def enhance_embedding_with_positional_info(self, embeddings, doc_position, total_docs):\n        position_effect = torch.sin(torch.tensor(doc_position / total_docs, dtype=torch.float))\n        enhanced_embeddings = embeddings + position_effect\n        return enhanced_embeddings\n\n    def vectorize_document(self, chunks, metadata_list=None):\n        all_embeddings = []\n        total_chunks = len(chunks)\n        if not metadata_list:\n            # Default empty metadata if none provided\n            metadata_list = [{} for _ in chunks]\n        \n        for i, (chunk, metadata) in enumerate(zip(chunks, metadata_list)):\n            # Use add_metadata to include any available metadata dynamically\n            embedded_text = self.add_metadata(chunk, metadata)\n            embeddings = self.tokenize_and_encode(embedded_text)\n            enhanced_embeddings = self.enhance_embedding_with_positional_info(embeddings, i, total_chunks)\n            all_embeddings.append(enhanced_embeddings)\n\n        return all_embeddings\n\n\n\n    def fit(self, data):\n        # Although this vectorizer might not need to be fitted in the traditional sense,\n        # this method placeholder allows integration into pipelines that expect a fit method.\n        pass\n\n    def transform(self, data):\n        print(data)\n        if isinstance(data, list):\n            return [self.infer_vector(text).value for text in data]\n        else:\n            return self.infer_vector(data).value\n\n    def fit_transform(self, data):\n        #self.fit(data)\n        return self.transform(data)\n\n    def infer_vector(self, data, *args, **kwargs):\n        print(data)\n        inputs = self.tokenize_and_encode(data)\n        print(inputs)\n        inputs = inputs.cpu().detach().numpy().tolist()\n        print(inputs)\n        return Vector(value=[1,2,3]) # Placeholder\n\n    def save_model(self, path):\n        torch.save({\n            'model_state_dict': self._model.state_dict(),\n            'tokenizer': self._tokenizer\n        }, path)\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self._model.load_state_dict(checkpoint['model_state_dict'])\n        self._tokenizer = checkpoint['tokenizer']\n\n    def extract_features(self, text):\n        inputs = self.tokenize_and_encode(text)\n        return Vector(value=inputs.cpu().detach().numpy().tolist())\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/embeddings/__init__.py",
        "content": "```swarmauri/experimental/embeddings/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vectors/__init__.py",
        "content": "```swarmauri/experimental/vectors/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/llms/ShuttleAIModel.py",
        "content": "```swarmauri/experimental/llms/ShuttleAIModel.py\nimport logging\nimport json\nfrom typing import List, Dict, Literal\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase \n\nimport requests \n\nclass ShuttleAIModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n    \"shuttle-2-turbo\", \"shuttle-turbo\", \"gpt-4o-2024-05-13\", \"gpt-4-turbo-2024-04-09\",\n    \"gpt-4-0125-preview\", \"gpt-4-1106-preview\", \"gpt-4-1106-vision-preview\", \"gpt-4-0613\",\n    \"gpt-4-bing\", \"gpt-4-turbo-bing\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-0125\",\n    \"gpt-3.5-turbo-1106\", \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\",\n    \"claude-2.1\", \"claude-2.0\", \"claude-instant-1.2\", \"claude-instant-1.1\",\n    \"claude-instant-1.0\", \"meta-llama-3-70b-instruct\", \"meta-llama-3-8b-instruct\", \"llama-3-sonar-large-32k-online\",\n    \"llama-3-sonar-small-32k-online\", \"llama-3-sonar-large-32k-chat\", \"llama-3-sonar-small-32k-chat\", \"blackbox\",\n    \"blackbox-code\", \"wizardlm-2-8x22b\", \"wizardlm-2-70b\", \"dolphin-2.6-mixtral-8x7b\",\n    \"codestral-latest\", \"mistral-large\", \"mistral-next\", \"mistral-medium\",\n    \"mistral-small\", \"mistral-tiny\", \"mixtral-8x7b-instruct-v0.1\", \"mixtral-8x22b-instruct-v0.1\",\n    \"mistral-7b-instruct-v0.2\", \"mistral-7b-instruct-v0.1\", \"nous-hermes-2-mixtral-8x7b\", \"gemini-1.5-pro-latest\",\n    \"gemini-1.0-pro-latest\", \"gemini-1.0-pro-vision\", \"lzlv-70b\", \"figgs-rp\", \"cinematika-7b\"\n    ]\n    name: str = \"shuttle-2-turbo\"\n    type: Literal['ShuttleAIModel'] = 'ShuttleAIModel'\n\n    def _format_messages(self, messages: List[SubclassUnion[MessageBase]]) -> List[Dict[str, str]]:\n       # Get only the properties that we require\n        message_properties = [\"content\", \"role\"]\n\n        # Exclude FunctionMessages\n        formatted_messages = [message.model_dump(include=message_properties) for message in messages]\n        return formatted_messages\n\n    \n    def predict(self, \n            conversation, \n            temperature=0.7, \n            max_tokens=256, \n            top_p=1, \n            internet=False, \n            citations=False, \n            tone='precise', \n            raw=False, \n            image=None): \n\n            formatted_messages = self._format_messages(conversation.history) \n\n            url = \"https://api.shuttleai.app/v1/chat/completions\" \n            payload = { \n                \"model\": self.name, \n                \"messages\": formatted_messages, \n                \"max_tokens\": max_tokens, \n                \"temperature\": temperature, \n                \"top_p\": top_p\n            } \n\n            if raw:\n                payload['raw'] = True\n\n            if internet:\n                payload['internet'] = True\n\n            # Only include the 'image' field if it's not None\n            if image is not None:\n                payload[\"image\"] = image\n\n            if self.name in ['gpt-4-bing', 'gpt-4-turbo-bing']: \n                payload['tone'] = tone\n                \n                # Include citations only if citations is True\n                if citations:\n                    payload['citations'] = True\n\n            headers = { \n                \"Authorization\": f\"Bearer {self.api_key}\", \n                \"Content-Type\": \"application/json\", \n            }\n\n            # Log payload for debugging\n            logging.info(f\"Payload being sent: {payload}\")\n\n            # Send the request\n            response = requests.post(url, json=payload, headers=headers)\n\n            # Log response for debugging\n            logging.info(f\"Response received: {response.text}\")\n\n            # Parse response JSON safely\n            try:\n                message_content = response.json()['choices'][0]['message']['content']\n            except KeyError as e:\n                logging.info(f\"Error parsing response: {response.text}\")\n                raise e\n\n            conversation.add_message(AgentMessage(content=message_content))  \n            return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/llms/ShuttleAIToolModel.py",
        "content": "```swarmauri/experimental/llms/ShuttleAIToolModel.py\nimport json\nimport logging\nfrom typing import List, Literal, Dict, Any\nimport requests\nfrom swarmauri.core.typing import SubclassUnion\n\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\nfrom swarmauri.standard.messages.concrete.AgentMessage import AgentMessage\nfrom swarmauri.standard.messages.concrete.FunctionMessage import FunctionMessage\nfrom swarmauri.standard.llms.base.LLMBase import LLMBase\nfrom swarmauri.standard.schema_converters.concrete.ShuttleAISchemaConverter import (\n    ShuttleAISchemaConverter,\n)\n\n\nclass ShuttleAIToolModel(LLMBase):\n    api_key: str\n    allowed_models: List[str] = [\n        \"shuttle-2-turbo\",\n        \"gpt-4-turbo-2024-04-09\",\n        \"gpt-4-0125-preview\",\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0613\",\n        \"gpt-3.5-turbo-0125\",\n        \"gpt-3.5-turbo-1106\",\n        \"claude-instant-1.1\",\n        \"wizardlm-2-8x22b\",\n        \"mistral-7b-instruct-v0.2\",\n        \"gemini-1.5-pro-latest\",\n        \"gemini-1.0-pro-latest\",\n    ]\n    name: str = \"shuttle-2-turbo\"\n    type: Literal[\"ShuttleAIToolModel\"] = \"ShuttleAIToolModel\"\n\n    def _schema_convert_tools(self, tools) -> List[Dict[str, Any]]:\n        return [ShuttleAISchemaConverter().convert(tools[tool]) for tool in tools]\n\n    def _format_messages(\n        self, messages: List[SubclassUnion[MessageBase]]\n    ) -> List[Dict[str, str]]:\n        message_properties = [\"content\", \"role\", \"name\", \"tool_call_id\", \"tool_calls\"]\n        formatted_messages = [\n            message.model_dump(include=message_properties, exclude_none=True)\n            for message in messages\n        ]\n        return formatted_messages\n\n    def predict(\n        self,\n        conversation,\n        toolkit=None,\n        tool_choice=\"auto\",\n        temperature=0.7,\n        max_tokens=1024,\n        top_p=1.0,\n        internet=False,\n        raw=False,\n        image=None,\n        citations=False,\n        tone=\"precise\",\n    ):\n        formatted_messages = self._format_messages(conversation.history)\n\n        if toolkit and not tool_choice:\n            tool_choice = \"auto\"\n\n        url = \"https://api.shuttleai.app/v1/chat/completions\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        formatted_messages = self._format_messages(conversation.history)\n\n        payload = {\n            \"model\": self.name,\n            \"messages\": formatted_messages,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"tool_choice\": tool_choice,\n            \"tools\": self._schema_convert_tools(toolkit.tools),\n        }\n\n        if raw:\n            payload[\"raw\"] = True\n\n        if internet:\n            payload[\"internet\"] = True\n\n        if image is not None:\n            payload[\"image\"] = image\n\n        if self.name in [\"gpt-4-bing\", \"gpt-4-turbo-bing\"]:\n            payload[\"tone\"] = tone\n            # Include citations only if citations is True\n            if citations:\n                payload['citations'] = True\n\n\n        logging.info(f\"payload: {payload}\")\n        \n        # First we ask agent to give us a response\n        agent_response = requests.request(\"POST\", url, json=payload, headers=headers)\n\n        logging.info(f\"agent response {agent_response.json()}\")\n\n        try:\n            messages = [\n                formatted_messages[-1],\n                agent_response.json()[\"choices\"][0][\"message\"][\"content\"],\n            ]\n        except Exception as error:\n            logging.warn(error)\n\n        tool_calls = agent_response.json()[\"choices\"][0][\"message\"].get(\n            \"tool_calls\", None\n        )\n\n\n        # If agent responds with tool call, then we execute the functions\n        if tool_calls:\n            for tool_call in tool_calls:\n                func_name = tool_call[\"function\"][\"name\"]\n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call[\"function\"][\"arguments\"])\n                func_result = func_call(**func_args)\n                func_message = FunctionMessage(content=func_result, \n                                               name=func_name, \n                                               tool_call_id=tool_call['id'])\n                conversation.add_message(func_message)\n\n\n\n        logging.info(f\"conversation: {conversation.history}\")\n\n\n        # After executing the functions, we present the results to the Agent\n        payload['messages'] = self._format_messages(conversation.history)\n\n        logging.info(f\"payload: {payload}\")\n\n        agent_response = requests.request(\"POST\", url, json=payload, headers=headers)\n        logging.info(f\"agent response {agent_response.json()}\")\n        \n        agent_message = AgentMessage(\n            content=agent_response.json()[\"choices\"][0][\"message\"][\"content\"]\n        )\n\n        conversation.add_message(agent_message)\n        logging.info(f\"conversation: {conversation.history}\")\n        return conversation\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/llms/__init__.py",
        "content": "```swarmauri/experimental/llms/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    }
]